<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[并发编程之synchronized与加锁机制]]></title>
    <url>%2F2017%2F12%2F17%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8Bsynchronized%E4%B8%8E%E5%8A%A0%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[synchronized 与 Lock目前在Java中存在两种锁机制：synchronized和Lock，Lock接口及其实现类是JDK5增加的内容。 数据同步需要依赖锁，那锁的同步又依赖谁？synchronized给出的答案是在软件层面依赖JVM，而Lock给出的方案是在硬件层面依赖特殊的CPU指令。 特性 synchronized volatile 原子性 true false 可见性 true true 是否阻塞线程 true false 使用级别 变量、方法、类 变量 用途 锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住 告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取 使用范围 a、对变量的写操作不依赖于当前值。b、该变量没有包含在具有其他变量的不变式中。 特性 synchronized ReentrantLock 是否需要手动释放 false true 等待中的线程是否可以中断等待 false true 是否可以主动获得锁 false true（投票等方式） 是否支持定时 false true 其他 synchronized是在JVM层面上实现的(语言级别)JVM会自动释放锁定 lock是通过代码实现的，要保证锁定一定会被释放，就必须将unLock()放到finally{}中 如果一个代码块被synchronized修饰了，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁，而这里获取锁的线程释放锁只会有两种情况： 1）获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 2）线程执行发生异常，此时JVM会让线程自动释放锁。 那么如果这个获取锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，试想一下，这多么影响程序执行效率。 因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过Lock就可以办到。 再举个例子：当有多个线程读写文件时，读操作和写操作会发生冲突现象，写操作和写操作会发生冲突现象，但是读操作和读操作不会发生冲突现象。 但是采用synchronized关键字来实现同步的话，就会导致一个问题： 如果多个线程都只是进行读操作，所以当一个线程在进行读操作时，其他线程只能等待无法进行读操作。 因此就需要一种机制来使得多个线程都只是进行读操作时，线程之间不会发生冲突，通过Lock就可以办到。 另外，通过Lock可以知道线程有没有成功获取到锁。这个是synchronized无法办到的。 总结一下，也就是说Lock提供了比synchronized更多的功能。但是要注意以下几点： 1）Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问； 2）Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当synchronized方法或者synchronized代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 synchronized 本文所指说的JVM是指Hotspot的6u23版本 synrhronized关键字简洁、清晰、语义明确，因此即使有了Lock接口，使用的还是非常广泛。其应用层的语义是可以把任何一个非null对象作为”锁”，当synchronized作用在方法上时，锁住的便是对象实例（this）；当作用在静态方法时锁住的便是对象对应的Class实例，因为Class数据存在于永久代，因此静态方法锁相当于该类的一个全局锁；当synchronized作用于某一个对象实例时，锁住的便是对应的代码块。在HotSpot JVM实现中，锁有个专门的名字：对象监视器。 在同步代码块中，同步锁就是这个代码块所属的那个对象 在同步方法中（非静态），同步锁就是调用这个方法的那个对象 在同步方法中（静态），同步锁就是这个方法所属的这个类的字节码文件，即 类名.class; 线程状态及状态转换 当多个线程同时请求某个对象监视器时，对象监视器会设置几种状态用来区分请求的线程： Contention List：所有请求锁的线程将被首先放置到该竞争队列 Entry List：Contention List中那些有资格成为候选人的线程被移到Entry - List Wait Set：那些调用wait方法被阻塞的线程被放置到Wait Set OnDeck：任何时刻最多只能有一个线程正在竞争锁，该线程称为OnDeck Owner：获得锁的线程称为Owner !Owner：释放锁的线程 下图反映了个状态转换关系： 新请求锁的线程将首先被加入到ConetentionList中，当某个拥有锁的线程（Owner状态）调用unlock之后，如果发现EntryList为空则从ContentionList中移动线程到EntryList，下面说明下ContentionList和EntryList的实现方式： ContentionList虚拟队列ContentionList并不是一个真正的Queue，而只是一个虚拟队列，原因在于ContentionList是由Node及其next指针逻辑构成，并不存在一个Queue的数据结构。ContentionList是一个后进先出（LIFO）的队列，每次新加入Node时都会在队头进行，通过CAS改变第一个节点的的指针为新增节点，同时设置新增节点的next指向后续节点，而取得操作则发生在队尾。显然，该结构其实是个Lock-Free的队列。 因为只有Owner线程才能从队尾取元素，也即线程出列操作无争用，当然也就避免了CAS的ABA问题。 EntryListEntryList与ContentionList逻辑上同属等待队列，ContentionList会被线程并发访问，为了降低对ContentionList队尾的争用，而建立EntryList。Owner线程在unlock时会从ContentionList中迁移线程到EntryList，并会指定EntryList中的某个线程（一般为Head）为Ready（OnDeck）线程。Owner线程并不是把锁传递给OnDeck线程，只是把竞争锁的权利交给OnDeck，OnDeck线程需要重新竞争锁。这样做虽然牺牲了一定的公平性，但极大的提高了整体吞吐量，在Hotspot中把OnDeck的选择行为称之为“竞争切换”。 OnDeck线程获得锁后即变为owner线程，无法获得锁则会依然留在EntryList中，考虑到公平性，在EntryList中的位置不发生变化（依然在队头）。如果Owner线程被wait方法阻塞，则转移到WaitSet队列；如果在某个时刻被notify/notifyAll唤醒，则再次转移到EntryList。 自旋锁那些处于ContetionList、EntryList、WaitSet中的线程均处于阻塞状态，阻塞操作由操作系统完成（在Linxu下通过pthread_mutex_lock函数）。线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能。 缓解上述问题的办法便是自旋，其原理是：当发生争用时，若Owner线程能在很短的时间内释放锁，则那些正在争用线程可以稍微等一等（自旋），在Owner线程释放锁后，争用线程可能会立即得到锁，从而避免了系统阻塞。但Owner运行的时间可能会超出了临界值，争用线程自旋一段时间后还是无法获得锁，这时争用线程则会停止自旋进入阻塞状态（后退）。基本思路就是自旋，不成功再阻塞，尽量降低阻塞的可能性，这对那些执行时间很短的代码块来说有非常重要的性能提高。自旋锁有个更贴切的名字：自旋-指数后退锁，也即复合锁。很显然，自旋在多处理器上才有意义。 还有个问题是，线程自旋时做些啥？其实啥都不做，可以执行几次for循环，可以执行几条空的汇编指令，目的是占着CPU不放，等待获取锁的机会。所以说，自旋是把双刃剑，如果旋的时间过长会影响整体性能，时间过短又达不到延迟阻塞的目的。显然，自旋的周期选择显得非常重要，但这与操作系统、硬件体系、系统的负载等诸多场景相关，很难选择，如果选择不当，不但性能得不到提高，可能还会下降，因此大家普遍认为自旋锁不具有扩展性。 对自旋锁周期的选择上，HotSpot认为最佳时间应是一个线程上下文切换的时间，但目前并没有做到。经过调查，目前只是通过汇编暂停了几个CPU周期，除了自旋周期选择，HotSpot还进行许多其他的自旋优化策略，具体如下： 如果平均负载小于CPUs则一直自旋 如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞 如果正在自旋的线程发现Owner发生了变化则延迟自旋时间（自旋计数）或进入阻塞 如果CPU处于节电模式则停止自旋 自旋时间的最坏情况是CPU的存储延迟（CPU A存储了一个数据，到CPU B得知这个数据直接的时间差） 自旋时会适当放弃线程优先级之间的差异 那synchronized实现何时使用了自旋锁？答案是在线程进入ContentionList时，也即第一步操作前。线程在进入等待队列时首先进行自旋尝试获得锁，如果不成功再进入等待队列。这对那些已经在等待队列中的线程来说，稍微显得不公平。还有一个不公平的地方是自旋线程可能会抢占了Ready线程的锁。自旋锁由每个监视对象维护，每个监视对象一个。 偏向锁在JVM1.6中引入了偏向锁，偏向锁主要解决无竞争下的锁性能问题，首先我们看下无竞争下锁存在什么问题： 现在几乎所有的锁都是可重入的，也即已经获得锁的线程可以多次锁住/解锁监视对象，按照之前的HotSpot设计，每次加锁/解锁都会涉及到一些CAS操作（比如对等待队列的CAS操作），CAS操作会延迟本地调用，因此偏向锁的想法是一旦线程第一次获得了监视对象，之后让监视对象“偏向”这个线程，之后的多次调用则可以避免CAS操作，说白了就是置个变量，如果发现为true则无需再走各种加锁/解锁流程。但还有很多概念需要解释、很多引入的问题需要解决： CAS及SMP架构CAS为什么会引入本地延迟？这要从SMP（对称多处理器）架构说起，下图大概表明了SMP的结构： 其意思是所有的CPU会共享一条系统总线（BUS），靠此总线连接主存。每个核都有自己的一级缓存，各核相对于BUS对称分布，因此这种结构称为“对称多处理器”。 而CAS的全称为Compare-And-Swap，是一条CPU的原子指令，其作用是让CPU比较后原子地更新某个位置的值，经过调查发现，其实现方式是基于硬件平台的汇编指令，就是说CAS是靠硬件实现的，JVM只是封装了汇编调用，那些AtomicInteger类便是使用了这些封装后的接口。 Core1和Core2可能会同时把主存中某个位置的值Load到自己的L1 Cache中，当Core1在自己的L1 Cache中修改这个位置的值时，会通过总线，使Core2中L1 Cache对应的值“失效”，而Core2一旦发现自己L1 Cache中的值失效（称为Cache命中缺失）则会通过总线从内存中加载该地址最新的值，大家通过总线的来回通信称为“Cache一致性流量”，因为总线被设计为固定的“通信能力”，如果Cache一致性流量过大，总线将成为瓶颈。而当Core1和Core2中的值再次一致时，称为“Cache一致性”，从这个层面来说，锁设计的终极目标便是减少Cache一致性流量。 而CAS恰好会导致Cache一致性流量，如果有很多线程都共享同一个对象，当某个Core CAS成功时必然会引起总线风暴，这就是所谓的本地延迟，本质上偏向锁就是为了消除CAS，降低Cache一致性流量。 Cache一致性：上面提到Cache一致性，其实是有协议支持的，现在通用的协议是MESI（最早由Intel开始支持），具体参考： http://en.wikipedia.org/wiki/MESI_protocol，以后会仔细讲解这部分。Cache一致性流量的例外情况：其实也不是所有的CAS都会导致总线风暴，这跟Cache一致性协议有关，具体参考：http://blogs.oracle.com/dave/entry/biased_locking_in_hotspotNUMA(Non Uniform Memory Access Achitecture）架构：与SMP对应还有非对称多处理器架构，现在主要应用在一些高端处理器上，主要特点是没有总线，没有公用主存，每个Core有自己的内存，针对这种结构此处不做讨论。 偏向解除偏向锁引入的一个重要问题是，在多争用的场景下，如果另外一个线程争用偏向对象，拥有者需要释放偏向锁，而释放的过程会带来一些性能开销，但总体说来偏向锁带来的好处还是大于CAS代价的。 总结关于锁，JVM中还引入了一些其他技术比如锁膨胀等，这些与自旋锁、偏向锁相比影响不是很大，这里就不做介绍。通过上面的介绍可以看出，synchronized的底层实现主要依靠Lock-Free的队列，基本思路是自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。 Lock前文分析了JVM中的synchronized实现，本文继续分析JVM中的另一种锁Lock的实现。与synchronized不同的是，Lock完全用Java写成，在java这个层面是无关JVM实现的。 在java.util.concurrent.locks包中有很多Lock的实现类，常用的有ReentrantLock、ReadWriteLock（实现类ReentrantReadWriteLock），其实现都依赖java.util.concurrent.AbstractQueuedSynchronizer类，实现思路都大同小异，因此我们以ReentrantLock作为讲解切入点。 ReentrantLock的调用过程经过观察ReentrantLock把所有Lock接口的操作都委派到一个Sync类上，该类继承了AbstractQueuedSynchronizer： static abstract class Sync extends AbstractQueuedSynchronizer Sync又有两个子类： final static class NonfairSync extends Sync final static class FairSync extends Sync 显然是为了支持公平锁和非公平锁而定义，默认情况下为非公平锁。 先理一下Reentrant.lock()方法的调用过程（默认非公平锁）： 这些讨厌的Template模式导致很难直观的看到整个调用过程，其实通过上面调用过程及AbstractQueuedSynchronizer的注释可以发现，AbstractQueuedSynchronizer中抽象了绝大多数Lock的功能，而只把tryAcquire方法延迟到子类中实现。tryAcquire方法的语义在于用具体子类判断请求线程是否可以获得锁，无论成功与否AbstractQueuedSynchronizer都将处理后面的流程。 锁实现（加锁）简单说来，AbstractQueuedSynchronizer会把所有的请求线程构成一个CLH队列，当一个线程执行完毕（lock.unlock()）时会激活自己的后继节点，但正在执行的线程并不在队列中，而那些等待执行的线程全部处于阻塞状态，经过调查线程的显式阻塞是通过调用LockSupport.park()完成，而LockSupport.park()则调用sun.misc.Unsafe.park()本地方法，再进一步，HotSpot在Linux中中通过调用pthread_mutex_lock函数把线程交给系统内核进行阻塞。 该队列如图： 与synchronized相同的是，这也是一个虚拟队列，不存在队列实例，仅存在节点之间的前后关系。令人疑惑的是为什么采用CLH队列呢？原生的CLH队列是用于自旋锁，但Doug Lea把其改造为阻塞锁。 当有线程竞争锁时，该线程会首先尝试获得锁，这对于那些已经在队列中排队的线程来说显得不公平，这也是非公平锁的由来，与synchronized实现类似，这样会极大提高吞吐量。 如果已经存在Running线程，则新的竞争线程会被追加到队尾，具体是采用基于CAS的Lock-Free算法，因为线程并发对Tail调用CAS可能会导致其他线程CAS失败，解决办法是循环CAS直至成功。AbstractQueuedSynchronizer的实现非常精巧，令人叹为观止，不入细节难以完全领会其精髓，下面详细说明实现过程： Sync.nonfairTryAcquirenonfairTryAcquire方法将是lock方法间接调用的第一个方法，每次请求锁时都会首先调用该方法。 final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 该方法会首先判断当前状态，如果c==0说明没有线程正在竞争该锁，如果不c !=0 说明有线程正拥有了该锁。 如果发现c==0，则通过CAS设置该状态值为acquires,acquires的初始调用值为1，每次线程重入该锁都会+1，每次unlock都会-1，但为0时释放锁。如果CAS设置成功，则可以预计其他任何线程调用CAS都不会再成功，也就认为当前线程得到了该锁，也作为Running线程，很显然这个Running线程并未进入等待队列。 如果c !=0但发现自己已经拥有锁，只是简单地++acquires，并修改status值，但因为没有竞争，所以通过setStatus修改，而非CAS，也就是说这段代码实现了偏向锁的功能，并且实现的非常漂亮。 AbstractQueuedSynchronizer.addWaiteraddWaiter方法负责把当前无法获得锁的线程包装为一个Node添加到队尾： private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } 其中参数mode是独占锁还是共享锁，默认为null，独占锁。追加到队尾的动作分两步： 如果当前队尾已经存在(tail!=null)，则使用CAS把当前线程更新为Tail如果当前Tail为null或则线程调用CAS设置队尾失败，则通过enq方法继续设置Tail下面是enq方法： private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize Node h = new Node(); // Dummy header h.next = node; node.prev = h; if (compareAndSetHead(h)) { tail = node; return h; } } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 该方法就是循环调用CAS，即使有高并发的场景，无限循环将会最终成功把当前线程追加到队尾（或设置队头）。总而言之，addWaiter的目的就是通过CAS把当前现在追加到队尾，并返回包装后的Node实例。 把线程要包装为Node对象的主要原因，除了用Node构造供虚拟队列外，还用Node包装了各种线程状态，这些状态被精心设计为一些数字值： SIGNAL(-1) ：线程的后继线程正/已被阻塞，当该线程release或cancel时要重新这个后继线程(unpark) CANCELLED(1)：因为超时或中断，该线程已经被取消 CONDITION(-2)：表明该线程被处于条件队列，就是因为调用了Condition.await而被阻塞 PROPAGATE(-3)：传播共享锁 0：0代表无状态 AbstractQueuedSynchronizer.acquireQueuedacquireQueued的主要作用是把已经追加到队列的线程节点（addWaiter方法返回值）进行阻塞，但阻塞前又通过tryAccquire重试是否能获得锁，如果重试成功能则无需阻塞，直接返回 final boolean acquireQueued(final Node node, int arg) { try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } catch (RuntimeException ex) { cancelAcquire(node); throw ex; } } 仔细看看这个方法是个无限循环，感觉如果p == head &amp;&amp; tryAcquire(arg)条件不满足循环将永远无法结束，当然不会出现死循环，奥秘在于第12行的parkAndCheckInterrupt会把当前线程挂起，从而阻塞住线程的调用栈。 private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } 如前面所述，LockSupport.park最终把线程交给系统（Linux）内核进行阻塞。当然也不是马上把请求不到锁的线程进行阻塞，还要检查该线程的状态，比如如果该线程处于Cancel状态则没有必要，具体的检查在shouldParkAfterFailedAcquire中： private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park */ return true; if (ws &gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don&#39;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 检查原则在于： 规则1：如果前继的节点状态为SIGNAL，表明当前节点需要unpark，则返回成功，此时acquireQueued方法的第12行（parkAndCheckInterrupt）将导致线程阻塞 规则2：如果前继节点状态为CANCELLED(ws&gt;0)，说明前置节点已经被放弃，则回溯到一个非取消的前继节点，返回false，acquireQueued方法的无限循环将递归调用该方法，直至规则1返回true，导致线程阻塞 规则3：如果前继节点状态为非SIGNAL、非CANCELLED，则设置前继的状态为SIGNAL，返回false后进入acquireQueued的无限循环，与规则2同总体看来，shouldParkAfterFailedAcquire就是靠前继节点判断当前线程是否应该被阻塞，如果前继节点处于CANCELLED状态，则顺便删除这些节点重新构造队列。 至此，锁住线程的逻辑已经完成，下面讨论解锁的过程。 解锁请求锁不成功的线程会被挂起在acquireQueued方法的第12行，12行以后的代码必须等线程被解锁锁才能执行，假如被阻塞的线程得到解锁，则执行第13行，即设置interrupted = true，之后又进入无限循环。 从无限循环的代码可以看出，并不是得到解锁的线程一定能获得锁，必须在第6行中调用tryAccquire重新竞争，因为锁是非公平的，有可能被新加入的线程获得，从而导致刚被唤醒的线程再次被阻塞，这个细节充分体现了“非公平”的精髓。通过之后将要介绍的解锁机制会看到，第一个被解锁的线程就是Head，因此p == head的判断基本都会成功。 至此可以看到，把tryAcquire方法延迟到子类中实现的做法非常精妙并具有极强的可扩展性，令人叹为观止！当然精妙的不是这个Templae设计模式，而是Doug Lea对锁结构的精心布局。 解锁代码相对简单，主要体现在AbstractQueuedSynchronizer.release和Sync.tryRelease方法中： class AbstractQueuedSynchronizer public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } class Sync protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } tryRelease与tryAcquire语义相同，把如何释放的逻辑延迟到子类中。tryRelease语义很明确：如果线程多次锁定，则进行多次释放，直至status==0则真正释放锁，所谓释放锁即设置status为0，因为无竞争所以没有使用CAS。 release的语义在于：如果可以释放锁，则唤醒队列第一个线程（Head），具体唤醒代码如下： private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); } 这段代码的意思在于找出第一个可以unpark的线程，一般说来head.next == head，Head就是第一个线程，但Head.next可能被取消或被置为null，因此比较稳妥的办法是从后往前找第一个可用线程。貌似回溯会导致性能降低，其实这个发生的几率很小，所以不会有性能影响。之后便是通知系统内核继续该线程，在Linux下是通过pthread_mutex_unlock完成。之后，被解锁的线程进入上面所说的重新竞争状态。 Lock &amp; SynchronizedAbstractQueuedSynchronizer通过构造一个基于阻塞的CLH队列容纳所有的阻塞线程，而对该队列的操作均通过Lock-Free（CAS）操作，但对已经获得锁的线程而言，ReentrantLock实现了偏向锁的功能。 synchronized的底层也是一个基于CAS操作的等待队列，但JVM实现的更精细，把等待队列分为ContentionList和EntryList，目的是为了降低线程的出列速度；当然也实现了偏向锁，从数据结构来说二者设计没有本质区别。但synchronized还实现了自旋锁，并针对不同的系统和硬件体系进行了优化，而Lock则完全依靠系统阻塞挂起等待线程。 当然Lock比synchronized更适合在应用层扩展，可以继承AbstractQueuedSynchronizer定义各种实现，比如实现读写锁（ReadWriteLock），公平或不公平锁；同时，Lock对应的Condition调用await()、signal()、signAll()这几个方法比notify()、wait()、notifyAll()要方便的多、灵活的多。]]></content>
      <categories>
        <category>java</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>synchronized</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jdk8💗特性]]></title>
    <url>%2F2017%2F12%2F12%2Fjdk8%F0%9F%92%97%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[本文翻译整理自 https://github.com/winterbe/java8-tutorial 希望阅读完本文的小伙伴能够熟练掌握和应用 Java8 的各种特性，使其成为在工作中的一门利器。话不多说，让我们一起开启 Java8 新特性之旅吧！ 接口内允许添加默认实现的方法Java 8 允许我们通过 default 关键字对接口中定义的抽象方法提供一个默认的实现。 请看下面示例代码： 12345678910// 定义一个公式接口interface Formula &#123; // 计算 double calculate(int a); // 求平方根 default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125; 在上面这个接口中，我们除了定义了一个抽象方法 calculate，还定义了一个带有默认实现的方法 sqrt。我们在实现这个接口时，可以只需要实现 calculate 方法，默认方法 sqrt 可以直接调用即可，也就是说我们可以不必强制实现 sqrt 方法。 补充：通过 default 关键字这个新特性，可以非常方便地对之前的接口做拓展，而此接口的实现类不必做任何改动。 123456789Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;;formula.calculate(100); // 100.0formula.sqrt(16); // 4.0 上面通过匿名对象实现了 Formula 接口。但是即使是这样，我们为了完成一个 sqrt(a * 100) 简单计算，就写了 6 行代码，很是冗余。 Lambda 表达式Lambda表达式就是一个匿名函数。是Java 为了引入函数式编程而引入的，在Java中，Lambda表达式是有函数式接口实现的。对于函数式接口在第一节简介中有提到，就是只包含一个抽象方法的接口。在Java 8 使用Lambda非常的简单例如： 123456(int a, int b) -&gt; a * b (a, b) -&gt; a - b () -&gt; 99 (String a) -&gt; System.out.println(a) a -&gt; 2 * a c -&gt; &#123; //some complex statements &#125; 这些都是lambda表达式,编写规则： Lambda 可以有0个或多个参数,多个参数使用括号括起来，并用逗号隔开，如果只有一个参数括号可以省略。 参数的类型可以明确声明，也可以省略，Lambda会自动推导出参数类型 Lambda 函数体可以包含一个或多个语句，如果只有一个表达式可以省略大括号。 在学习 Lambda 表达式之前，我们先来看一段老版本的示例代码，其对一个含有字符串的集合进行排序： 12345678List&lt;String&gt; names = Arrays.asList("peter", "anna", "mike", "xenia");Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;); Collections 工具类提供了静态方法 sort 方法，入参是一个 List 集合，和一个 Comparator 比较器，以便对给定的 List 集合进行排序。上面的示例代码创建了一个匿名内部类作为入参，这种类似的操作在我们日常的工作中随处可见。 Java 8 中不再推荐这种写法，而是推荐使用 Lambda 表达： 123Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;); 正如你看到的，上面这段代码变得简短很多而且易于阅读。但是我们还可以再精炼一点： 1Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); 对于只包含一行方法的代码块，我们可以省略大括号，直接 return 关键代码即可。追求极致，我们还可以让它再短点： 1names.sort((a, b) -&gt; b.compareTo(a)); List 集合现在已经添加了 sort 方法。而且 Java 编译器能够根据类型推断机制判断出参数类型，这样，你连入参的类型都可以省略啦，怎么样，是不是感觉很强大呢！ 使用Lambda表达式 遍历列表并执行一些操作 12345678List&lt;String&gt; pointList = new ArrayList();pointList.add("1");pointList.add("2"); pointList.forEach(p -&gt; &#123; System.out.println(p); &#125; ); 在线程中创建 Runnable 123new Thread( () -&gt; System.out.println("My Runnable");).start(); 根据员工的名字排序,这里直接使用了方法引用 123456789101112131415161718192021222324252627282930public class LambdaIntroduction &#123; public static void main (String[] ar)&#123; Employee[] employees = &#123; new Employee("David"), new Employee("Naveen"), new Employee("Alex"), new Employee("Richard")&#125;; System.out.println("Before Sorting Names: "+Arrays.toString(employees)); Arrays.sort(employees, Employee::nameCompare); System.out.println("After Sorting Names "+Arrays.toString(employees)); &#125;&#125; class Employee &#123; String name; Employee(String name) &#123; this.name = name; &#125; public static int nameCompare(Employee a1, Employee a2) &#123; return a1.name.compareTo(a2.name); &#125; public String toString() &#123; return name; &#125;&#125; 对GUI添加实践监听器：1234JButton button = new JButton("Submit");button.addActionListener((e) -&gt; &#123; System.out.println("Click event triggered !!");&#125;); ) 以上这些都是使用Java 8 lambda表达式的示例。 默认方法什么是默认方法正如其名，在Java 8中，默认方法就是使用default关键字标记的定义在接口中的方法。如果子类不重写这些方法，那么在子类调用这些方法的时候会直接调用接口中的默认方法。其形式如下： 12345public interfce Moveable&#123; default void move()&#123; System.out.println("move"); &#125;&#125; 子类继承接口，无需重写这个方法即可调用 123456public class Animal implements Moveable&#123; public static void main(String[] args)&#123; Animal tiger = new Animal(); tiger.move(); &#125;&#125; 输出： Move 如果子类想要重写这个默认方法，只需要和普通的重写接口一样 1234567891011public class Animal implements Moveable&#123; public void move()&#123; System.out.println("I am running"); &#125; public static void main(String[] args)&#123; Animal tiger = new Animal(); tiger.move(); &#125;&#125; 使用默认方法的好处： 定义静态默认方法：在接口中定义静态默认方法，使得其所有继承接口的子类都可以调用这些静态方法，这样可以在接口中集中静态方法，其子类都可以分享，而无需将静态方法分开定义。 提供兼容性，如果已经发布的接口想要添加新的方法，只需要添加默认方法即可，子类无需做出任何改变。 为什么在Java 8 要引入默认方法 最简单的回单就是，为了支持Lambda表达式。因为Java 8中引入了Lambda表达式，所以很多Java的核心类都需要修改以便使用新的Lambda表达式，但是一些类例如，java.util.List，不仅仅是JDK的类实现，同样被很多第三方的库继承，如果修改了java.util.List 接口，那么其他所有的第三方库都必须重新实现这些方法，最终导致无法兼容。所以就引入了默认方法。例如：java.lang.Iterable，添加的新方法。 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 使用 1234567891011121314import java.util.ArrayList;import java.util.List; public class Animal implements Moveable&#123; public static void main(String[] args)&#123; List&lt;Animal&gt; list = new ArrayList(); list.add(new Animal()); list.add(new Animal()); list.add(new Animal()); //Iterator code reduced to one line list.forEach((Moveable p) -&gt; p.move()); &#125;&#125; ==添加的默认方法，其所有的子类都默认继承，而无需重写。== 默认方法继承发生冲突如何解决在Java中接口是可以多继承的，当一个类继承多个接口，但是默认方法相同的处理原则是： 如果子类覆盖，直接调用子类方法 如果函数有相同的签名，选择更加明确的默认方法，例如，Moveable和Walkable,Walkable继承于Moveable，那么就选择Walkable中的默认方法。 如果两个接口平级，没有继承关系，编译器报错。这个时候可以通过制定执行某个方法来确定 123Walkable.super.move();//orMoveable.super.move(); 函数式接口 Functional Interface抛出一个疑问：在我们书写一段 Lambda 表达式后，Java 编译器是如何进行类型推断的，它又是怎么知道重写的哪个方法的？ 需要说明的是，不是每个接口都可以缩写成 Lambda 表达式。只有那些函数式接口（Functional Interface）才能缩写成 Lambda 表示式。 那么什么是函数式接口（Functional Interface）呢？ 所谓函数式接口（Functional Interface）就是只包含一个抽象方法的声明。针对该接口类型的所有 Lambda 表达式都会与这个抽象方法匹配。 注意：你可能会有疑问，Java 8 中不是允许通过 defualt 关键字来为接口添加默认方法吗？那它算不算抽象方法呢？答案是：不算。因此，你可以毫无顾忌的添加默认方法，它并不违反函数式接口（Functional Interface）的定义。 总结一下：只要接口中仅仅包含一个抽象方法，我们就可以将其改写为 Lambda 表达式。为了保证一个接口明确的被定义为一个函数式接口（Functional Interface），Java 8添加了@FunctionalInterface 注解提示编译器检查是否是一个函数式接口，==这个注解并不是必须的==，只是起到提示的作用。推荐加上 @FunctionalInterface 来标记这个接口是函数式接口，就可以在编译期间检查错误。 函数式接口只能有一个抽象方法，但是可以添加任意的默认方法。 示例代码： @FunctionalInterface interface Converter&lt;F, T&gt; { T convert(F from); } 示例代码2： Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from); Integer converted = converter.convert(&quot;123&quot;); System.out.println(converted); // 123 引用类的构造器及方法在Java 8中可以通过class::methodName的方式引用一个类或者实例的方法。 方法引用的类型 引用静态方法：直接引用类的静态方法，例如Math::max 等同于 Math.max(x,y) 通过实例引用实例方法：例如：System.out::println等同于System.out.println(x) 通过类引用实例方法： 例如String::length等同于 str.length() 引用构造函数：例如ArrayList::new等同于 new ArrayList() 下面详细介绍一下这几种方法引用的形式： 引用静态方法123List&lt;Integer&gt; integers = Arrays.asList(1,2,3,5);Optional&lt;Integer&gt; max = integers.stream().reduce(Math::max);max.ifPresent(value - &gt; System.out.println(value)); 输出： 15 通过实例引用实例方法 123List&lt;Integer&gt; integers = Arrays.asList(1,2,3,5);Optional&lt;Integer&gt; max = integers.stream().reduce(Math::max);max.ifPresent(value - &gt; System.out::println); 输出： 15 通过类引用实例方法 12345678910111213141516List&lt;String&gt; strings = Arrays .asList("how", "to", "do", "in", "java", "dot", "com"); List&lt;String&gt; sorted = strings .stream() .sorted((s1, s2) -&gt; s1.compareTo(s2)) .collect(Collectors.toList()); System.out.println(sorted); List&lt;String&gt; sortedAlt = strings .stream() .sorted(String::compareTo) .collect(Collectors.toList()); System.out.println(sortedAlt); 输出： 12[com, do, dot, how, in, java, to][com, do, dot, how, in, java, to] 引用构造函数 12345678List&lt;Integer&gt; integers = IntStream .range(1, 100) .boxed() .collect(Collectors.toCollection( ArrayList::new )); Optional&lt;Integer&gt; max = integers.stream().reduce(Math::max); max.ifPresent(System.out::println); 输出： 199 1234@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; 123Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert("123");System.out.println(converted); // 123 上面这段代码，通过 Java 8 的新特性，进一步简化上面的代码： 123Converter&lt;String, Integer&gt; converter = Integer::valueOf;Integer converted = converter.convert("123");System.out.println(converted); // 123 Java 8 中允许你通过 :: 关键字来引用类的方法或构造器。上面的代码简单的示例了如何引用静态方法，当然，除了静态方法，我们还可以引用普通方法： 12345class Something &#123; String startsWith(String s) &#123; return String.valueOf(s.charAt(0)); &#125;&#125; 1234Something something = new Something();Converter&lt;String, String&gt; converter = something::startsWith;String converted = converter.convert("Java");System.out.println(converted); // "J" 接下来，我们再来看看如何通过 :: 关键字来引用类的构造器。首先，我们先来定义一个示例类，在类中声明两个构造器： 1234567891011class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 然后，我们再定义一个工厂接口，用来生成 Person 类： 1234// Person 工厂interface PersonFactory&lt;P extends Person&gt; &#123; P create(String firstName, String lastName);&#125; 我们可以通过 :: 关键字来引用 Person 类的构造器，来代替手动去实现这个工厂接口： 123// 直接引用 Person 构造器PersonFactory&lt;Person&gt; personFactory = Person::new;Person person = personFactory.create("Peter", "Parker"); Person::new 这段代码，能够直接引用 Person 类的构造器。然后 Java 编译器能够根据上下文选中正确的构造器去实现 PersonFactory.create 方法。 Lambda 访问外部变量及接口默认方法在本章节中，我们将会讨论如何在 lambda 表达式中访问外部变量（包括：局部变量，成员变量，静态变量，接口的默认方法.），它与匿名内部类访问外部变量很相似。 访问局部变量在 Lambda 表达式中，我们可以访问外部的 final 类型变量，如下面的示例代码： 12345// 转换器@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; 12345final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 与匿名内部类不同的是，我们不必显式声明 num 变量为 final 类型，下面这段代码同样有效： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 但是 num 变量必须为隐式的 final 类型，何为隐式的 final 呢？就是说到编译期为止，num 对象是不能被改变的，如下面这段代码，就不能被编译通过： 1234int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);num = 3; 在 lambda 表达式内部改变 num 值同样编译不通过，需要注意, 比如下面的示例代码： 123456int num = 1;Converter&lt;Integer, String&gt; converter = (from) -&gt; &#123; String value = String.valueOf(from + num); num = 3; return value;&#125;; 访问成员变量和静态变量上一章节中，了解了如何在 Lambda 表达式中访问局部变量。与局部变量相比，在 Lambda 表达式中对成员变量和静态变量拥有读写权限： 1234@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; 1234567891011121314151617181920class Lambda4 &#123; // 静态变量 static int outerStaticNum; // 成员变量 int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; // 对成员变量赋值 outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; // 对静态变量赋值 outerStaticNum = 72; return String.valueOf(from); &#125;; &#125; &#125; 访问接口的默认方法还记得第一章节中定义的那个 Formula (公式) 接口吗？ 12345678910@FunctionalInterfaceinterface Formula &#123; // 计算 double calculate(int a); // 求平方根 default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125; 当时，我们在接口中定义了一个带有默认实现的 sqrt 求平方根方法，在匿名内部类中我们可以很方便的访问此方法： 123456Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;; 但是在 lambda 表达式中可不行： 1Formula formula = (a) -&gt; sqrt(a * 100); 带有默认实现的接口方法，是不能在 lambda 表达式中访问的，上面这段代码将无法被编译通过。 内置的函数式接口JDK 1.8 API 包含了很多内置的函数式接口。其中就包括我们在老版本中经常见到的 Comparator 和 Runnable，Java 8 为他们都添加了 @FunctionalInterface 注解，以用来支持 Lambda 表达式。 值得一提的是，除了 Comparator 和 Runnable 外，还有一些新的函数式接口，它们很多都借鉴于知名的 Google Guava 库。 对于它们，即使你已经非常熟悉了，还是最好了解一下的： Predicate 断言Predicate 是一个可以指定入参类型，并返回 boolean 值的函数式接口。它内部提供了一些带有默认实现的方法，可以被用来组合一个复杂的逻辑判断（and, or, negate）： 1234567891011Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test("foo"); // truepredicate.negate().test("foo"); // falsepredicate.and((s) -&gt; s.contains("a")).test("foo"); //falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate(); 例如：在filter中 就会接收一个Predicate 123456789101112/** * Returns a stream consisting of the elements of this stream that match * the given predicate. * * &lt;p&gt;This is an &lt;a href="package-summary.html#StreamOps"&gt;intermediate * operation&lt;/a&gt;. * * @param predicate a non-interfering stateless predicate to apply to each element to determine if it * should be included in the new returned stream. * @return the new stream */Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate); 下面来演示一下如何使用Predicate 12345678910111213141516171819202122232425262728293031323334package predicateExample; import lombok.AccessLevel;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;import lombok.NoArgsConstructor;import java.util.function.Predicate;@Data@Builder@NoArgsConstructor@AllArgsConstructor(access = AccessLevel.PUBLIC)public class Member &#123; private Integer id; private Integer age; private String gender; private String firstName; private String lastName; public static Predicate&lt;Member&gt; isAdultMale() &#123; return p -&gt; p.getAge() &gt; 21 &amp;&amp; p.getGender().equalsIgnoreCase("M"); &#125; public static Predicate&lt;Member&gt; isAdultFemale() &#123; return p -&gt; p.getAge() &gt; 18 &amp;&amp; p.getGender().equalsIgnoreCase("F"); &#125; public static Predicate&lt;Member&gt; isAgeMoreThan(Integer age) &#123; return p -&gt; p.getAge() &gt; age; &#125;&#125; 上面的代码定义了多个Predicate，分别对应多个筛选条件，下面开始使用这些断言： 1234567891011121314151617181920212223package predicateExample; import java.util.List;import java.util.function.Predicate;import java.util.stream.Collectors;public class MemberPredicates &#123; public static Predicate&lt;Member&gt; isAdultMale() &#123; return p -&gt; p.getAge() &gt; 21 &amp;&amp; p.getGender().equalsIgnoreCase("M"); &#125; public static Predicate&lt;Member&gt; isAdultFemale() &#123; return p -&gt; p.getAge() &gt; 18 &amp;&amp; p.getGender().equalsIgnoreCase("F"); &#125; public static Predicate&lt;Member&gt; isAgeMoreThan(Integer age) &#123; return p -&gt; p.getAge() &gt; age; &#125; public static List&lt;Integer&gt; filterMembers (List&lt;Member&gt; employees, Predicate&lt;Member&gt; predicate) &#123; return employees.stream().filter( predicate ).map(Member::getId).collect(Collectors.toList()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637package predicateExample; import com.mintbao.eva.dto.admin.Member;import com.mintbao.eva.dto.admin.MemberPredicates;import lombok.extern.slf4j.Slf4j;import java.util.ArrayList;import java.util.Arrays;import java.util.List;@Slf4jpublic class TestMemberPredicates &#123; public static void main(String[] args) &#123; Member member = new Member(1, 23, &quot;M&quot;, &quot;Rick&quot;, &quot;Beethovan&quot;); Member member1 = new Member(2, 13, &quot;F&quot;, &quot;Martina&quot;, &quot;Hengis&quot;); Member member2 = new Member(3, 43, &quot;M&quot;, &quot;Ricky&quot;, &quot;Martin&quot;); Member member3 = new Member(4, 26, &quot;M&quot;, &quot;Jon&quot;, &quot;Lowman&quot;); Member member4 = new Member(5, 19, &quot;F&quot;, &quot;Cristine&quot;, &quot;Maria&quot;); Member member5 = new Member(6, 15, &quot;M&quot;, &quot;David&quot;, &quot;Feezor&quot;); Member member6 = new Member(7, 68, &quot;F&quot;, &quot;Melissa&quot;, &quot;Roy&quot;); Member member7 = new Member(8, 79, &quot;M&quot;, &quot;Alex&quot;, &quot;Gussin&quot;); Member member8 = new Member(9, 15, &quot;F&quot;, &quot;Neetu&quot;, &quot;Singh&quot;); Member member9 = new Member(10, 45, &quot;M&quot;, &quot;Naveen&quot;, &quot;Jain&quot;); List&lt;Member&gt; employees = new ArrayList&lt;&gt;(Arrays.asList(member, member1, member2, member3, member4, member5, member6, member7, member8, member9)); log.info(&quot;adult and male: &#123;&#125;&quot;, MemberPredicates.filterMembers(employees, MemberPredicates.isAdultMale())); log.info(&quot;adult and female: &#123;&#125;&quot;, MemberPredicates.filterMembers(employees, MemberPredicates.isAdultFemale())); log.info(&quot;older than: &#123;&#125;&quot;, MemberPredicates.filterMembers(employees, MemberPredicates.isAgeMoreThan(35))); log.info(&quot;not older than: &#123;&#125;&quot;, MemberPredicates.filterMembers(employees, MemberPredicates.isAgeMoreThan(35).negate())); &#125;&#125; 输出： 1234adult and male: [1, 3, 4, 8, 10]adult and female: [5, 7]older than: [3, 7, 8, 10]not older than: [1, 2, 4, 5, 6, 9] 正则表达式表示：可以通过Pattern.compile().asPredicate()将正则表达式转换为Predicate。 在Java 8之前，从一个数组中找出符合正则规则的字符串的方法是 123456789101112131415161718192021222324252627import lombok.extern.slf4j.Slf4j;import java.util.Arrays;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;@Slf4jpublic class TestMemberPredicates &#123; private static final String EMAIL_PATTERN = "^(.+)@example.com$"; public static void main(String[] args) &#123; Pattern pattern = Pattern.compile(EMAIL_PATTERN); List&lt;String&gt; emails = Arrays.asList("alex@example.com", "bob@yahoo.com", "cat@google.com", "david@example.com"); for (String email : emails) &#123; Matcher matcher = pattern.matcher(email); if (matcher.matches()) &#123; log.info(email); &#125; &#125; &#125;&#125; 转换为Predicat： 123456789101112131415161718192021222324252627import java.util.Arrays;import java.util.List;import java.util.function.Predicate;import java.util.regex.Pattern;import java.util.stream.Collectors; public class RegexPredicateExample &#123; public static void main(String[] args) &#123; // Compile regex as predicate Predicate&lt;String&gt; emailFilter = Pattern .compile(EMAIL_PATTERN) .asPredicate(); // Input list List&lt;String&gt; emails = Arrays.asList("alex@example.com", "bob@yahoo.com", "cat@google.com", "david@example.com"); // Apply predicate filter List&lt;String&gt; desiredEmails = emails .stream() .filter(emailFilter) .collect(Collectors.toList()); // Now perform desired operation desiredEmails.forEach(log::info); &#125;&#125; FunctionFunction 函数式接口的作用是，我们可以为其提供一个原料，他给生产一个最终的产品。通过它提供的默认方法，组合,链行处理(compose, andThen)： 1234Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf);backToString.apply("123"); // "123" OptionalOptional是Java8提供的为了解决null安全问题的一个API。善用Optional可以使我们代码中很多繁琐、丑陋的设计变得十分优雅。这篇文章是建立在你对Optional的用法有一定了解的基础上的，如果你还不太了解Optional，可以先去看看相关教程，或者查阅Java文档。 使用Optional，我们就可以把下面这样的代码进行改写。 12345public static String getName(User u) &#123; if (u == null) return "Unknown"; return u.name;&#125; 不过，千万不要这样改 123456public static String getName(User u) &#123; Optional&lt;User&gt; user = Optional.ofNullable(u); if (!user.isPresent()) return "Unknown"; return user.get().name;&#125; 这样改写非但不简洁，而且其操作还是和第一段代码一样。无非就是用isPresent方法来替代u==null。这样的改写并不是Optional正确的用法，我们再来改写一次。 12345public static String getName(User u) &#123; return Optional.ofNullable(u) .map(user-&gt;user.name) .orElse("Unknown");&#125; 这样才是正确使用Optional的姿势。那么按照这种思路，我们可以安心的进行==链式调用==，而不是一层层判断了。 123456789101112public static String getChampionName(Competition comp) throws IllegalArgumentException &#123; if (comp != null) &#123; CompResult result = comp.getResult(); if (result != null) &#123; User champion = result.getChampion(); if (champion != null) &#123; return champion.getName(); &#125; &#125; &#125; throw new IllegalArgumentException("The value of param comp isn't available.");&#125; 由于种种原因（比如：比赛还没有产生冠军、方法的非正常调用、某个方法的实现里埋藏的大礼包等等），我们并不能开心的一路comp.getResult().getChampion().getName(到底。而其他语言比如kotlin，就提供了在语法层面的操作符加持：comp?.getResult()?.getChampion()?.getName()。 让我们看看经过Optional加持过后，这些代码会变成什么样子。 1234567public static String getChampionName(Competition comp) throws IllegalArgumentException &#123; return Optional.ofNullable(comp) .map(c-&gt;c.getResult()) .map(r-&gt;r.getChampion()) .map(u-&gt;u.getName()) .orElseThrow(()-&gt;new IllegalArgumentException("The value of param comp isn't available."));&#125; Optional给了我们一个真正优雅的Java风格的方法来解决null安全问题。虽然没有直接提供一个操作符写起来短，但是代码看起来依然很爽很舒服。更何况?.这样的语法好不好看还见仁见智呢。 还有很多不错的使用姿势，比如为空则不打印可以这么写： 1string.ifPresent(System.out::println); Optional的魅力还不止于此，Optional还有一些神奇的用法，比如Optional可以用来检验参数的合法性。 1234public void setName(String name) throws IllegalArgumentException &#123; this.name = Optional.ofNullable(name).filter(User::isNameValid) .orElseThrow(()-&gt;new IllegalArgumentException("Invalid username."));&#125; 123456Optional&lt;Integer&gt; canBeEmpty1 = Optional.of(5);canBeEmpty1.isPresent(); // returns truecanBeEmpty1.get(); // returns 5 Optional&lt;Integer&gt; canBeEmpty2 = Optional.empty();canBeEmpty2.isPresent(); // returns false 可以将Optional看作单值容器，要么包含一个值，要么不包含。注意，Optional并不能完全替代所有的null引用，它的主要作用是用来设计更易理解的API，==当一个函数返回的是Optional的时候，调用者就会注意需要检测其是否有值才能够调用==。 创建Optional对象 主要有三种创建Optional对象的方法 1).使用 Optional.empty() 创建一个空的Optional Optional&lt;Integer&gt; possible = Optional.empty(); 2). 使用Optional.of() 创建一个非null的值。 Optional&lt;Integer&gt; possible = Optional.of(5); 3). 使用Optional.ofNullable() 创建一个可以包含空值的Optional对象，如果参数是null，则返回一个空的Optional Optional&lt;Integer&gt; possible = Optional.ofNullable(5); 处理Optional的值 当得到一个Optional的值的时候，首先需要检查其是否包含了值 Optional&lt;Integer&gt; possible = Optional.of(5); possible.ifPresent(System.out::println); 当然上面的代码也可以通过下面的方式重写： if(possible.isPresent()){ System.out.println(possible.get()); } 不过这么做并不推荐，因为这个处理null没啥区别。推荐使用上面的代码返回Optional 一般来讲对于方法如果最终需要返回null的时候，都会返回一个默认值以便替换null，使用Optional可以使用如下的方式返回 Optional&lt;Company&gt; companyOptional = Optional.empty(); 检查Optional，如果有值直接返回，否则返回 new Company(); Company company = companyOptional.orElse(new Company()); 检查Optional，如果有值直接返回，否则返回 抛出异常; Company company = companyOptional.orElseThrow(IllegalStateException::new); 在Filter中使用Optional 例如： Optional&lt;Company&gt; companyOptional = Optional.empty(); companyOptional.filter(department -&gt; &quot;Finance&quot;.equals(department.getName()) .ifPresent(() -&gt; System.out.println(&quot;Finance is present&quot;));) filter 函数接受一个Predicate接口作为参数，如果Optional中存在值，那么就会匹配直接输出。 结论 Optional的目的并不是要完全替代null引用，而是为了设计更友好的API而设计的。 Supplier 生产者Supplier 与 Function 不同，它不接受入参，直接为我们生产一个指定的结果，有点像生产者模式： 1234567891011class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 12Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person Consumer 消费者对于 Consumer，我们需要提供入参，用来被消费，如下面这段示例代码： 12345678910111213141516class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125;``` ```javaConsumer&lt;Person&gt; greeter = (p) -&gt; System.out.println("Hello, " + p.firstName);greeter.accept(new Person("Luke", "Skywalker")); ComparatorComparator 在 Java 8 之前是使用比较普遍的。Java 8 中除了将其升级成了函数式接口，还为它拓展了一些默认方法： 1234567Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person("John", "Doe");Person p2 = new Person("Alice", "Wonderland");comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0 Stream 流这一章节，我们开始步入学习 Stream 流。 什么是 Stream 流？ 简单来说，我们可以使用 java.util.Stream 对一个包含一个或多个元素的集合做各种操作。这些操作可能是 中间操作 亦或是 终端操作。终端操作会返回一个结果，而中间操作会返回一个 Stream 流。 需要注意的是，你只能对实现了 java.util.Collection 接口的类做流的操作。 Map 不支持 Stream 流。 Stream 流支持同步执行，也支持并发执行。 Filter 过滤首先，我们创建一个 List 集合： 123456789List&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add("ddd2");stringCollection.add("aaa2");stringCollection.add("bbb1");stringCollection.add("aaa1");stringCollection.add("bbb3");stringCollection.add("ccc");stringCollection.add("bbb2");stringCollection.add("ddd1"); Filter 的入参是一个 Predicate, 上面已经说到，Predicate 是一个断言的中间操作，它能够帮我们筛选出我们需要的集合元素。它的返参同样是一个 Stream 流，我们可以通过 foreach 终端操作，来打印被筛选的元素： 123456stringCollection .stream() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// "aaa2", "aaa1" 注意：foreach 是一个终端操作，它的返参是 void, 我们无法对其再次进行流操作。 Sorted 排序Sorted 同样是一个中间操作，它的返参是一个 Stream 流。另外，我们可以传入一个 Comparator 用来自定义排序，如果不传，则使用默认的自然排序。 // resourceDOS是一个List,实体和list定义省略// 根据修改时间排序123resourceDOS.stream().sorted(Comparator.comparing(ResourceDO::getUpdateTime()).collect(Collectors.toList());resourceDOS.stream().sorted(Comparator.comparing(e -&gt; e.getUpdateTime()).collect(Collectors.toList()); // 使用Comparator.reversed()可以反转一个排序1resourceDOS.stream().sorted(Comparator.comparing(ResourceDO::getUpdateTime().reversed()).collect(Collectors.toList()); // 先根据updateTime排序，再根据createTime排序，使用thenComparing()1resourceDOS.stream().sorted(Comparator.comparing(ResourceDO::getUpdateTime().thenComparing(ResourceDO::getCreateTime)).collect(Collectors.toList()); // 自然排序1234567stringCollection .stream() .sorted() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// "aaa1", "aaa2" 并行排序,只需要使用 parallelSort() 即可进行并行排序。1234//Parallel SortingResourceDO[] resourceDOArray = resourceDOS.toArray(new ResourceDO[resourceDOS.size()]);Arrays.parallelSort(resourceDOArray, Comparator.comparing(ResourceDO::getUpdateTime().thenComparing(ResourceDO::getCreateTime));System.out.println(resourceDOArray); 需要注意，sorted 不会对 stringCollection 做出任何改变，stringCollection 还是原有的那些元素，且顺序不变，需要使用排序后的就得重新赋值或者保存到新的集合： 12System.out.println(stringCollection);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 count() 和 counting() 统计数量count 是一个终端操作，它能够统计 stream 流中的元素总数，返回值是 long 类型。 12345678// 先对 list 中字符串开头为 b 进行过滤，然后统计数量long startsWithB = stringCollection .stream() .filter((s) -&gt; s.startsWith("b")) .count();System.out.println(startsWithB); // 3 和counting()的区别是，count()是一个终端操作，对Stream计数并返回一个long，counting()则是Collector`的收集方法,用法区别如下： 使用counting() 1234567891011public static void main(String[] args)&#123; long count = Stream.of("how","to","do","in","java").collect(Collectors.counting()); System.out.printf("There are %d elements in the stream %n", count); count = Stream.of(1,2,3,4,5,6,7,8,9).collect(Collectors.counting()); System.out.printf("There are %d elements in the stream %n", count); count = Stream.of(1,2,3,4,5,6,7,8,9).filter(i -&gt; i%2 == 0).collect(Collectors.counting()); System.out.printf("There are %d elements in the stream %n", count);&#125; Map 转换(将Stream中的元素从一种形式转化为另一种形式)中间操作 Map 能够帮助我们将 List 中的每一个元素做功能处理。例如下面的示例，通过 map 我们将每一个 string 转成大写： 1234567stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println);// "DDD2", "DDD1", "CCC", "BBB3", "BBB2", "AAA2", "AAA1" 另外，我们还可以做对象之间的转换，业务中比较常用的是将 DO（数据库对象） 转换成 VO（表现对象） 。 Match 匹配顾名思义，match 用来做匹配操作，它的返回值是一个 boolean 类型。通过 match, 我们可以方便的验证一个 list 中是否存在某个类型的元素。 1234567891011121314151617181920212223// 验证 list 中 string 是否有以 a 开头的, 匹配到第一个，即返回 trueboolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -&gt; s.startsWith("a"));System.out.println(anyStartsWithA); // true// 验证 list 中 string 是否都是以 a 开头的boolean allStartsWithA = stringCollection .stream() .allMatch((s) -&gt; s.startsWith("a"));System.out.println(allStartsWithA); // false// 验证 list 中 string 是否都不是以 z 开头的,boolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -&gt; s.startsWith("z"));System.out.println(noneStartsWithZ); // true collect()转换为Collectioncollect() 收集stream中的元素转化为collection 123List&lt;String&gt; memNamesInUppercase = stringCollection.stream().sorted() .map(String::toUpperCase) .collect(Collectors.toList()); 原生类型转换对于原生类型不能直接使用这些方法，可以通过使用boxed()方法做中间处理，例如： 123List&lt;Integer&gt; ints = IntStream.of(1,2,3,4,5) .boxed() .collect(Collectors.toList()); 也可以使用mapToLong(),mapToDouble,mapToObj()方法来实现例如： 123List&lt;Integer&gt; ints = IntStream.of(1,2,3,4,5) .mapToObj(Integer::valueOf) .collect(Collectors.toList()); 对于LongStream和DoubleStream都有相同的方法 1234567List&lt;Long&gt; longs = LongStream.of(1l,2l,3l,4l,5l) .boxed() .collect(Collectors.toList()); List&lt;Double&gt; doubles = DoubleStream.of(1d,2d,3d,4d,5d) .boxed() .collect(Collectors.toList()); 同样可以利用Stream来生成随机数 1234567891011121314151617181920public class RandomNumberExample&#123; public static void main(String[] args) &#123; Random random = new Random(); //Five random integers random.ints( 5 ).sorted().forEach( System.out::println ); //Five random doubles between 0 (inclusive) and 0.5 (exclusive) random.doubles( 5, 0, 0.5 ).sorted().forEach( System.out::println ); //Boxing long to Long so they can be collected List&lt;Long&gt; longs = random.longs( 5 ).boxed().collect( Collectors.toList() ); System.out.println(longs); &#125;&#125; 输出： 1234567891011121314-1106798351565075181493532761205422799920867986567.724902934355127E-40.26881486593583550.35253878003241650.39474374755213280.4903926740688234[-2722967875761490425, 5531368074516893531, -8225300378125625898, -9067114159502460546, -5286357091370510421] toArray()转换为数组1int[] intArray = IntStream.of(1, 2, 3, 4, 5).toArray(); foreach()其作用是遍历stream所有的元素 1memberNames.forEach(System.out::println); 中间操作中间操作返回的是Stream本身，所以多个中间操作可以作为处理链处理Stream 终止操作终止操作会返回一个特定的类型，而不是返回Stream本身。 reduce()Reduce 中文翻译为：减少、缩小。通过入参的 Function，我们能够将 list 归约成一个值。它的返回类型是 Optional 类型。 12345678Optional&lt;String&gt; reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -&gt; s1 + "#" + s2);reduced.ifPresent(System.out::println);// "aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2" distinct()可以使用Stream.distinct()来过滤相同元素，例如：1234567Collection&lt;String&gt; list = Arrays.asList("A", "B", "C", "D", "A", "B", "C"); // Get collection without duplicated element distinct onlyList&lt;String&gt; distinctElements = list.stream().distinct().collect(Collectors.toList()); //Let's verify distinct elementsSystem.out.println(distinctElements); 通过distinct()函数过滤非常的简单容易，但是大多数情况下，处理的对象多会比较复杂，例如包含各种属性等，很少会只处理原生类型和字符串。 我们可以通过定制一个 Predicate的接口来处理复杂对象，例如： 12345public static &lt;T&gt; Predicate&lt;T&gt; distinctByKey(Function&lt;? super T, Object&gt; keyExtractor)&#123; Map&lt;Object, Boolean&gt; map = new ConcurrentHashMap&lt;&gt;(); return t -&gt; map.putIfAbsent(keyExtractor.apply(t), Boolean.TRUE) == null;&#125; 这个方法非常的简单，通过ConcurrentHashMap来确定是否包含某个元素，其接受一个函数式对象的引用。下面我们来使用这个方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.util.Arrays;import java.util.Collection;import java.util.List;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.function.Function;import java.util.function.Predicate;import java.util.stream.Collectors; public class JavaStreamDistinctExamples&#123; public static void main(String[] args) &#123; Person lokesh = new Person(1, "Lokesh", "Gupta"); Person brian = new Person(2, "Brian", "Clooney"); Person alex = new Person(3, "Alex", "Kolen"); //Add some random persons Collection&lt;Person&gt; list = Arrays.asList(lokesh,brian,alex,lokesh,brian,lokesh); // Get distinct only List&lt;Person&gt; distinctElements = list.stream().filter(distinctByKey(p -&gt; p.getId())).collect(Collectors.toList()); // Let's verify distinct elements System.out.println(distinctElements); &#125; public static &lt;T&gt; Predicate&lt;T&gt; distinctByKey(Function&lt;? super T, Object&gt; keyExtractor) &#123; Map&lt;Object, Boolean&gt; map = new ConcurrentHashMap&lt;&gt;(); return t -&gt; map.putIfAbsent(keyExtractor.apply(t), Boolean.TRUE) == null; &#125;&#125; class Person&#123; public Person(Integer id, String fname, String lname) &#123; super(); this.id = id; this.fname = fname; this.lname = lname; &#125; private Integer id; private String fname; private String lname; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getFname() &#123; return fname; &#125; public void setFname(String fname) &#123; this.fname = fname; &#125; public String getLname() &#123; return lname; &#125; public void setLname(String lname) &#123; this.lname = lname; &#125; @Override public String toString() &#123; return "Person [id=" + id + ", fname=" + fname + ", lname=" + lname + "]"; &#125;&#125; max() 和 min()对于max和min，只需要给他们传递一个Comparator，就可以进行比较，从而获取最大和最小值。 获取最大，最小得日期123456789101112LocalDate start = LocalDate.now();LocalDate end = LocalDate.now().plusMonths(1).with(TemporalAdjusters.lastDayOfMonth());List&lt;LocalDate&gt; dates = Stream.iterate(start, date -&gt; date.plusDays(1)) .limit(ChronoUnit.DAYS.between(start, end)) .collect(Collectors.toList()); // Get Min or Max DateLocalDate maxDate = dates.stream().max( Comparator.comparing( LocalDate::toEpochDay ) ).get();LocalDate minDate = dates.stream().min( Comparator.comparing( LocalDate::toEpochDay ) ).get(); System.out.println("maxDate = " + maxDate);System.out.println("minDate = " + minDate); 获取最大数和最小数 1234567Integer maxNumber = Stream.of(1, 2, 3, 4, 5, 6, 7, 8, 9) .max(Comparator.comparing(Integer::valueOf)).get();Integer minNumber = Stream.of(1, 2, 3, 4, 5, 6, 7, 8, 9) .min(Comparator.comparing(Integer::valueOf)).get(); System.out.println("maxNumber = " + maxNumber);System.out.println("minNumber = " + minNumber); 获取最大的char或者String12345String maxChar = Stream.of("H", "T", "D", "I", "J").max(Comparator.comparing(String::valueOf)).get();String minChar = Stream.of("H", "T", "D", "I", "J").min(Comparator.comparing(String::valueOf)).get(); System.out.println("maxChar = " + maxChar);System.out.println("minChar = " + minChar); 获取最大和最小的对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859List&lt;Employee&gt; emps = new ArrayList&lt;Employee&gt;(); emps.add(new Employee(1, "Lokesh", 36));emps.add(new Employee(2, "Alex", 46));emps.add(new Employee(3, "Brian", 52)); Comparator&lt;Employee&gt; comparator = Comparator.comparing(Employee::getAge); // Get Min or Max ObjectEmployee minObject = emps.stream().min(comparator).get();Employee maxObject = emps.stream().max(comparator).get(); System.out.println("minObject = " + minObject);System.out.println("maxObject = " + maxObject);class Employee &#123; private int id; private String name; private int age; public Employee(int id, String name, int age) &#123; super(); this.id = id; this.name = name; this.age = age; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; StringBuilder str = null; str = new StringBuilder(); str.append("Id:- " + getId() + " Name:- " + getName() + " Age:- " + getAge()); return str.toString(); &#125;&#125; Parallel-Streams 并行流前面章节我们说过，stream 流是支持顺序和并行的。顺序流操作是单线程操作，而并行流是通过多线程来处理的，能够充分利用物理机多核 CPU 的优势，同时处理速度更快。 首先，我们创建一个包含 1000000 UUID list 集合。 123456int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max);for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125; 分别通过顺序流和并行流，对这个 list 进行排序，测算耗时: 顺序流排序12345678910111213// 纳秒long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();// 纳秒转微秒long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("顺序流排序耗时: %d ms", millis));// 顺序流排序耗时: 899 ms 并行流排序12345678910111213// 纳秒long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();// 纳秒转微秒long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("并行流排序耗时: %d ms", millis));// 并行流排序耗时: 472 ms 正如你所见，同样的逻辑处理，通过并行流，我们的性能提升了近 50%。完成这一切，我们需要做的仅仅是将 stream 改成了 parallelStream。 Map 集合前面已经提到过 Map 是不支持 Stream 流的，因为 Map 接口并没有像 Collection 接口那样，定义了 stream() 方法。但是，我们可以对其 key, values, entry 使用流操作，如 map.keySet().stream(), map.values().stream() 和 map.entrySet().stream(). 另外, JDK 8 中对 map 提供了一些其他新特性: 12345678910Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; // 与老版不同的是，putIfAbent() 方法在 put 之前， // 会判断 key 是否已经存在，存在则直接返回 value, 否则 put, 再返回 value map.putIfAbsent(i, "val" + i);&#125;// forEach 可以很方便地对 map 进行遍历操作map.forEach((key, value) -&gt; System.out.println(value)); 除了上面的 putIfAbsent() 和 forEach() 外，我们还可以很方便地对某个 key 的值做相关操作： 1234567891011121314151617// computeIfPresent(), 当 key 存在时，才会做相关处理// 如下：对 key 为 3 的值，内部会先判断值是否存在，存在，则做 value + key 的拼接操作map.computeIfPresent(3, (num, val) -&gt; val + num);map.get(3); // val33// 先判断 key 为 9 的元素是否存在，存在，则做删除操作map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // false// computeIfAbsent(), 当 key 不存在时，才会做相关处理// 如下：先判断 key 为 23 的元素是否存在，不存在，则添加map.computeIfAbsent(23, num -&gt; "val" + num);map.containsKey(23); // true// 先判断 key 为 3 的元素是否存在，存在，则不做任何处理map.computeIfAbsent(3, num -&gt; "bam");map.get(3); // val33 关于删除操作，JDK 8 中提供了能够新的 remove() API: 12345map.remove(3, "val3");map.get(3); // val33map.remove(3, "val33");map.get(3); // null 如上代码，只有当给定的 key 和 value 完全匹配时，才会执行删除操作。 关于添加方法，JDK 8 中提供了带有默认值的 getOrDefault() 方法： 12// 若 key 42 不存在，则返回 not foundmap.getOrDefault(42, "not found"); // not found 对于 value 的合并操作也变得更加简单： 1234567// merge 方法，会先判断进行合并的 key 是否存在，不存在，则会添加元素map.merge(9, "val9", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9// 若 key 的元素存在，则对 value 执行拼接操作map.merge(9, "concat", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9concat 新的日期 APIJava 8 中在包 java.time 下添加了新的日期 API. 它和 Joda-Time 库相似，但又不完全相同。接下来，我会通过一些示例代码介绍一下新 API 中最关键的特性： ClockClock 提供对当前日期和时间的访问。我们可以利用它来替代 System.currentTimeMillis() 方法。另外，通过 clock.instant() 能够获取一个 instant 实例，此实例能够方便地转换成老版本中的 java.util.Date 对象。 12345Clock clock = Clock.systemDefaultZone();long millis = clock.millis();Instant instant = clock.instant();Date legacyDate = Date.from(instant); // 老版本 java.util.Date Timezones 时区ZoneId 代表时区类。通过静态工厂方法方便地获取它，入参我们可以传入某个时区编码。另外，时区类还定义了一个偏移量，用来在当前时刻或某时间与目标时区时间之间进行转换。 12345678910System.out.println(ZoneId.getAvailableZoneIds());// prints all available timezone idsZoneId zone1 = ZoneId.of("Europe/Berlin");ZoneId zone2 = ZoneId.of("Brazil/East");System.out.println(zone1.getRules());System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=+01:00]// ZoneRules[currentStandardOffset=-03:00] LocalTimeLocalTime 表示一个没有指定时区的时间类，例如，10 p.m.或者 17：30:15，下面示例代码中，将会使用上面创建的时区对象创建两个 LocalTime。然后我们会比较两个时间，并计算它们之间的小时和分钟的不同。 12345678910LocalTime now1 = LocalTime.now(zone1);LocalTime now2 = LocalTime.now(zone2);System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239 LocalTime 提供多个静态工厂方法，目的是为了简化对时间对象实例的创建和操作，包括对时间字符串进行解析的操作等。 12345678910LocalTime late = LocalTime.of(23, 59, 59);System.out.println(late); // 23:59:59DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse("13:37", germanFormatter);System.out.println(leetTime); // 13:37 LocalDateLocalDate 是一个日期对象，例如：2014-03-11。它和 LocalTime 一样是个 final 类型对象。下面的例子演示了如何通过加减日，月，年等来计算一个新的日期。 LocalDate, LocalTime, 因为是 final 类型的对象，每一次操作都会返回一个新的时间对象。 12345678910LocalDate today = LocalDate.now();// 今天加一天LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);// 明天减两天LocalDate yesterday = tomorrow.minusDays(2);// 2014 年七月的第四天LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println(dayOfWeek); // 星期五 也可以直接解析日期字符串，生成 LocalDate 实例。（和 LocalTime 操作一样简单） 1234567DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN);LocalDate xmas = LocalDate.parse("24.12.2014", germanFormatter);System.out.println(xmas); // 2014-12-24 LocalDateTimeLocalDateTime 是一个日期-时间对象。你也可以将其看成是 LocalDate 和 LocalTime 的结合体。操作上，也大致相同。 LocalDateTime 同样是一个 final 类型对象。 1234567891011LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // 星期三Month month = sylvester.getMonth();System.out.println(month); // 十二月// 获取改时间是该天中的第几分钟long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439 如果再加上的时区信息，LocalDateTime 还能够被转换成 Instance 实例。Instance 能够被转换成老版本中 java.util.Date 对象。 123456Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 格式化 LocalDateTime 对象就和格式化 LocalDate 或者 LocalTime 一样。除了使用预定义的格式以外，也可以自定义格式化输出。 1234567DateTimeFormatter formatter = DateTimeFormatter .ofPattern("MMM dd, yyyy - HH:mm");LocalDateTime parsed = LocalDateTime.parse("Nov 03, 2014 - 07:13", formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13 注意：和 java.text.NumberFormat 不同，新的 DateTimeFormatter 类是 final 类型的，同时也是线程安全的。更多细节请查看这里 Annotations 注解在 Java 8 中，注解是可以重复的。让我通过下面的示例代码，来看看到底是咋回事。 首先，我们定义一个包装注解，里面包含了一个有着实际注解的数组： 12345678@interface Hints &#123; Hint[] value();&#125;@Repeatable(Hints.class)@interface Hint &#123; String value();&#125; Java 8 中，通过 @Repeatable，允许我们对同一个类使用多重注解： 第一种形态：使用注解容器（老方法） 12@Hints(&#123;@Hint("hint1"), @Hint("hint2")&#125;)class Person &#123;&#125; 第二种形态：使用可重复注解（新方法） 123@Hint("hint1")@Hint("hint2")class Person &#123;&#125; 使用第二种形态，Java 编译器能够在内部自动对 @Hint 进行设置。这对于需要通过反射来读取注解信息时，是非常重要的。 12345678Hint hint = Person.class.getAnnotation(Hint.class);System.out.println(hint); // nullHints hints1 = Person.class.getAnnotation(Hints.class);System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);System.out.println(hints2.length); // 2 尽管我们绝对不会在 Person 类上声明 @Hints 注解，但是它的信息仍然是可以通过 getAnnotation(Hints.class) 来读取的。并且，getAnnotationsByType 方法会更方便，因为它赋予了所有 @Hints 注解标注的方法直接的访问权限。 12@Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125; 结语Java 8 新特性的编程指南到此就告一段落了。当然，还有很多内容需要进一步研究和说明。这就需要靠读者您来对 JDK 8 进一步探究了，例如：Arrays.parallelSort, StampedLock 和 CompletableFuture 等等，我这里也仅是起到抛砖引玉的作用而已。 最后，我希望这个讲解能够对您有所帮助，也希望您阅读愉快。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cron]]></title>
    <url>%2F2017%2F12%2F05%2FCron%2F</url>
    <content type="text"><![CDATA[cron表达式格式 {秒数} {分钟} {小时} {日期} {月份} {星期} {年份(可为空)} 先了解每个位置代表的含义，再了解每个位置允许的范围，以及一些特殊写法，还有常用的案例，足够你掌握cron表达式 每个字段的允许值 字段 描述 允许值 允许的特殊字符 Seconds 秒 0-59 , - * / Minutes 分 0-59 , - * / Hours 小时 0-23 , - * / Day-of-Month 日期 1-31 , - * ? / L W C Month 月份 1-12 或者 JAN-DEC , - * / Day-of-Week 星期 1-7 或者 SUN-SAT , - * ? / L C # Year 年（可选） 留空, 1970-2099 , - * / 秒允许值范围: 0~59 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异常 * 代表每隔1秒钟触发 , 代表在指定的秒数触发，比如”0,15,45”代表0秒、15秒和45秒时触发任务 - 代表在指定的范围内触发，比如”25-45”代表从25秒开始触发到45秒结束触发，每隔1秒触发1次 / 代表触发步进(step)，”/“前面的值代表初始值(“*“等同”0”)，后面的值代表偏移量，比如”0/20”或者”*/20”代表从0秒钟开始，每隔20秒钟触发1次，即0秒触发1次，20秒触发1次，40秒触发1次；”5/20”代表5秒触发1次，25秒触发1次，45秒触发1次；”10-45/20”代表在[10,45]内步进20秒命中的时间点触发，即10秒触发1次，30秒触发1次 分钟允许值范围: 0~59 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异常 * 代表每隔1分钟触发 , 代表在指定的分钟触发，比如”10,20,40”代表10分钟、20分钟和40分钟时触发任务 -代表在指定的范围内触发，比如”5-30”代表从5分钟开始触发到30分钟结束触 发，每隔1分钟触发 /代表触发步进(step)，”/“前面的值代表初始值(““等同”0”)，后面的值代表偏移量，比如”0/25”或者”/25”代表从0分钟开始，每隔25分钟触发1次，即0分钟触发1次，第25分钟触发1次，第50分钟触发1次；”5/25”代表5分钟触发1次，30分钟触发1次，55分钟触发1次；”10-45/20”代表在[10,45]内步进20分钟命中的时间点触发，即10分钟触发1次，30分钟触发1次 小时允许值范围: 0~23 ,不允许为空值，若值不合法，调度器将抛出SchedulerException异常 * 代表每隔1小时触发 , 代表在指定的时间点触发，比如”10,20,23”代表10点钟、20点钟和23点触发任务 - 代表在指定的时间段内触发，比如”20-23”代表从20点开始触发到23点结束触发，每隔1小时触发 / 代表触发步进(step)，”/“前面的值代表初始值(““等同”0”)，后面的值代表偏移量，比如”0/1”或者”/1”代表从0点开始触发，每隔1小时触发1次；”1/2”代表从1点开始触发，以后每隔2小时触发一次 日期允许值范围: 1~12 (JAN-DEC),不允许为空值，若值不合法，调度器将抛出SchedulerException异常 * 代表每个月都触发 , 代表在指定的月份触发，比如”1,6,12”代表1月份、6月份和12月份触发任务 - 代表在指定的月份范围内触发，比如”1-6”代表从1月份开始触发到6月份结束触发，每隔1个月触发 / 代表触发步进(step)，”/“前面的值代表初始值(““等同”1”)，后面的值代表偏移量，比如”1/2”或者”/2”代表从1月份开始触发，每隔2个月触发1次；”6/6”代表从6月份开始触发，以后每隔6个月触发一次；”1-6/12”表达式意味着每年1月份触发 星期允许值范围: 1~7 (SUN-SAT),1代表星期天(一星期的第一天)，以此类推，7代表星期六(一星期的最后一天)，不允许为空值，若值不合法，调度器将抛出SchedulerException异常 * 代表每星期都触发； ? 与{日期}互斥，即意味着若明确指定{日期}触发，则表示{星期}无意义，以免引起冲突和混乱 , 代表在指定的星期约定触发，比如”1,3,5”代表星期天、星期二和星期四触发 - 代表在指定的星期范围内触发，比如”2-4”代表从星期一开始触发到星期三结束触发，每隔1天触发 / 代表触发步进(step)，”/“前面的值代表初始值(““等同”1”)，后面的值代表偏移量，比如”1/3”或者”/3”代表从星期天开始触发，每隔3天触发1次；”1-5/2”表达式意味着在[1,5]范围内，每隔2天触发，即星期天、星期二、星期四触发 L 如果{星期}占位符如果是”L”，即意味着星期的的最后一天触发，即星期六触发，L= 7或者 L = SAT，因此，”5L”意味着一个月的最后一个星期四触发 # 用来指定具体的周数，”#”前面代表星期，”#”后面代表本月第几周，比如”2#2”表示本月第二周的星期一，”5#3”表示本月第三周的星期四，因此，”5L”这种形式只不过是”#”的特殊形式而已 年份允许值范围: 1970~2099 ,允许为空，若值不合法，调度器将抛出SchedulerException异常 * 代表每年都触发 , 代表在指定的年份才触发，比如”2011,2012,2013”代表2011年、2012年和2013年触发任务 - 代表在指定的年份范围内触发，比如”2011-2020”代表从2011年开始触发到2020年结束触发，每隔1年触发 / 代表触发步进(step)，”/“前面的值代表初始值(““等同”1970”)，后面的值代表偏移量，比如”2011/2”或者”/2”代表从2011年开始触发，每隔2年触发1次 注意： 除了{日期}和{星期}可以使用”?”来实现互斥，表达无意义的信息之外，其他占位符都要具有具体的时间含义，且依赖关系为：年-&gt;月-&gt;日期(星期)-&gt;小时-&gt;分钟-&gt;秒数 特殊字符** 字符被用来指定所有的值。如：”*”在分钟的字段域里表示“每分钟”。 ?? 字符只在日期域和星期域中使用。它被用来指定“非明确的值”。当你需要通过在这两个域中的一个来指定一些东西的时候，它是有用的。看下面的例子你就会明白。 月份中的日期和星期中的日期这两个元素时互斥的一起应该通过设置一个问号来表明不想设置那个字段。 -- 字符被用来指定一个范围。如：“10-12”在小时域意味着“10点、11点、12点”。 ,, 字符被用来指定另外的值。如：“MON,WED,FRI”在星期域里表示”星期一、星期三、星期五”。 // 字符用于指定增量。如：“0/15”在秒域意思是每分钟的0，15，30和45秒。“5/15”在分钟域表示每小时的5，20，35和50。符号“”在“/”前面（如：/10）等价于0在“/”前面（如：0/10）。记住一条本质：表达式的每个数值域都是一个有最大值和最小值的集合，如：秒域和分钟域的集合是0-59，日期域是1-31，月份域是1-12。字符“/”可以帮助你在每个字符域中取相应的数值。如：“7/6”在月份域的时候只有当7月的时候才会触发，并不是表示每个6月。 LL 是‘last’的省略写法可以表示day-of-month和day-of-week域，但在两个字段中的意思不同，例如day-of-month域中表示一个月的最后一天。如果在day-of-week域表示‘7’或者‘SAT’，如果在day-of-week域中前面加上数字，它表示一个月的最后几天，例如‘6L’就表示一个月的最后一个星期五。 WW 字符“W”只允许日期域出现。这个字符用于指定日期的最近工作日。例如：如果你在日期域中写 “15W”，表示：这个月15号最近的工作日。所以，如果15号是周六，则任务会在14号触发。如果15好是周日，则任务会在周一也就是16号触发。如果是在日期域填写“1W”即使1号是周六，那么任务也只会在下周一，也就是3号触发，“W”字符指定的最近工作日是不能够跨月份的。字符“W”只能配合一个单独的数值使用，不能够是一个数字段，如：1-15W是错误的。 L和W可以在日期域中联合使用，LW表示这个月最后一周的工作日。 \# 字符“#”只允许在星期域中出现。这个字符用于指定本月的某某天。例如：“6#3”表示本月第三周的星期五（6表示星期五，3表示第三周）。“2#1”表示本月第一周的星期一。“4#5”表示第五周的星期三。 CC 字符“C”允许在日期域和星期域出现。这个字符依靠一个指定的“日历”。也就是说这个表达式的值依赖于相关的“日历”的计算结果，如果没有“日历”关联，则等价于所有包含的“日历”。如：日期域是“5C”表示关联“日历”中第一天，或者这个月开始的第一天的后5天。星期域是“1C”表示关联“日历”中第一天，或者星期的第一天的后1天，也就是周日的后一天（周一）。 一些cron表达式案例 /5 * ? 每隔5秒执行一次 0 /1 ? 每隔1分钟执行一次 0 0 5-15 ? 每天5-15点整点触发 0 0/3 * ? 每三分钟触发一次 0 0-5 14 ? 在每天下午2点到下午2:05期间的每1分钟触发 0 0/5 14 ? 在每天下午2点到下午2:55期间的每5分钟触发 0 0/5 14,18 ? 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发 0 0/30 9-17 ? 朝九晚五工作时间内每半小时 0 0 10,14,16 ? 每天上午10点，下午2点，4点 0 0 12 ? * WED 表示每个星期三中午12点 0 0 17 ? * TUES,THUR,SAT 每周二、四、六下午五点 0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发 0 15 10 ? * MON-FRI 周一至周五的上午10:15触发 0 0 23 L * ? 每月最后一天23点执行一次 0 15 10 L * ? 每月最后一日的上午10:15触发 0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发 0 15 10 ? 2005 2005年的每天上午10:15触发 0 15 10 ? * 6L 2002-2005 2002年至2005年的每月的最后一个星期五上午10:15触发 0 15 10 ? * 6#3 每月的第三个星期五上午10:15触发 “30 ?” 每半分钟触发任务 “30 10 * ?” 每小时的10分30秒触发任务 “30 10 1 ?” 每天1点10分30秒触发任务 “30 10 1 20 * ?” 每月20号1点10分30秒触发任务 “30 10 1 20 10 ? *” 每年10月20号1点10分30秒触发任务 “30 10 1 20 10 ? 2011” 2011年10月20号1点10分30秒触发任务 “30 10 1 ? 10 * 2011” 2011年10月每天1点10分30秒触发任务 “30 10 1 ? 10 SUN 2011” 2011年10月每周日1点10分30秒触发任务 “15,30,45 ?” 每15秒，30秒，45秒时触发任务 “15-45 ?” 15到45秒内，每秒都触发任务 “15/5 ?” 每分钟的每15秒开始触发，每隔5秒触发一次 “15-30/5 ?” 每分钟的15秒到30秒之间开始触发，每隔5秒触发一次 “0 0/3 * ?” 每小时的第0分0秒开始，每三分钟触发一次 “0 15 10 ? * MON-FRI” 星期一到星期五的10点15分0秒触发任务 “0 15 10 L * ?” 每个月最后一天的10点15分0秒触发任务 “0 15 10 LW * ?” 每个月最后一个工作日的10点15分0秒触发任务 “0 15 10 ? * 5L” 每个月最后一个星期四的10点15分0秒触发任务 “0 15 10 ? * 5#3” 每个月第三周的星期四的10点15分0秒触发任务 表达式生成器有很多的cron表达式在线生成器，这里给大家推荐几款http://www.bejson.com/othertools/cron/http://cron.qqe2.com/]]></content>
      <categories>
        <category>cron</category>
        <category>定时</category>
      </categories>
      <tags>
        <tag>quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis-MapperScannerConfigurer]]></title>
    <url>%2F2017%2F11%2F30%2FMybatis-MapperScannerConfigurer%2F</url>
    <content type="text"><![CDATA[每个现象背后都有其缘由，越离奇的bug越是由不起眼的细节引发，每个bug背后都有框架或代码运行的原理和机制所在，解决bug，不仅仅需要去网上查询，还需要对其背后的原理进行了解和总结。 同事大佬最近在学习并使用Mybatis，他使用Mybatis的MapperScannerConfigurer来进行相关配置，并希望通过yml配置来指定basePackage，mappers等属性。为此，编写了自定义的配置类 StarterAutoConfiguration和自定义属性类 TkProperties，并在初始化 MapperScannerConfigurer时使用 TkProperties中的属性。但是，事与愿违，在初始化 MapperScannerConfigurer时， TkProperties实例中的属性死活都是未初始化状态。 为此，我们花了大量时间探查缘由，最后不得不询问了另一位大佬，才发现这个离奇问题的背后竟然有着这样的缘由。 我们首先来看一下大佬关于 MapperScannerConfigurer的自定义配置实现。他首先定义了自定义配置类 BkStarterAutoConfiguration，使用 @EnableConfigurationProperties注解将 TkProperties声明为配置属性类。 MapperScannerConfigurer@Configuration @EnableConfigurationProperties({TkProperties.class}) @AutoConfigureBefore(MybatisAutoConfiguration.class)` public class BkStarterAutoConfiguration { @Bean @ConditionalOnMissingBean @Order(Ordered.HIGHEST_PRECEDENCE) public TkProperties tkProperties() { return new TkProperties();` } } 下面是 TkProperties的定义，使用 @ConfigurationProperties注解声明了该属性配置的前缀，两个属性名称为 basePackage和 mappers。 @Data @ConfigurationProperties(prefix = &quot;tk&quot;) public class TkProperties { private String basePackage; private String mappers; } MapperConfig是声明并配置 MapperScannerConfigurer实例的配置类，使用被 @Bean注解修饰的 mapperScannerConfigurer方法来初始化，其方法参数为 TkProperties。 @Configuration public class MapperConfig { @Bean public MapperScannerConfigurer mapperScannerConfigurer(TkProperties tkProperties) { MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setSqlSessionFactoryBeanName(&quot;sqlSessionFactory&quot;); //使用TkProperties的成员变量来配置mapperScannerConfigurer mapperScannerConfigurer.setBasePackage(tkProperties.getBasePackage()); Properties properties = new Properties(); properties.setProperty(&quot;mappers&quot;, tkProperties.getMappers()); mapperScannerConfigurer.setProperties(properties); return mapperScannerConfigurer; } } yml配置文件如下所示。 tk: basePackage: cn.remcarpediem.mybatis.dao mappers: cn.remcarpediem.mappers.BaseDao 代码乍看起来一定问题都没有，但是运行时，在初始化MapperScannerConfigurer实例时，TkProperties实例的属性死活就是没有初始化成功。 一定有很多见多识广的读者已经知道这个现象背后的原因。“凶手”就是 MapperScannerConfigurer实现的接口 BeanDefinitionRegistryPostProcessor。具体原因我们还需要慢慢来解释，因为它涉及了Spring Boot的很多原理。 首先， BeanDefinitionRegistryPostProcessor接口继承了 BeanFactoryPostProcessor接口，大家一般都对 BeanFactoryPostProcessor较为熟悉，它是实例工厂(BeanFactory)的后处理器(PostProcessor)，与之类似的还有实例的后处理器(BeanPostProcessor)。 BeanFactoryPostProcessor中只定义了一个方法，其将会在 ApplicationContext内部的 BeanFactory加载完 BeanDefinition后，但是在Bean实例化之前进行。所以通常我们可以通过实现该接口来对实例化之前的 BeanDefinition进行修改。比如说 PropertySourcesPlaceholderConfigurer就实现 BeanFactoryPostProcessor接口，用于处理实例中被 @Value注解修饰的变量，修改其数值。 public interface BeanFactoryPostProcessor { void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; } 而 BeanDefinitionRegistryPostProcessor接口扩展自 BeanFactoryPostProcessor，它是 BeanDefinitionRegistry的后处理器，它可以在 BeanFactoryPostProcessor检测之前注册一些特殊的 BeanDefinition，比如说可以注册用来定义 BeanFactoryPostProcessor的 BeanDefintion，比如说我们之前提到的 MapperScannerConfigurer和 ConfigurationClassPostProcessor。 public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor { void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException; } MapperScannerConfigurer的 postProcessBeanDefinitionRegistry主要用来 ClassPathMapperScanner来扫描 Mybatis的 Mapper。 ClassPathMapperScanner继承了 ClassPathBeanDefinitionScanner，在 doScan方法中获取了 basePackage指定的包路径下的所有 Mapper的 BeanDefinition，然后进行注册。 而 BeanPostProcessor就是Bean实例的后处理器。每个Bean实例在进行初始化前会调用其 postProcessBeforeInitialization方法和初始化之后调用其 postProcessAfterInitialization方法。 ConfigurationPropertiesBindingPostProcessor实现了 BeanPostProcessor接口，用于处理被 @ConfigurationProperties修饰的实例。 public interface BeanPostProcessor { Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; } 我们可以总结一下 BeanDefinitionRegistryPostProcessor， BeanFactoryPostProcessor和 BeanPostProcessor三个后处理器发挥作用的次序和时机。 由此，我们也能够理解为什么 MapperScannerConfigurer初始化时， TkProperties还没有初始化，那是因为 ConfigurationPropertiesBindingPostProcessor还没有初始化，并且也没有对 TkProperties进行处理。 遇到问题和bug，不要百度一下解决方案处理就结束了，而是要深入了解一下背后的机制和原理，希望大家都能够多多探索更加深入的原理，获得更多的知识。 $和#的区别#相当于对数据 加上 双引号，$相当于直接显示数据 # $ 相当于对数据加上双引号 相当于直接显示数据 很大程度上防止SQL注入 无法防止SQL注入 #{xxx},使用的是PreparedStatement,会有类型转换，比较安全 ${xxx}，使用字符串拼接，容易SQL注入 使用#{参数}传入会加上单引号，sql语句解析是会加上”” 比如 select * from table where name = #{name} ,传入的name为小李，那么最后打印出来的就是 select * from table where name = ‘小李’，就是会当成字符串来解析，这样相比于$的好处是比较明显对的吧，#{}传参能防止sql注入，如果你传入的参数为 单引号’，那么如果使用${},这种方式 那么是会报错的， ${} 另外一种场景是，如果你要做动态的排序，比如 order by column，这个时候务必要用${}, 因为如果你使用了#{},那么打印出来的将会是 select * from table order by &#39;name&#39; ,这样是没用。]]></content>
      <categories>
        <category>java</category>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>MapperScannerConfigurer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-SQL语句执行]]></title>
    <url>%2F2017%2F11%2F28%2FMysql-SQL%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[昔日庖丁解牛，未见全牛，所赖者是其对牛内部骨架结构的了解，对于MySQL亦是如此，只有更加全面地了解SQL语句执行的每个过程，才能更好的进行SQL的设计和优化。 当希望MySQL能够以更高的性能运行查询时，最好的办法就是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，很多查询优化工作实际上就是遵循一些原则能够按照预想的合理的方式运行。 如下图所示，当向MySQL发送一个请求的时候，MySQL到底做了什么： 客户端发送一条查询给服务器。 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。 MySQL根据优化器生成的执行计划，再调用存储引擎的API来执行查询。 将结果返回给客户端。 查询缓存 MySQL查询缓存保存查询返回的完整结构。当查询命中该缓存时，MySQL会立刻返回结果，跳过了解析、优化和执行阶段。 查询缓存系统会跟踪查询中涉及的每个表，如果这些表发生了变化，那么和这个表相关的所有缓存数据都将失效。 MySQL将缓存存放在一个引用表中，通过一个哈希值引用，这个哈希值包括了以下因素，即查询本身、当前要查询的数据库、客户端协议的版本等一些其他可能影响返回结果的信息。 当判断缓存是否命中时，MySQL不会进行解析查询语句，而是直接使用SQL语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中。 当查询语句中有一些不确定的数据时，则不会被缓存。例如包含函数NOW()或者CURRENT_DATE()的查询不会缓存。包含任何用户自定义函数，存储函数，用户变量，临时表，mysql数据库中的系统表或者包含任何列级别权限的表，都不会被缓存。 有一点需要注意，MySQL并不是会因为查询中包含一个不确定的函数而不检查查询缓存，因为检查查询缓存之前，MySQL不会解析查询语句，所以也无法知道语句中是否有不确定的函数。 事实则是，如果查询语句中包含任何的不确定的函数，那么其查询结果不会被缓存，因为查询缓存中也无法找到对应的缓存结果。 有关查询缓存的配置如下所示。 query_cache_type:是否打开查询缓存。可以设置为OFF、ON和DEMAND。DEMAND表示只有在查询语句中明确写明SQL_CACHE的语句才会放入查询缓存。 query_cache_size:查询缓存使用的总内存空间。 query_cache_min_res_unit:在查询缓存中分配内存块时的最小单元。较小的该值可以减少碎片导致的内存空间浪费，但是会导致更频繁的内存块操作。 query_cache_limit:MySQL能够查询的最大查询结果。如果查询结果大于这个值，则不会被缓存。因为查询缓存在数据生成的时候就开始尝试缓存数据，所以当结果全部返回后，MySQL才知道查询结果是否超出限制。超出之后，才会将结果从查询缓存中删除。 对查询缓存的优化是数据库性能优化的重要一环。判断流程大致如下图所示。 缓存命中率可以通过如下公式计算：Qcache_hits/(Qcache_hits + Com_select)来计算。 解析和预处理 解析器通过关键字将SQL语句进行解析，并生成对应的解析树。MySQL解析器将使用MySQL语法规则验证和解析查询。 预处理器则根据一些MySQL规则进行进一步检查解析书是否合法，例如检查数据表和数据列是否存在，还会解析名字和别名，看看它们是否有歧义。 查询优化器 查询优化器会将解析树转化成执行计划。一条查询可以有多种执行方法，最后都是返回相同结果。优化器的作用就是找到这其中最好的执行计划。 生成执行计划的过程会消耗较多的时间，特别是存在许多可选的执行计划时。如果在一条SQL语句执行的过程中将该语句对应的最终执行计划进行缓存，当相似的语句再次被输入服务器时，就可以直接使用已缓存的执行计划，从而跳过SQL语句生成执行计划的整个过程，进而可以提高语句的执行速度。 MySQL使用基于成本的查询优化器(Cost-Based Optimizer，CBO)。它会尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最少的一个。 优化器会根据优化规则对关系表达式进行转换，这里的转换是说一个关系表达式经过优化规则后会生成另外一个关系表达式，同时原有表达式也会保留，经过一系列转换后会生成多个执行计划，然后CBO会根据统计信息和代价模型(Cost Model)计算每个执行计划的Cost，从中挑选Cost最小的执行计划。由上可知，CBO中有两个依赖：统计信息和代价模型。统计信息的准确与否、代价模型的合理与否都会影响CBO选择最优计划。 有关优化器的原理十分复杂，这里就不进行详细讲解了，大家可以自行学习。 查询执行引擎 在解析和优化阶段，MySQL将生成查询对应的执行计划，MySQL的查询执行引擎根据这个执行计划来完成整个查询。这里执行计划是一个数据结构，而不是和其他的关系型数据库那样生成对应的字节码。 返回结果给客户端 如果查询可以被缓存，那么MySQL在这个阶段页会将结果存放到查询缓存中。 MySQL将结果集返回给客户端是一个增量、逐步返回的过程。在查询生成第一条结果时，MySQL就可以开始向客户端逐步返回结果集了。 参考 SQL优化器原理——查询优化器综述 https://zhuanlan.zhihu.com/p/40478975 《高性能MySQL》 《MySQL技术内幕-InnoDB存储引擎》]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>语句执行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-B-Tree索引]]></title>
    <url>%2F2017%2F11%2F28%2FMysql-B-Tree%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[MySQL是目前业界最为流行的关系型数据库之一，而索引的优化也是数据库性能优化的关键之一。所以，充分地了解MySQL索引有助于提升开发人员对MySQL数据库的使用优化能力。 MySQL的索引有很多种类型，可以为不同的场景提供更好的性能。而B-Tree索引是最为常见的MySQL索引类型，一般谈论MySQL索引时，如果没有特别说明，就是指B-Tree索引。本文就详细讲解一下B-Tree索引的的底层结构，使用原则和特性。 主要内容如下： B-Tree索引的底层结构 B-Tree索引的使用规则 聚簇索引 InnoDB和MyISAM引擎索引的差异 松散索引 覆盖索引 B-Tree索引B-Tree索引使用B-Tree来存储数据，当然不同存储引擎的实现方式不同。B-Tree通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同，下图展示了B-Tree索引的抽象表示，由此可以看出MySQL的B-Tree索引的大致工作机制。 B-Tree索引的底层数据结构一般是B+树，其具体数据结构和优势这里就不作详细描述，下图展示了B-树索引的抽象表示，大致反应了MyISAM索引是如何工作的，而InnoDB使用的结构有所不同。 MySQL可以在单独一列上添加B-Tree索引，也可以在多列数据上添加B-Tree索引，多列的数据按照添加索引声明的顺序组合起来，存储在B-Tree的页中。假设有如下数据表： 对于表中的每一行数据，索引中包含了last_name，first_name和birthday列的值，下图展示了该索引是如何组织数据的存储的。 B-Tree索引使用B-Tree作为其存储数据的数据结构，其使用的查询规则也由此决定。一般来说，B-Tree索引适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于根据最左前缀查找。B-Tree索引支持的查询原则如下所示： 全值匹配：全值匹配指的是和索引中的所有列进行匹配。 匹配最左前缀：前边提到的索引可以用于查找所有姓Allen的人，即只使用索引中的第一列。 匹配列前缀：也可以只匹配某一列的值的开头部分。例如前面提到的索引可用于查找所有以J开头的姓的人。这里也只用到了索引的第一列。 匹配范围值：例如前边提到的索引可用于查找姓在Allen和Barrymore之间的人。这里也只使用了索引的第一列。 精确匹配某一列并范围匹配另外一列：前边提到的索引也可用于查找所有姓为Allen，并且名字是字母K开头(比如Kim,Karl等)的人。即第一列last_name全匹配，第二列first_name范围匹配。 因为索引树的节点是有序的，所以除了按值查找之外，索引还可以用于查询中的ORDER BY操作(按顺序查找)，如果ORDER BY子句满足前面列出的几种查询类型，则这个索引也可以满足对应的排序需求。 下面是一些关于B-Tree索引的限制： 如果不是按照索引的最左列开始查找，则无法使用索引。例如上面例子中的索引无法查找名字为Bill的人，也无法查找某个特定生日的日，因为这两列都不是最左数据列。 如果查询中有某个列的范围查询，则其右侧所有列都无法使用索引优化查找。 聚簇索引聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。具体的细节依赖于其实现方式，但是InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引和数据行。 当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中，这也就是说数据行和相邻的键值紧凑地存储在一起。 下图展示了聚簇索引中的记录是如何存放的。注意到，叶子页包含了行的全部数据行，但是节点页只包含了索引列。 聚簇索引可能对性能有帮助，但也可能导致严重的性能问题。聚簇的数据是有一些重要的优点： 数据访问更快，聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 如果在设计表和查询时能充分利用上面的优点，那么就能极大地提升性能。同时，聚簇索引也有一些缺点： 插入顺序严重依赖插入顺序。按照主键的顺序插入是向InnoDB表中插入数据速度最快的方式，需要避免主键键值随机的(不连续且值得分布范围非常大)聚簇索引，比如使用UUID作为主键，而应该使用类似AUTO_INCREMENT的自增列。 更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动位置到新的位置。 基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行时，可能面临“页分裂”的问题。当行的主键值要求必须将这行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。 二级索引可能比想象的更大，因为在二级索引中的叶节点包含了引用行的主键列。 二级索引访问需要两次索引查找，而不是一次。 InnoDB和MyISAM的索引区别聚簇索引和非聚簇索引的数据分布有区别，以及对应的主键索引和二级索引的数据分布也有区别，通常会让人感到困惑和意外。下图展示了MyISAM和InnoDB的不同索引和数据存储方式。 MyISAM的数据分布非常简单，按照数据插入的顺序存储在磁盘上，主键索引和二级索引的叶节点存储着指针，指向对应的数据行。InnoDB中，聚簇索引“就是”表，所以不会像MyISAM那样需要独立的行存储。聚簇索引的每个叶节点都包含了主键值和所有的剩余列(在此例中是col2)。 InnoDB的二级索引和聚簇索引很不同。InnoDB二级索引的叶节点中存储的不是“行指针”，而是主键值，并以此作为指向行的“指针”。 松散索引扫描MySQL并不支持松散索引扫描，也就是无法按照不连续的方式扫描一个索引。通常，MySQL的索引扫描需要先定义一个起点和终点，即使需要的数据只是这段索引中很少数的几个，MySQL仍然需要扫描这段索引中的每个条目。 下面，我们通过一个示例说明这点，假设我们有如下索引(a,b)，有下面的查询： 因为索引的前导字段是列a，但是在查询中只指定了字段b，MySQL无法使用这个索引，从而只能通过全表扫描找到匹配的行，如下图所示。 了解索引的物理结构的话，不难发现还可以有一个更快的办法执行上面的查询。索引的物理结构(不是存储引擎的API)是的可以先扫描a列第一个值对应的b列的范围，然后再跳到a列第二个不不同值扫描对应的b列的范围。下图展示了如果由MySQL来实现这个过程会怎样。 注意到，这时就无须再使用WHERE子句过滤，因为松散索引扫描已经跳过了所有不需要的记录。 MySQL 5.0之后的版本，在某些特殊的场景下是可以使用松散索引扫描的，例如，在一个分组查询中需要找到分组的最大值和最小值： 在EXPLAIN中的Extra字段显示”Using index for group-by”，表示这里将使用松散索引扫描。 覆盖索引索引除了是一种查找数据的高效方式之外，也是一种列数据的直接获取方式。MySQL可以使用索引来直接获取列的数据，这样就不需要读取数据行。如果一个索引包含所有需要查询的字段的值，我们就称之为“覆盖索引”。 覆盖索引是非常有用的工具，能够极大地提高性能。SQL查询只需要扫描索引而无需回表，会带来很多好处： 索引条目数量和大小通常远小于数据行的条目和大小，所以如果只需要读取索引，那么MySQL就会极大地减少数据访问量。 因为索引是按照列顺序存储的，所以对于I/O密集型的范围查找会比随机从磁盘读取每一行数据的I/O要少的多。 由于InnoDB的聚簇索引，覆盖索引对InnoDB表特别有用。InnoDB的二级索引在叶子节点中保存了行的主键，索引如果二级主键能够覆盖查询，则避免对主键索引的第二次查询。 当发起一个被覆盖索引的查询(也叫索引覆盖查询)时，在EXPLAIN的Extra列可以看到”Using Index”的信息。例如，表sakila.inventory有一个多列索引(store_id, film_id)。MySQL如果只需要访问这两列，就可以使用这个索引做覆盖索引，如下所示： 参考 MySQL索引背后的数据结构及算法原理 blog.codinglabs.org 《高性能MySQL》]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>B-Tree索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程之CountDownLatch、CyclicBarrier和Semaphore]]></title>
    <url>%2F2017%2F10%2F18%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8BCountDownLatch%E3%80%81CyclicBarrier%E5%92%8CSemaphore%2F</url>
    <content type="text"><![CDATA[CountDownLatch，CyclicBarrier和信号量Semaphore都是java并发包concurrent下提供的并发工具类，是比synchrorized关键字更高效的同步结构。 CountDownLatchCountDownLatch类位于java.util.concurrent包下，利用它可以实现类似计数器的功能。比如有一个任务A，它要等待其他4个任务执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能了。 CountDownLatch类只提供了一个构造器： public CountDownLatch(int count) { }; //参数count为计数值 然后下面这3个方法是CountDownLatch类中最重要的方法： public void await() throws InterruptedException { }; //调用await()方法的线程会被挂起，它会等待直到count值为0才继续执行 public boolean await(long timeout, TimeUnit unit) throws InterruptedException { }; //和await()类似，只不过等待一定的时间后count值还没变为0的话就会继续执行 public void countDown() { }; //将count值减1 下面看一个例子大家就清楚CountDownLatch的用法了： public class Test { public static void main(String[] args) { final CountDownLatch latch = new CountDownLatch(2); new Thread(){ public void run() { try { System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;正在执行&quot;); Thread.sleep(3000); System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;执行完毕&quot;); latch.countDown(); } catch (InterruptedException e) { e.printStackTrace(); } }; }.start(); new Thread(){ public void run() { try { System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;正在执行&quot;); Thread.sleep(3000); System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;执行完毕&quot;); latch.countDown(); } catch (InterruptedException e) { e.printStackTrace(); } }; }.start(); try { System.out.println(&quot;等待2个子线程执行完毕...&quot;); latch.await(); System.out.println(&quot;2个子线程已经执行完毕&quot;); System.out.println(&quot;继续执行主线程&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } } 执行结果： 线程Thread-0正在执行 线程Thread-1正在执行 等待2个子线程执行完毕... 线程Thread-0执行完毕 线程Thread-1执行完毕 2个子线程已经执行完毕 继续执行主线程 CyclicBarrier 字面意思回环栅栏，通过它可以实现让一组线程等待至某个状态之后再全部同时执行。叫做回环是因为当所有等待线程都被释放以后，CyclicBarrier可以被重用。我们暂且把这个状态就叫做barrier，当调用await()方法之后，线程就处于barrier了。 CyclicBarrier类位于java.util.concurrent包下，CyclicBarrier提供2个构造器： public CyclicBarrier(int parties, Runnable barrierAction) { } public CyclicBarrier(int parties) { } 参数parties指让多少个线程或者任务等待至barrier状态；参数barrierAction为当这些线程都达到barrier状态时会执行的内容。 然后CyclicBarrier中最重要的方法就是await方法，它有2个重载版本： public int await() throws InterruptedException, BrokenBarrierException { }; public int await(long timeout, TimeUnit unit)throws InterruptedException,BrokenBarrierException,TimeoutException { }; 第一个版本比较常用，用来挂起当前线程，直至所有线程都到达barrier状态再同时执行后续任务； 第二个版本是让这些线程等待至一定的时间，如果还有线程没有到达barrier状态就直接让到达barrier的线程执行后续任务。 下面举几个例子就明白了： 假若有若干个线程都要进行写数据操作，并且只有所有线程都完成写数据操作之后，这些线程才能继续做后面的事情，此时就可以利用CyclicBarrier了： public class Test { public static void main(String[] args) { int N = 4; CyclicBarrier barrier = new CyclicBarrier(N); for(int i=0;i&lt;N;i++) new Writer(barrier).start(); } static class Writer extends Thread{ private CyclicBarrier cyclicBarrier; public Writer(CyclicBarrier cyclicBarrier) { this.cyclicBarrier = cyclicBarrier; } @Override public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;正在写入数据...&quot;); try { Thread.sleep(5000); //以睡眠来模拟写入数据操作 System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;写入数据完毕，等待其他线程写入完毕&quot;); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); }catch(BrokenBarrierException e){ e.printStackTrace(); } System.out.println(&quot;所有线程写入完毕，继续处理其他任务...&quot;); } } } 执行结果： 线程Thread-0正在写入数据... 线程Thread-3正在写入数据... 线程Thread-2正在写入数据... 线程Thread-1正在写入数据... 线程Thread-2写入数据完毕，等待其他线程写入完毕 线程Thread-0写入数据完毕，等待其他线程写入完毕 线程Thread-3写入数据完毕，等待其他线程写入完毕 线程Thread-1写入数据完毕，等待其他线程写入完毕 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 从上面输出结果可以看出，每个写入线程执行完写数据操作之后，就在等待其他线程写入操作完毕。 当所有线程线程写入操作完毕之后，所有线程就继续进行后续的操作了。 如果说想在所有线程写入操作完之后，进行额外的其他操作可以为CyclicBarrier提供Runnable参数： public class Test { public static void main(String[] args) { int N = 4; CyclicBarrier barrier = new CyclicBarrier(N,new Runnable() { @Override public void run() { System.out.println(&quot;当前线程&quot;+Thread.currentThread().getName()); } }); for(int i=0;i&lt;N;i++) new Writer(barrier).start(); } static class Writer extends Thread{ private CyclicBarrier cyclicBarrier; public Writer(CyclicBarrier cyclicBarrier) { this.cyclicBarrier = cyclicBarrier; } @Override public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;正在写入数据...&quot;); try { Thread.sleep(5000); //以睡眠来模拟写入数据操作 System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;写入数据完毕，等待其他线程写入完毕&quot;); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); }catch(BrokenBarrierException e){ e.printStackTrace(); } System.out.println(&quot;所有线程写入完毕，继续处理其他任务...&quot;); } } } 运行结果： 线程Thread-0正在写入数据... 线程Thread-1正在写入数据... 线程Thread-2正在写入数据... 线程Thread-3正在写入数据... 线程Thread-0写入数据完毕，等待其他线程写入完毕 线程Thread-1写入数据完毕，等待其他线程写入完毕 线程Thread-2写入数据完毕，等待其他线程写入完毕 线程Thread-3写入数据完毕，等待其他线程写入完毕 当前线程Thread-3 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 所有线程写入完毕，继续处理其他任务... 从结果可以看出，当四个线程都到达barrier状态后，最后执行完任务达到 barrier的线程去执行Runnable。 下面看一下为await指定时间的效果： public class Test { public static void main(String[] args) { int N = 4; CyclicBarrier barrier = new CyclicBarrier(N); for(int i=0;i&lt;N;i++) { if(i&lt;N-1) new Writer(barrier).start(); else { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } new Writer(barrier).start(); } } } static class Writer extends Thread{ private CyclicBarrier cyclicBarrier; public Writer(CyclicBarrier cyclicBarrier) { this.cyclicBarrier = cyclicBarrier; } @Override public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;正在写入数据...&quot;); try { Thread.sleep(5000); //以睡眠来模拟写入数据操作 System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;写入数据完毕，等待其他线程写入完毕&quot;); try { cyclicBarrier.await(2000, TimeUnit.MILLISECONDS); } catch (TimeoutException e) { // TODO Auto-generated catch block e.printStackTrace(); } } catch (InterruptedException e) { e.printStackTrace(); }catch(BrokenBarrierException e){ e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+&quot;所有线程写入完毕，继续处理其他任务...&quot;); } } } 执行结果： 线程Thread-0正在写入数据... 线程Thread-2正在写入数据... 线程Thread-1正在写入数据... 线程Thread-2写入数据完毕，等待其他线程写入完毕 线程Thread-0写入数据完毕，等待其他线程写入完毕 线程Thread-1写入数据完毕，等待其他线程写入完毕 线程Thread-3正在写入数据... java.util.concurrent.TimeoutException Thread-1所有线程写入完毕，继续处理其他任务... Thread-0所有线程写入完毕，继续处理其他任务... at java.util.concurrent.CyclicBarrier.dowait(Unknown Source) at java.util.concurrent.CyclicBarrier.await(Unknown Source) at com.cxh.test1.Test$Writer.run(Test.java:58) java.util.concurrent.BrokenBarrierException at java.util.concurrent.CyclicBarrier.dowait(Unknown Source) at java.util.concurrent.CyclicBarrier.await(Unknown Source) at com.cxh.test1.Test$Writer.run(Test.java:58) java.util.concurrent.BrokenBarrierException at java.util.concurrent.CyclicBarrier.dowait(Unknown Source) at java.util.concurrent.CyclicBarrier.await(Unknown Source) at com.cxh.test1.Test$Writer.run(Test.java:58) Thread-2所有线程写入完毕，继续处理其他任务... java.util.concurrent.BrokenBarrierException 线程Thread-3写入数据完毕，等待其他线程写入完毕 at java.util.concurrent.CyclicBarrier.dowait(Unknown Source) at java.util.concurrent.CyclicBarrier.await(Unknown Source) at com.cxh.test1.Test$Writer.run(Test.java:58) Thread-3所有线程写入完毕，继续处理其他任务… 上面的代码在main方法的for循环中，故意让最后一个线程启动延迟，因为在前面三个线程都达到barrier之后，等待了指定的时间发现第四个线程还没有达到barrier，就抛出异常并继续执行后面的任务。 另外CyclicBarrier是可以重用的，看下面这个例子： public class Test { public static void main(String[] args) { int N = 4; CyclicBarrier barrier = new CyclicBarrier(N); for(int i=0;i&lt;N;i++) { new Writer(barrier).start(); } try { Thread.sleep(25000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;CyclicBarrier重用&quot;); for(int i=0;i&lt;N;i++) { new Writer(barrier).start(); } } static class Writer extends Thread{ private CyclicBarrier cyclicBarrier; public Writer(CyclicBarrier cyclicBarrier) { this.cyclicBarrier = cyclicBarrier; } @Override public void run() { System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;正在写入数据...&quot;); try { Thread.sleep(5000); //以睡眠来模拟写入数据操作 System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;写入数据完毕，等待其他线程写入完毕&quot;); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); }catch(BrokenBarrierException e){ e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+&quot;所有线程写入完毕，继续处理其他任务...&quot;); } } } 执行结果： 线程Thread-0正在写入数据... 线程Thread-1正在写入数据... 线程Thread-3正在写入数据... 线程Thread-2正在写入数据... 线程Thread-1写入数据完毕，等待其他线程写入完毕 线程Thread-3写入数据完毕，等待其他线程写入完毕 线程Thread-2写入数据完毕，等待其他线程写入完毕 线程Thread-0写入数据完毕，等待其他线程写入完毕 Thread-0所有线程写入完毕，继续处理其他任务... Thread-3所有线程写入完毕，继续处理其他任务... Thread-1所有线程写入完毕，继续处理其他任务... Thread-2所有线程写入完毕，继续处理其他任务... CyclicBarrier重用 线程Thread-4正在写入数据... 线程Thread-5正在写入数据... 线程Thread-6正在写入数据... 线程Thread-7正在写入数据... 线程Thread-7写入数据完毕，等待其他线程写入完毕 线程Thread-5写入数据完毕，等待其他线程写入完毕 线程Thread-6写入数据完毕，等待其他线程写入完毕 线程Thread-4写入数据完毕，等待其他线程写入完毕 Thread-4所有线程写入完毕，继续处理其他任务... Thread-5所有线程写入完毕，继续处理其他任务... Thread-6所有线程写入完毕，继续处理其他任务... Thread-7所有线程写入完毕，继续处理其他任务... 从执行结果可以看出，在初次的4个线程越过barrier状态后，又可以用来进行新一轮的使用。而CountDownLatch无法进行重复使用。 SemaphoreSemaphore翻译成字面意思为 信号量，Semaphore可以控同时访问的线程个数，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。 Semaphore类位于java.util.concurrent包下，它提供了2个构造器： public Semaphore(int permits) { //参数permits表示许可数目，即同时可以允许多少线程进行访问 sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { //这个多了一个参数fair表示是否是公平的，即等待时间越久的越先获取许可 sync = (fair)? new FairSync(permits) : new NonfairSync(permits); } 下面说一下Semaphore类中比较重要的几个方法，首先是acquire()、release()方法： public void acquire() throws InterruptedException { } //获取一个许可 public void acquire(int permits) throws InterruptedException { } //获取permits个许可 public void release() { } //释放一个许可 public void release(int permits) { } //释放permits个许可 acquire()用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可。 release()用来释放许可。注意，在释放许可之前，必须先获获得许可。否则直接release，permit数目可能会大于设置的最大数。 这4个方法都会被阻塞，如果想立即得到执行结果，可以使用下面几个方法： public boolean tryAcquire() { }; //尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(long timeout, TimeUnit unit) throws InterruptedException { }; //尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false public boolean tryAcquire(int permits) { }; //尝试获取permits个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(int permits, long timeout, TimeUnit unit) throws InterruptedException { }; //尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false 另外还可以通过availablePermits()方法得到可用的许可数目。 下面通过一个例子来看一下Semaphore的具体使用： 假若一个工厂有5台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore来实现： public class Test { public static void main(String[] args) { int N = 8; //工人数 Semaphore semaphore = new Semaphore(5); //机器数目 for(int i=0;i&lt;N;i++) new Worker(i,semaphore).start(); } static class Worker extends Thread{ private int num; private Semaphore semaphore; public Worker(int num,Semaphore semaphore){ this.num = num; this.semaphore = semaphore; } @Override public void run() { try { semaphore.acquire(); System.out.println(&quot;工人&quot;+this.num+&quot;占用一个机器在生产...&quot;); Thread.sleep(2000); System.out.println(&quot;工人&quot;+this.num+&quot;释放出机器&quot;); semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } } } 执行结果： 工人0占用一个机器在生产... 工人1占用一个机器在生产... 工人2占用一个机器在生产... 工人4占用一个机器在生产... 工人5占用一个机器在生产... 工人0释放出机器 工人2释放出机器 工人3占用一个机器在生产... 工人7占用一个机器在生产... 工人4释放出机器 工人5释放出机器 工人1释放出机器 工人6占用一个机器在生产... 工人3释放出机器 工人7释放出机器 工人6释放出机器 总结 CountDownLatch和CyclicBarrier都能够实现线程之间的等待，只不过它们侧重点不同： CountDownLatch一般用于某个线程A等待若干个其他线程执行完任务之后，它才执行； 而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行； 另外，CountDownLatch是不能够重用的，而CyclicBarrier是可以重用的。 Semaphore其实和锁有点类似，它一般用于控制对某组资源的访问权限。]]></content>
      <categories>
        <category>java</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>CountDownLatch</tag>
        <tag>CyclicBarrier</tag>
        <tag>Semaphore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中bean的生命周期]]></title>
    <url>%2F2017%2F09%2F13%2FSpring%E4%B8%ADbean%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[Spring中bean的生命周期 一分钟掌握Spring中bean的生命周期！在spring中，从BeanFactory或ApplicationContext取得的实例为Singleton，也就是预设为每一个Bean的别名只能维持一个实例. 在spring中，从BeanFactory或ApplicationContext取得的实例为Singleton，也就是预设为每一个Bean的别名只能维持一个实例，而不是每次都产生一个新的对象使用Singleton模式产生单一实例，对单线程的程序说并不会有什么问题，但对于多线程的程序，就必须注意安全(Thread-safe)的议题，防止多个线程同时存取共享资源所引发的数据不同步问题。 然而在spring中 可以设定每次从BeanFactory或ApplicationContext指定别名并取得Bean时都产生一个新的实例：例如： 在spring中，singleton属性默认是true，只有设定为false，则每次指定别名取得的Bean时都会产生一个新的实例 一个Bean从创建到销毁，如果是用BeanFactory来生成,管理Bean的话，会经历几个执行阶段(如图)： Bean的建立： 容器寻找Bean的定义信息并将其实例化。 属性注入： 使用依赖注入，Spring按照Bean定义信息配置Bean所有属性 BeanNameAware的setBeanName()： 如果Bean类有实现org.springframework.beans.BeanNameAware接口，工厂调用Bean的setBeanName()方法传递Bean的ID。 BeanFactoryAware的setBeanFactory()： 如果Bean类有实现org.springframework.beans.factory.BeanFactoryAware接口，工厂调用setBeanFactory()方法传入工厂自身。 BeanPostProcessors的ProcessBeforeInitialization() 如果有org.springframework.beans.factory.config.BeanPostProcessors和Bean关联，那么其postProcessBeforeInitialization()方法将被将被调用。 initializingBean的afterPropertiesSet()： 如果Bean类已实现org.springframework.beans.factory.InitializingBean接口，则执行他的afterProPertiesSet()方法 Bean定义文件中定义init-method： 可以在Bean定义文件中使用”init-method”属性设定方法名称例如： 如果有以上设置的话，则执行到这个阶段，就会执行initBean()方法 BeanPostProcessors的ProcessaAfterInitialization() 如果有任何的BeanPostProcessors实例与Bean实例关联，则执行BeanPostProcessors实例的ProcessaAfterInitialization()方法 此时，Bean已经可以被应用系统使用，并且将保留在BeanFactory中知道它不在被使用。有两种方法可以将其从BeanFactory中删除掉(如图): DisposableBean的destroy() 在容器关闭时，如果Bean类有实现org.springframework.beans.factory.DisposableBean接口，则执行他的destroy()方法 Bean定义文件中定义destroy-method 在容器关闭时，可以在Bean定义文件中使用”destroy-method”属性设定方法名称，例如： 如果有以上设定的话，则进行至这个阶段时，就会执行destroy()方法，如果是使用ApplicationContext来生成并管理Bean的话则稍有不同，使用ApplicationContext来生成及管理Bean实例的话，在执行BeanFactoryAware的setBeanFactory()阶段后，若Bean类上有实现org.springframework.context.ApplicationContextAware接口，则执行其setApplicationContext()方法，接着才执行BeanPostProcessors的ProcessBeforeInitialization()及之后的流程。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Bean的生命周期</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2017%2F08%2F08%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言设计模式简介设计模式（Design pattern）代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。 设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。项目中合理地运用设计模式可以完美地解决很多问题，每种模式在现实中都有相应的原理来与之对应，每种模式都描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是设计模式能被广泛应用的原因。什么是 GOF（四人帮，全拼 Gang of Four）？在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书，该书首次提到了软件开发中设计模式的概念。四位作者合称 GOF（四人帮，全拼 Gang of Four）。他们所提出的设计模式主要是基于以下的面向对象设计原则。对接口编程而不是对实现编程。优先使用对象组合而不是继承。设计模式的使用设计模式在软件开发中的两个主要用途。开发人员的共同平台设计模式提供了一个标准的术语系统，且具体到特定的情景。例如，单例设计模式意味着使用单个对象，这样所有熟悉单例设计模式的开发人员都能使用单个对象，并且可以通过这种方式告诉对方，程序使用的是单例模式。最佳的实践设计模式已经经历了很长一段时间的发展，它们提供了软件开发过程中面临的一般问题的最佳解决方案。学习这些模式有助于经验不足的开发人员通过一种简单快捷的方式来学习软件设计。设计模式的类型根据设计模式的参考书 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 中所提到的，总共有 23 种设计模式。这些模式可以分为三大类：创建型模式（Creational Patterns）、结构型模式（Structural Patterns）、行为型模式（Behavioral Patterns）。当然，我们还会讨论另一类设计模式：J2EE 设计模式。 序号 模式&amp;描述 包含 1 创建型模式这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。 工厂模式（Factory Pattern）抽象工厂模式（Abstract Factory Pattern）单例模式（Singleton Pattern）建造者模式（Builder Pattern）原型模式（Prototype Pattern） 2 结构型模式这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。 适配器模式（Adapter Pattern）桥接模式（Bridge Pattern）过滤器模式（Filter、Criteria Pattern）组合模式（Composite Pattern）装饰器模式（Decorator Pattern）外观模式（Facade Pattern）享元模式（Flyweight Pattern）代理模式（Proxy Pattern） 3 行为型模式这些设计模式特别关注对象之间的通信。 责任链模式（Chain of Responsibility Pattern）命令模式（Command Pattern）解释器模式（Interpreter Pattern）迭代器模式（Iterator Pattern）中介者模式（Mediator Pattern）备忘录模式（Memento Pattern）观察者模式（Observer Pattern）状态模式（State Pattern）空对象模式（Null Object Pattern）策略模式（Strategy Pattern）模板模式（Template Pattern）访问者模式（Visitor Pattern） 4 J2EE 模式这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。 MVC 模式（MVC Pattern）业务代表模式（Business Delegate Pattern）组合实体模式（Composite Entity Pattern）数据访问对象模式（Data Access Object Pattern）前端控制器模式（Front Controller Pattern）拦截过滤器模式（Intercepting Filter Pattern）服务定位器模式（Service Locator Pattern）传输对象模式（Transfer Object Pattern） 下面用一个图片来整体描述一下设计模式之间的关系： 设计模式的六大原则 开闭原则（Open Close Principle）开闭原则的意思是：对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。 里氏代换原则（Liskov Substitution Principle）里氏代换原则是面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。LSP 是继承复用的基石，只有当派生类可以替换掉基类，且软件单位的功能不受到影响时，基类才能真正被复用，而派生类也能够在基类的基础上增加新的行为。里氏代换原则是对开闭原则的补充。实现开闭原则的关键步骤就是抽象化，而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。 依赖倒转原则（Dependence Inversion Principle）这个原则是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。 接口隔离原则（Interface Segregation Principle）这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。它还有另外一个意思是：降低类之间的耦合度。由此可见，其实设计模式就是从大型软件架构出发、便于升级和维护的软件设计思想，它强调降低依赖，降低耦合。 迪米特法则，又称最少知道原则（Demeter Principle）最少知道原则是指：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立。 合成复用原则（Composite Reuse Principle）合成复用原则是指：尽量使用合成/聚合的方式，而不是使用继承。 java常用设计模式通俗一点讲就是：一个程序员对设计模式的理解:“不懂”为什么要把很简单的东西搞得那么复杂。后来随着软件开发经验的增加才开始明白我所看到的“复杂”恰恰就是设计模式的精髓所在，我所理解的“简单”就是一把钥匙开一把锁的模式，目的仅仅是着眼于解决现在的问题，而设计模式的“复杂”就在于它是要构造一个“万能钥匙”，目的是提出一种对所有锁的开锁方案。在真正理解设计模式之前我一直在编写“简单”的代码.这个“简单”不是功能的简单，而是设计的简单。简单的设计意味着缺少灵活性，代码很钢硬，只在这个项目里有用，拿到其它的项目中就是垃圾，我将其称之为“一次性代码”。 要使代码可被反复使用,请用’设计模式’对你的代码进行设计。 很多我所认识的程序员在接触到设计模式之后，都有一种相见恨晚的感觉，有人形容学习了设计模式之后感觉自己好像已经脱胎换骨，达到了新的境界，还有人甚至把是否了解设计模式作为程序员划分水平的标准。我们也不能陷入模式的陷阱，为了使用模式而去套模式，那样会陷入形式主义。我们在使用模式的时候，一定要注意模式的意图（intent），而不要过多的去关注模式的实现细节，因为这些实现细节在特定情况下，可能会发生一些改变。不要顽固地认为设计模式一书中的类图或实现代码就代表了模式本身。 设计原则：(重要) 逻辑代码独立到单独的方法中，注重封装性—易读，易复用。不要在一个方法中，写下上百行的逻辑代码。把各小逻辑代码独立出来，写于其它方法中，易读其可重复调用。 写类，写方法，写功能时，应考虑其移植性，复用性：防止一次性代码！是否可以拿到其它同类事物中应该？是否可以拿到其它系统中应该？ 熟练运用继承的思想：找出应用中相同之处，且不容易发生变化的东西，把它们抽取到抽象类中，让子类去继承它们；继承的思想，也方便将自己的逻辑建立于别人的成果之上。如ImageField extends JTextField；熟练运用接口的思想：找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。 把很简单的东西搞得那么复杂，一次性代码，设计模式优势的实例说明：（策略模式）说明：模拟鸭子游戏的应用程序，要求：游戏中会出现各种颜色外形的鸭子，一边游泳戏水，一边呱呱叫。1、 一次性代码 直接编写出各种鸭子的类：MallardDuck//野鸭，RedheadDuck//红头鸭，各类有三个方法：quack()：叫的方法swim()：游水的方法display()：外形的方法2、运用继承的特性，将其中共同的部分提升出来，避免重复编程。即：设计一个鸭子的超类（Superclass）,并让各种鸭子继承这个超类。 123456789public class Duck&#123; public void quack()&#123; //呱呱叫 System.out.println(&quot;呱呱叫&quot;); &#125; public void swim()&#123; //游泳 System.out.println(&quot; 游泳&quot;); &#125; public abstratact void display(); /*因为外观不一样，让子类自己去决定了。*/&#125; 对于它的子类只需简单的继承就可以了，并实现自己的display()方法。 123456789101112//野鸭 public class MallardDuck extends Duck&#123; public void display()&#123; System.out.println(&quot;野鸭的颜色...&quot;); &#125; &#125;//红头鸭 public class RedheadDuck extends Duck&#123; public void display()&#123; System.out.println(&quot;红头鸭的颜色...&quot;); &#125;&#125; 不幸的是，现在客户又提出了新的需求，想让鸭子飞起来。这个对于我们OO程序员，在简单不过了，在超类中在加一个方法就可以了。 123456789101112public class Duck&#123; public void quack()&#123; //呱呱叫 System.out.println(&quot;呱呱叫&quot;); &#125; public void swim()&#123; //游泳 System.out.println(&quot; 游泳&quot;); &#125; public abstract void display(); /*因为外观不一样，让子类自己去决定了。*/ public void fly()&#123; System.out.println(&quot;飞吧！鸭子&quot;); &#125;&#125; 对于不能飞的鸭子，在子类中只需简单的覆盖。 123456789//残废鸭 public class DisabledDuck extends Duck&#123; public void display()&#123; System.out.println(&quot;残废鸭的颜色...&quot;); &#125; public void fly()&#123; //覆盖，变成什么事都不做。 &#125;&#125; 其它会飞的鸭子不用覆盖。这样所有的继承这个超类的鸭子都会fly了。但是问题又出来了，客户又提出有的鸭子会飞，有的不能飞。 对于上面的设计，你可能发现一些弊端，如果超类有新的特性，子类都必须变动，这是我们开发最不喜欢看到的，一个类变让另一个类也跟着变，这有点不符合OO设计了。这样很显然的耦合了一起。利用继承—&gt;耦合度太高了. 3、用接口改进我们把容易引起变化的部分提取出来并封装之，来应付以后的变法。虽然代码量加大了，但可用性提高了，耦合度也降低了。我们把Duck中的fly方法和quack提取出来。 123456 public interface Flyable&#123; public void fly(); &#125; public interface Quackable&#123; public void quack();&#125; 最后Duck的设计成为： 123456public class Duck&#123; public void swim()&#123; //游泳 System.out.println(&quot; 游泳&quot;); &#125; public abstract void display(); /*因为外观不一样，让子类自 己去决定了。*/&#125; 而MallardDuck,RedheadDuck,DisabledDuck 就可以写成为： 123456789101112131415161718192021222324252627282930313233//野鸭 public class MallardDuck extends Duck implements Flyable,Quackable&#123; public void display()&#123; System.out.println(&quot;野鸭的颜色...&quot;); &#125; public void fly()&#123; //实现该方法 &#125; public void quack()&#123; //实现该方法 &#125; &#125;//红头鸭 public class RedheadDuck extends Duck implements Flyable,Quackable&#123; public void display()&#123; System.out.println(&quot;红头鸭的颜色...&quot;); &#125; public void fly()&#123; //实现该方法 &#125; public void quack()&#123; //实现该方法 &#125;&#125; //残废鸭 只实现Quackable（能叫不能飞） public class DisabledDuck extends Duck implements Quackable&#123; public void display()&#123; System.out.println(&quot;残废鸭的颜色...&quot;); &#125; public void quack()&#123; //实现该方法 &#125;&#125; 好处:这样已设计，我们的程序就降低了它们之间的耦合。不足:Flyable和 Quackable接口一开始似乎还挺不错的，解决了问题（只有会飞到鸭子才实现 Flyable），但是Java接口不具有实现代码，所以实现接口无法达到代码的复用。 继承的好处:让共同部分,可以复用.避免重复编程.继承的不好:耦合性高.一旦超类添加一个新方法,子类都继承,拥有此方法,若子类相当部分不实现此方法,则要进行大批量修改.继承时,子类就不可继承其它类了.接口的好处:解决了继承耦合性高的问题,且可让实现类,继承或实现其它类或接口.接口的不好:不能真正实现代码的复用.可用以下的策略模式来解决. strategy(策略模式)我们有一个设计原则：找出应用中相同之处，且不容易发生变化的东西，把它们抽取到抽象类中，让子类去继承它们；找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。 现在，为了要分开“变化和不变化的部分”，我们准备建立两组类（完全远离Duck类），一个是”fly”相关的，另一个是“quack”相关的，每一组类将实现各自的动作。比方说，我们可能有一个类实现“呱呱叫”，另一个类实现“吱吱叫”，还有一个类实现“安静”。首先写两个接口。FlyBehavior(飞行行为)和QuackBehavior（叫的行为）. 123456public interface FlyBehavior&#123; public void fly(); &#125;public interface QuackBehavior&#123; public void quack();&#125; 我们在定义一些针对FlyBehavior的具体实现。 12345678910public class FlyWithWings implements FlyBehavior&#123; public void fly()&#123; //实现了所有有翅膀的鸭子飞行行为。 &#125; &#125;public class FlyNoWay implements FlyBehavior&#123; public void fly()&#123; //什么都不做，不会飞 &#125; &#125; 针对QuackBehavior的几种具体实现。 1234567891011121314151617public class Quack implements QuackBehavior&#123; public void quack()&#123; //实现呱呱叫的鸭子 &#125;&#125; public class Squeak implements QuackBehavior&#123; public void quack()&#123; //实现吱吱叫的鸭子 &#125;&#125; public class MuteQuack implements QuackBehavior&#123; public void quack()&#123; //什么都不做，不会叫 &#125;&#125; 点评一:这样的设计，可以让飞行和呱呱叫的动作被其他的对象复用，因为这些行为已经与鸭子类无关了。而我们增加一些新的行为，不会影响到既有的行为类，也不会影响“使用”到飞行行为的鸭子类。最后我们看看Duck 如何设计。 123456789101112131415public class Duck&#123; ---------&gt;在抽象类中,声明各接口,定义各接口对应的方法. FlyBehavior flyBehavior;//接口 QuackBehavior quackBehavior;//接口 public Duck()&#123;&#125; public abstract void display(); public void swim()&#123; //实现游泳的行为 &#125; public void performFly()&#123; flyBehavior.fly(); --&gt;由于是接口,会根据继承类实现的方式,而调用相应的方法. &#125; public void performQuack()&#123; quackBehavior.quack();(); &#125;&#125; 看看MallardDuck如何实现通过构造方法,生成’飞’,’叫’具体实现类的实例,从而指定’飞’,’叫’的具体属性 12345678910public class MallardDuck extends Duck&#123; public MallardDuck &#123; flyBehavior = new FlyWithWings (); quackBehavior = new Quack(); //因为MallardDuck 继承了Duck，所有具有flyBehavior 与quackBehavior 实例变量 &#125; public void display()&#123; //实现 &#125;&#125; 这样就满足了即可以飞，又可以叫，同时展现自己的颜色了。这样的设计我们可以看到是把flyBehavior ，quackBehavior 的实例化写在子类了。我们还可以动态的来决定。我们只需在Duck中加上两个方法。 在构造方法中对属性进行赋值与用属性的setter的区别：构造方法中对属性进行赋值：固定，不可变；用属性的setter，可以在实例化对象后，动态的变化，比较灵活。 12345678910public class Duck&#123; FlyBehavior flyBehavior;//接口 QuackBehavior quackBehavior;//接口 public void setFlyBehavior(FlyBehavior flyBehavior)&#123; this.flyBehavior = flyBehavior; &#125; public void setQuackBehavior(QuackBehavior quackBehavior &#123; this.quackBehavior= quackBehavior; &#125; &#125; 工厂模式工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。主要解决：主要解决接口选择的问题。何时使用：我们明确地计划不同条件下创建不同实例时。如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。关键代码：创建过程在其子类执行。应用实例： 1、您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。 2、Hibernate 换数据库只需换方言和驱动就可以。优点： 1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。注意事项：作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂方法模式。有一点需要注意的地方就是复杂对象适合使用工厂模式，而简单对象，特别是只需要通过 new 就可以完成创建的对象，无需使用工厂模式。如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。实现我们将创建一个 Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。FactoryPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（CIRCLE / RECTANGLE / SQUARE），以便获取它所需对象的类型。 步骤 1 创建一个接口: Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2 创建实现接口的实体类: Rectangle.java 1234567public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Rectangle::draw() method.&quot;); &#125;&#125; Square.java 1234567public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Square::draw() method.&quot;); &#125;&#125; Circle.java 1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125; 步骤 3 创建一个工厂，生成基于给定信息的实体类的对象: ShapeFactory.java 1234567891011121314151617public class ShapeFactory &#123; //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125;&#125; 步骤 4 使用该工厂，通过传递类型信息来获取实体类的对象: FactoryPatternDemo.java 123456789101112131415161718192021222324public class FactoryPatternDemo &#123; public static void main(String[] args) &#123; ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法 Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;); //调用 Circle 的 draw 方法 shape1.draw(); //获取 Rectangle 的对象，并调用它的 draw 方法 Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取 Square 的对象，并调用它的 draw 方法 Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;); //调用 Square 的 draw 方法 shape3.draw(); &#125;&#125; 步骤 5 验证输出: 123Inside Circle::draw() method.Inside Rectangle::draw() method.Inside Square::draw() method. 抽象工厂模式抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。介绍意图：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。主要解决：主要解决接口选择的问题。何时使用：系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。如何解决：在一个产品族里面，定义多个产品。关键代码：在一个工厂里聚合多个同类产品。应用实例：工作了，为了参加一些聚会，肯定有两套或多套衣服吧，比如说有商务装（成套，一系列具体产品）、时尚装（成套，一系列具体产品），甚至对于一个家庭来说，可能有商务女装、商务男装、时尚女装、时尚男装，这些也都是成套的，即一系列具体产品。假设一种情况（现实中是不存在的，要不然，没法进入共产主义了，但有利于说明抽象工厂模式），在您的家中，某一个衣柜（具体工厂）只能存放某一种这样的衣服（成套，一系列具体产品），每次拿这种成套的衣服时也自然要从这个衣柜中取出了。用 OO 的思想去理解，所有的衣柜（具体工厂）都是衣柜类的（抽象工厂）某一个，而每一件成套的衣服又包括具体的上衣（某一具体产品），裤子（某一具体产品），这些具体的上衣其实也都是上衣（抽象产品），具体的裤子也都是裤子（另一个抽象产品）。优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。使用场景： 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。注意事项：产品族难扩展，产品等级易扩展。实现我们将创建 Shape 和 Color 接口和实现这些接口的实体类。下一步是创建抽象工厂类 AbstractFactory。接着定义工厂类 ShapeFactory 和 ColorFactory，这两个工厂类都是扩展了 AbstractFactory。然后创建一个工厂创造器/生成器类 FactoryProducer。AbstractFactoryPatternDemo，我们的演示类使用 FactoryProducer 来获取 AbstractFactory 对象。它将向 AbstractFactory 传递形状信息 Shape（CIRCLE / RECTANGLE / SQUARE），以便获取它所需对象的类型。同时它还向 AbstractFactory 传递颜色信息 Color（RED / GREEN / BLUE），以便获取它所需对象的类型。 步骤 1 为形状创建一个接口。 Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2 创建实现接口的实体类。 Rectangle.java 1234567public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Rectangle::draw() method.&quot;); &#125;&#125; Square.java 1234567public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Square::draw() method.&quot;); &#125;&#125; Circle.java 1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125; 步骤 3 为颜色创建一个接口。 Color.java 123public interface Color &#123; void fill();&#125; 步骤4 创建实现接口的实体类。 Red.java 1234567public class Red implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Inside Red::fill() method.&quot;); &#125;&#125; Green.java 1234567public class Green implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Inside Green::fill() method.&quot;); &#125;&#125; Blue.java 1234567public class Blue implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Inside Blue::fill() method.&quot;); &#125;&#125; 步骤 5 为 Color 和 Shape 对象创建抽象类来获取工厂。 12345AbstractFactory.javapublic abstract class AbstractFactory &#123; abstract Color getColor(String color); abstract Shape getShape(String shape) ;&#125; 步骤 6 创建扩展了 AbstractFactory 的工厂类，基于给定的信息生成实体类的对象。 ShapeFactory.java 12345678910111213141516171819202122public class ShapeFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125; @Override Color getColor(String color) &#123; return null; &#125;&#125; ColorFactory.java 12345678910111213141516171819202122public class ColorFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; return null; &#125; @Override Color getColor(String color) &#123; if(color == null)&#123; return null; &#125; if(color.equalsIgnoreCase(&quot;RED&quot;))&#123; return new Red(); &#125; else if(color.equalsIgnoreCase(&quot;GREEN&quot;))&#123; return new Green(); &#125; else if(color.equalsIgnoreCase(&quot;BLUE&quot;))&#123; return new Blue(); &#125; return null; &#125;&#125; 步骤 7 创建一个工厂创造器/生成器类，通过传递形状或颜色信息来获取工厂。 FactoryProducer.java 12345678910public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase(&quot;SHAPE&quot;))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase(&quot;COLOR&quot;))&#123; return new ColorFactory(); &#125; return null; &#125;&#125; 步骤 8 使用 FactoryProducer 来获取 AbstractFactory，通过传递类型信息来获取实体类的对象。 AbstractFactoryPatternDemo.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class AbstractFactoryPatternDemo &#123; public static void main(String[] args) &#123; //获取形状工厂 AbstractFactory shapeFactory = FactoryProducer.getFactory(&quot;SHAPE&quot;); //获取形状为 Circle 的对象 Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;); //调用 Circle 的 draw 方法 shape1.draw(); //获取形状为 Rectangle 的对象 Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取形状为 Square 的对象 Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;); //调用 Square 的 draw 方法 shape3.draw(); //获取颜色工厂 AbstractFactory colorFactory = FactoryProducer.getFactory(&quot;COLOR&quot;); //获取颜色为 Red 的对象 Color color1 = colorFactory.getColor(&quot;RED&quot;); //调用 Red 的 fill 方法 color1.fill(); //获取颜色为 Green 的对象 Color color2 = colorFactory.getColor(&quot;Green&quot;); //调用 Green 的 fill 方法 color2.fill(); //获取颜色为 Blue 的对象 Color color3 = colorFactory.getColor(&quot;BLUE&quot;); //调用 Blue 的 fill 方法 color3.fill(); &#125;&#125; 步骤 9 验证输出。 123456Inside Circle::draw() method.Inside Rectangle::draw() method.Inside Square::draw() method.Inside Red::fill() method.Inside Green::fill() method.Inside Blue::fill() method. 单例模式单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。注意：1、单例类只能有一个实例。2、单例类必须自己创建自己的唯一实例。3、单例类必须给所有其他对象提供这一实例。意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。主要解决：一个全局使用的类频繁地创建与销毁。何时使用：当您想控制实例数目，节省系统资源的时候。如何解决：判断系统是否已经有这个单例，如果有则返回，如果没有则创建。关键代码：构造函数是私有的。优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。 2、避免对资源的多重占用（比如写文件操作）。缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。使用场景： 1、要求生产唯一序列号。 2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。注意事项：getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。实现我们将创建一个 SingleObject 类。SingleObject 类有它的私有构造函数和本身的一个静态实例。SingleObject 类提供了一个静态方法，供外界获取它的静态实例。SingletonPatternDemo，我们的演示类使用 SingleObject 类来获取 SingleObject 对象。 步骤 1 创建一个 Singleton 类。 SingleObject.java 1234567891011121314151617public class SingleObject &#123; //创建 SingleObject 的一个对象 private static SingleObject instance = new SingleObject(); //让构造函数为 private，这样该类就不会被实例化 private SingleObject()&#123;&#125; //获取唯一可用的对象 public static SingleObject getInstance()&#123; return instance; &#125; public void showMessage()&#123; System.out.println(&quot;Hello World!&quot;); &#125;&#125; 步骤 2 从 singleton 类获取唯一的对象。 SingletonPatternDemo.java 1234567891011121314public class SingletonPatternDemo &#123; public static void main(String[] args) &#123; //不合法的构造函数 //编译时错误：构造函数 SingleObject() 是不可见的 //SingleObject object = new SingleObject(); //获取唯一可用的对象 SingleObject object = SingleObject.getInstance(); //显示消息 object.showMessage(); &#125;&#125; 步骤 3 验证输出。 1Hello World! 单例模式的几种实现方式单例模式的实现有多种方式，如下所示：1、懒汉式，线程不安全是否 Lazy 初始化：是是否多线程安全：否实现难度：易描述：这种方式是最基本的实现方式，这种实现最大的问题就是不支持多线程。因为没有加锁 synchronized，所以严格意义上它并不算单例模式。这种方式 lazy loading 很明显，不要求线程安全，在多线程不能正常工作。代码实例： 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 接下来介绍的几种实现方式都支持多线程，但是在性能上有所差异。 2、懒汉式，线程安全是否 Lazy 初始化：是是否多线程安全：是实现难度：易描述：这种方式具备很好的 lazy loading，能够在多线程中很好的工作，但是，效率很低，99% 情况下不需要同步。优点：第一次调用才初始化，避免内存浪费。缺点：必须加锁 synchronized 才能保证单例，但加锁会影响效率。getInstance() 的性能对应用程序不是很关键（该方法使用不太频繁）。代码实例： 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 3、饿汉式是否 Lazy 初始化：否是否多线程安全：是实现难度：易描述：这种方式比较常用，但容易产生垃圾对象。优点：没有加锁，执行效率会提高。缺点：类加载时就初始化，浪费内存。它基于 classloder 机制避免了多线程的同步问题，不过，instance 在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用 getInstance 方法， 但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化 instance 显然没有达到 lazy loading 的效果。代码实例： 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 4、双检锁/双重校验锁（DCL，即 double-checked locking）JDK 版本：JDK1.5 起是否 Lazy 初始化：是是否多线程安全：是实现难度：较复杂描述：这种方式采用双锁机制，安全且在多线程情况下能保持高性能。getInstance() 的性能对应用程序很关键。代码实例： 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 5、登记式/静态内部类是否 Lazy 初始化：是是否多线程安全：是实现难度：一般描述：这种方式能达到双检锁方式一样的功效，但实现更简单。对静态域使用延迟初始化，应使用这种方式而不是双检锁方式。这种方式只适用于静态域的情况，双检锁方式可在实例域需要延迟初始化时使用。这种方式同样利用了 classloder 机制来保证初始化 instance 时只有一个线程，它跟第 3 种方式不同的是：第 3 种方式只要 Singleton 类被装载了，那么 instance 就会被实例化（没有达到 lazy loading 效果），而这种方式是 Singleton 类被装载了，instance 不一定被初始化。因为 SingletonHolder 类没有被主动使用，只有显示通过调用 getInstance 方法时，才会显示装载 SingletonHolder 类，从而实例化 instance。想象一下，如果实例化 instance 很消耗资源，所以想让它延迟加载，另外一方面，又不希望在 Singleton 类加载时就实例化，因为不能确保 Singleton 类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化 instance 显然是不合适的。这个时候，这种方式相比第 3 种方式就显得很合理。代码实例： 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 6、枚举JDK 版本：JDK1.5 起是否 Lazy 初始化：否是否多线程安全：是实现难度：易描述：这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。它更简洁，自动支持序列化机制，绝对防止多次实例化。这种方式是 Effective Java 作者 Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还自动支持序列化机制，防止反序列化重新创建新的对象，绝对防止多次实例化。不过，由于 JDK1.5 之后才加入 enum 特性，用这种方式写不免让人感觉生疏，在实际工作中，也很少用。不能通过 reflection attack 来调用私有构造方法。代码实例： 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 经验之谈：一般情况下，不建议使用第 1 种和第 2 种懒汉方式，建议使用第 3 种饿汉方式。只有在要明确实现 lazy loading 效果时，才会使用第 5 种登记方式。如果涉及到反序列化创建对象时，可以尝试使用第 6 种枚举方式。如果有其他特殊的需求，可以考虑使用第 4 种双检锁方式。 建造者模式建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。意图：将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。主要解决：主要解决在软件系统中，有时候面临着”一个复杂对象”的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。何时使用：一些基本部件不会变，而其组合经常变化的时候。如何解决：将变与不变分离开。关键代码：建造者：创建和提供实例，导演：管理建造出来的实例的依赖关系。应用实例： 1、去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的”套餐”。 2、JAVA 中的 StringBuilder。优点： 1、建造者独立，易扩展。 2、便于控制细节风险。缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。使用场景： 1、需要生成的对象具有复杂的内部结构。 2、需要生成的对象内部属性本身相互依赖。注意事项：与工厂模式的区别是：建造者模式更加关注与零件装配的顺序。实现我们假设一个快餐店的商业案例，其中，一个典型的套餐可以是一个汉堡（Burger）和一杯冷饮（Cold drink）。汉堡（Burger）可以是素食汉堡（Veg Burger）或鸡肉汉堡（Chicken Burger），它们是包在纸盒中。冷饮（Cold drink）可以是可口可乐（coke）或百事可乐（pepsi），它们是装在瓶子中。我们将创建一个表示食物条目（比如汉堡和冷饮）的 Item 接口和实现 Item 接口的实体类，以及一个表示食物包装的 Packing 接口和实现 Packing 接口的实体类，汉堡是包在纸盒中，冷饮是装在瓶子中。然后我们创建一个 Meal 类，带有 Item 的 ArrayList 和一个通过结合 Item 来创建不同类型的 Meal 对象的 MealBuilder。BuilderPatternDemo，我们的演示类使用 MealBuilder 来创建一个 Meal。 步骤 1 创建一个表示食物条目和食物包装的接口。 Item.java 12345public interface Item &#123; public String name(); public Packing packing(); public float price(); &#125; Packing.java 123public interface Packing &#123; public String pack();&#125; 步骤 2 创建实现 Packing 接口的实体类。 Wrapper.java 1234567public class Wrapper implements Packing &#123; @Override public String pack() &#123; return &quot;Wrapper&quot;; &#125;&#125; Bottle.java 1234567public class Bottle implements Packing &#123; @Override public String pack() &#123; return &quot;Bottle&quot;; &#125;&#125; 步骤 3 创建实现 Item 接口的抽象类，该类提供了默认的功能。 Burger.java 12345678910public abstract class Burger implements Item &#123; @Override public Packing packing() &#123; return new Wrapper(); &#125; @Override public abstract float price();&#125; ColdDrink.java 12345678910public abstract class ColdDrink implements Item &#123; @Override public Packing packing() &#123; return new Bottle(); &#125; @Override public abstract float price();&#125; 步骤 4 创建扩展了 Burger 和 ColdDrink 的实体类。 VegBurger.java 123456789101112public class VegBurger extends Burger &#123; @Override public float price() &#123; return 25.0f; &#125; @Override public String name() &#123; return &quot;Veg Burger&quot;; &#125;&#125; ChickenBurger.java 123456789101112public class ChickenBurger extends Burger &#123; @Override public float price() &#123; return 50.5f; &#125; @Override public String name() &#123; return &quot;Chicken Burger&quot;; &#125;&#125; Coke.java 123456789101112public class Coke extends ColdDrink &#123; @Override public float price() &#123; return 30.0f; &#125; @Override public String name() &#123; return &quot;Coke&quot;; &#125;&#125; Pepsi.java 123456789101112public class Pepsi extends ColdDrink &#123; @Override public float price() &#123; return 35.0f; &#125; @Override public String name() &#123; return &quot;Pepsi&quot;; &#125;&#125; 步骤 5 创建一个 Meal 类，带有上面定义的 Item 对象。 Meal.java 1234567891011121314151617181920212223242526import java.util.ArrayList;import java.util.List;public class Meal &#123; private List&lt;Item&gt; items = new ArrayList&lt;Item&gt;(); public void addItem(Item item)&#123; items.add(item); &#125; public float getCost()&#123; float cost = 0.0f; for (Item item : items) &#123; cost += item.price(); &#125; return cost; &#125; public void showItems()&#123; for (Item item : items) &#123; System.out.print(&quot;Item : &quot;+item.name()); System.out.print(&quot;, Packing : &quot;+item.packing().pack()); System.out.println(&quot;, Price : &quot;+item.price()); &#125; &#125; &#125; 步骤 6 创建一个 MealBuilder 类，实际的 builder 类负责创建 Meal 对象。 MealBuilder.java 12345678910111213141516public class MealBuilder &#123; public Meal prepareVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new VegBurger()); meal.addItem(new Coke()); return meal; &#125; public Meal prepareNonVegMeal ()&#123; Meal meal = new Meal(); meal.addItem(new ChickenBurger()); meal.addItem(new Pepsi()); return meal; &#125;&#125; 步骤 7 BuiderPatternDemo 使用 MealBuider 来演示建造者模式（Builder Pattern）。 BuilderPatternDemo.java 123456789101112131415public class BuilderPatternDemo &#123; public static void main(String[] args) &#123; MealBuilder mealBuilder = new MealBuilder(); Meal vegMeal = mealBuilder.prepareVegMeal(); System.out.println(&quot;Veg Meal&quot;); vegMeal.showItems(); System.out.println(&quot;Total Cost: &quot; +vegMeal.getCost()); Meal nonVegMeal = mealBuilder.prepareNonVegMeal(); System.out.println(&quot;\n\nNon-Veg Meal&quot;); nonVegMeal.showItems(); System.out.println(&quot;Total Cost: &quot; +nonVegMeal.getCost()); &#125;&#125; 步骤 8 验证输出。 123456789Veg MealItem : Veg Burger, Packing : Wrapper, Price : 25.0Item : Coke, Packing : Bottle, Price : 30.0Total Cost: 55.0Non-Veg MealItem : Chicken Burger, Packing : Wrapper, Price : 50.5Item : Pepsi, Packing : Bottle, Price : 35.0Total Cost: 85.5 原型模式原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。例如，一个对象需要在一个高代价的数据库操作之后被创建。我们可以缓存该对象，在下一个请求时返回它的克隆，在需要的时候更新数据库，以此来减少数据库调用。意图：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。主要解决：在运行期建立和删除原型。何时使用： 1、当一个系统应该独立于它的产品创建，构成和表示时。 2、当要实例化的类是在运行时刻指定时，例如，通过动态装载。 3、为了避免创建一个与产品类层次平行的工厂类层次时。 4、当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。如何解决：利用已有的一个原型对象，快速地生成和原型对象一样的实例。关键代码： 1、实现克隆操作，在 JAVA 继承 Cloneable，重写 clone()，在 .NET 中可以使用 Object 类的 MemberwiseClone() 方法来实现对象的浅拷贝或通过序列化的方式来实现深拷贝。 2、原型模式同样用于隔离类对象的使用者和具体类型（易变类）之间的耦合关系，它同样要求这些”易变类”拥有稳定的接口。应用实例： 1、细胞分裂。 2、JAVA 中的 Object clone() 方法。优点： 1、性能提高。 2、逃避构造函数的约束。缺点： 1、配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类不是很难，但对于已有的类不一定很容易，特别当一个类引用不支持串行化的间接对象，或者引用含有循环结构的时候。 2、必须实现 Cloneable 接口。 3、逃避构造函数的约束。使用场景： 1、资源优化场景。 2、类初始化需要消化非常多的资源，这个资源包括数据、硬件资源等。 3、性能和安全要求的场景。 4、通过 new 产生一个对象需要非常繁琐的数据准备或访问权限，则可以使用原型模式。 5、一个对象多个修改者的场景。 6、一个对象需要提供给其他对象访问，而且各个调用者可能都需要修改其值时，可以考虑使用原型模式拷贝多个对象供调用者使用。 7、在实际项目中，原型模式很少单独出现，一般是和工厂方法模式一起出现，通过 clone 的方法创建一个对象，然后由工厂方法提供给调用者。原型模式已经与 Java 融为浑然一体，大家可以随手拿来使用。注意事项：与通过对一个类进行实例化来构造新对象不同的是，原型模式是通过拷贝一个现有对象生成新对象的。浅拷贝实现 Cloneable，重写，深拷贝是通过实现 Serializable 读取二进制流。实现 我们将创建一个抽象类 Shape 和扩展了 Shape 类的实体类。下一步是定义类 ShapeCache，该类把 shape 对象存储在一个 Hashtable 中，并在请求的时候返回它们的克隆。PrototypPatternDemo，我们的演示类使用 ShapeCache 类来获取 Shape 对象 步骤 1 创建一个实现了 Clonable 接口的抽象类。 Shape.java 1234567891011121314151617181920212223242526272829public abstract class Shape implements Cloneable &#123; private String id; protected String type; abstract void draw(); public String getType()&#123; return type; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Object clone() &#123; Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return clone; &#125;&#125; 步骤 2 创建扩展了上面抽象类的实体类。 Rectangle.java 1234567891011public class Rectangle extends Shape &#123; public Rectangle()&#123; type = &quot;Rectangle&quot;; &#125; @Override public void draw() &#123; System.out.println(&quot;Inside Rectangle::draw() method.&quot;); &#125;&#125; Square.java 1234567891011public class Square extends Shape &#123; public Square()&#123; type = &quot;Square&quot;; &#125; @Override public void draw() &#123; System.out.println(&quot;Inside Square::draw() method.&quot;); &#125;&#125; Circle.java 1234567891011public class Circle extends Shape &#123; public Circle()&#123; type = &quot;Circle&quot;; &#125; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125; 步骤 3 创建一个类，从数据库获取实体类，并把它们存储在一个 Hashtable 中。 ShapeCache.java 1234567891011121314151617181920212223242526272829import java.util.Hashtable;public class ShapeCache &#123; private static Hashtable&lt;String, Shape&gt; shapeMap = new Hashtable&lt;String, Shape&gt;(); public static Shape getShape(String shapeId) &#123; Shape cachedShape = shapeMap.get(shapeId); return (Shape) cachedShape.clone(); &#125; // 对每种形状都运行数据库查询，并创建该形状 // shapeMap.put(shapeKey, shape); // 例如，我们要添加三种形状 public static void loadCache() &#123; Circle circle = new Circle(); circle.setId(&quot;1&quot;); shapeMap.put(circle.getId(),circle); Square square = new Square(); square.setId(&quot;2&quot;); shapeMap.put(square.getId(),square); Rectangle rectangle = new Rectangle(); rectangle.setId(&quot;3&quot;); shapeMap.put(rectangle.getId(),rectangle); &#125;&#125; 步骤 4 PrototypePatternDemo 使用 ShapeCache 类来获取存储在 Hashtable 中的形状的克隆。 PrototypePatternDemo.java 1234567891011121314public class PrototypePatternDemo &#123; public static void main(String[] args) &#123; ShapeCache.loadCache(); Shape clonedShape = (Shape) ShapeCache.getShape(&quot;1&quot;); System.out.println(&quot;Shape : &quot; + clonedShape.getType()); Shape clonedShape2 = (Shape) ShapeCache.getShape(&quot;2&quot;); System.out.println(&quot;Shape : &quot; + clonedShape2.getType()); Shape clonedShape3 = (Shape) ShapeCache.getShape(&quot;3&quot;); System.out.println(&quot;Shape : &quot; + clonedShape3.getType()); &#125;&#125; 步骤 5 验证输出。 123Shape : CircleShape : SquareShape : Rectangle 适配器模式适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种类型的设计模式属于结构型模式，它结合了两个独立接口的功能。这种模式涉及到一个单一的类，该类负责加入独立的或不兼容的接口功能。举个真实的例子，读卡器是作为内存卡和笔记本之间的适配器。您将内存卡插入读卡器，再将读卡器插入笔记本，这样就可以通过笔记本来读取内存卡。我们通过下面的实例来演示适配器模式的使用。其中，音频播放器设备只能播放 mp3 文件，通过使用一个更高级的音频播放器来播放 vlc 和 mp4 文件。意图：将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。主要解决：主要解决在软件系统中，常常要将一些”现存的对象”放到新的环境中，而新环境要求的接口是现对象不能满足的。何时使用： 1、系统需要使用现有的类，而此类的接口不符合系统的需要。 2、想要建立一个可以重复使用的类，用于与一些彼此之间没有太大关联的一些类，包括一些可能在将来引进的类一起工作，这些源类不一定有一致的接口。 3、通过接口转换，将一个类插入另一个类系中。（比如老虎和飞禽，现在多了一个飞虎，在不增加实体的需求下，增加一个适配器，在里面包容一个虎对象，实现飞的接口。）如何解决：继承或依赖（推荐）。关键代码：适配器继承或依赖已有的对象，实现想要的目标接口。应用实例： 1、美国电器 110V，中国 220V，就要有一个适配器将 110V 转化为 220V。 2、JAVA JDK 1.1 提供了 Enumeration 接口，而在 1.2 中提供了 Iterator 接口，想要使用 1.2 的 JDK，则要将以前系统的 Enumeration 接口转化为 Iterator 接口，这时就需要适配器模式。 3、在 LINUX 上运行 WINDOWS 程序。 4、JAVA 中的 jdbc。优点： 1、可以让任何两个没有关联的类一起运行。 2、提高了类的复用。 3、增加了类的透明度。 4、灵活性好。缺点： 1、过多地使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是 A 接口，其实内部被适配成了 B 接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 2.由于 JAVA 至多继承一个类，所以至多只能适配一个适配者类，而且目标类必须是抽象类。使用场景：有动机地修改一个正常运行的系统的接口，这时应该考虑使用适配器模式。注意事项：适配器不是在详细设计时添加的，而是解决正在服役的项目的问题。实现我们有一个 MediaPlayer 接口和一个实现了 MediaPlayer 接口的实体类 AudioPlayer。默认情况下，AudioPlayer 可以播放 mp3 格式的音频文件。我们还有另一个接口 AdvancedMediaPlayer 和实现了 AdvancedMediaPlayer 接口的实体类。该类可以播放 vlc 和 mp4 格式的文件。我们想要让 AudioPlayer 播放其他格式的音频文件。为了实现这个功能，我们需要创建一个实现了 MediaPlayer 接口的适配器类 MediaAdapter，并使用 AdvancedMediaPlayer 对象来播放所需的格式。AudioPlayer 使用适配器类 MediaAdapter 传递所需的音频类型，不需要知道能播放所需格式音频的实际类。AdapterPatternDemo，我们的演示类使用 AudioPlayer 类来播放各种格式。 步骤 1 为媒体播放器和更高级的媒体播放器创建接口。 MediaPlayer.java 12345678public interface MediaPlayer &#123; public void play(String audioType, String fileName);&#125;AdvancedMediaPlayer.javapublic interface AdvancedMediaPlayer &#123; public void playVlc(String fileName); public void playMp4(String fileName);&#125; 步骤 2 创建实现了 AdvancedMediaPlayer 接口的实体类。 VlcPlayer.java 1234567891011public class VlcPlayer implements AdvancedMediaPlayer&#123; @Override public void playVlc(String fileName) &#123; System.out.println(&quot;Playing vlc file. Name: &quot;+ fileName); &#125; @Override public void playMp4(String fileName) &#123; //什么也不做 &#125;&#125; Mp4Player.java 123456789101112public class Mp4Player implements AdvancedMediaPlayer&#123; @Override public void playVlc(String fileName) &#123; //什么也不做 &#125; @Override public void playMp4(String fileName) &#123; System.out.println(&quot;Playing mp4 file. Name: &quot;+ fileName); &#125;&#125; 步骤 3 创建实现了 MediaPlayer 接口的适配器类。 MediaAdapter.java 123456789101112131415161718192021public class MediaAdapter implements MediaPlayer &#123; AdvancedMediaPlayer advancedMusicPlayer; public MediaAdapter(String audioType)&#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;) )&#123; advancedMusicPlayer = new VlcPlayer(); &#125; else if (audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer = new Mp4Player(); &#125; &#125; @Override public void play(String audioType, String fileName) &#123; if(audioType.equalsIgnoreCase(&quot;vlc&quot;))&#123; advancedMusicPlayer.playVlc(fileName); &#125;else if(audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; advancedMusicPlayer.playMp4(fileName); &#125; &#125;&#125; 步骤 4 创建实现了 MediaPlayer 接口的实体类。 AudioPlayer.java 12345678910111213141516171819202122public class AudioPlayer implements MediaPlayer &#123; MediaAdapter mediaAdapter; @Override public void play(String audioType, String fileName) &#123; //播放 mp3 音乐文件的内置支持 if(audioType.equalsIgnoreCase(&quot;mp3&quot;))&#123; System.out.println(&quot;Playing mp3 file. Name: &quot;+ fileName); &#125; //mediaAdapter 提供了播放其他文件格式的支持 else if(audioType.equalsIgnoreCase(&quot;vlc&quot;) || audioType.equalsIgnoreCase(&quot;mp4&quot;))&#123; mediaAdapter = new MediaAdapter(audioType); mediaAdapter.play(audioType, fileName); &#125; else&#123; System.out.println(&quot;Invalid media. &quot;+ audioType + &quot; format not supported&quot;); &#125; &#125; &#125; 步骤 5 使用 AudioPlayer 来播放不同类型的音频格式。 AdapterPatternDemo.java 12345678910public class AdapterPatternDemo &#123; public static void main(String[] args) &#123; AudioPlayer audioPlayer = new AudioPlayer(); audioPlayer.play(&quot;mp3&quot;, &quot;beyond the horizon.mp3&quot;); audioPlayer.play(&quot;mp4&quot;, &quot;alone.mp4&quot;); audioPlayer.play(&quot;vlc&quot;, &quot;far far away.vlc&quot;); audioPlayer.play(&quot;avi&quot;, &quot;mind me.avi&quot;); &#125;&#125; 步骤 6 验证输出。 1234Playing mp3 file. Name: beyond the horizon.mp3Playing mp4 file. Name: alone.mp4Playing vlc file. Name: far far away.vlcInvalid media. avi format not supported 桥接模式桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。这种类型的设计模式属于结构型模式，它通过提供抽象化和实现化之间的桥接结构，来实现二者的解耦。这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。这两种类型的类可被结构化改变而互不影响。我们通过下面的实例来演示桥接模式（Bridge Pattern）的用法。其中，可以使用相同的抽象类方法但是不同的桥接实现类，来画出不同颜色的圆。意图：将抽象部分与实现部分分离，使它们都可以独立的变化。主要解决：在有多种可能会变化的情况下，用继承会造成类爆炸问题，扩展起来不灵活。何时使用：实现系统可能有多个角度分类，每一种角度都可能变化。如何解决：把这种多角度分类分离出来，让它们独立变化，减少它们之间耦合。关键代码：抽象类依赖实现类。应用实例： 1、猪八戒从天蓬元帅转世投胎到猪，转世投胎的机制将尘世划分为两个等级，即：灵魂和肉体，前者相当于抽象化，后者相当于实现化。生灵通过功能的委派，调用肉体对象的功能，使得生灵可以动态地选择。 2、墙上的开关，可以看到的开关是抽象的，不用管里面具体怎么实现的。优点： 1、抽象和实现的分离。 2、优秀的扩展能力。 3、实现细节对客户透明。缺点：桥接模式的引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计与编程。使用场景： 1、如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。 2、对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 3、一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。注意事项：对于两个独立变化的维度，使用桥接模式再适合不过了。实现 我们有一个作为桥接实现的 DrawAPI 接口和实现了 DrawAPI 接口的实体类 RedCircle、GreenCircle。Shape 是一个抽象类，将使用 DrawAPI 的对象。BridgePatternDemo，我们的演示类使用 Shape 类来画出不同颜色的圆。 步骤 1 创建桥接实现接口。 DrawAPI.java 123public interface DrawAPI &#123; public void drawCircle(int radius, int x, int y);&#125; 步骤 2 创建实现了 DrawAPI 接口的实体桥接实现类。 RedCircle.java 1234567public class RedCircle implements DrawAPI &#123; @Override public void drawCircle(int radius, int x, int y) &#123; System.out.println(&quot;Drawing Circle[ color: red, radius: &quot; + radius +&quot;, x: &quot; +x+&quot;, &quot;+ y +&quot;]&quot;); &#125;&#125; GreenCircle.java 1234567public class GreenCircle implements DrawAPI &#123; @Override public void drawCircle(int radius, int x, int y) &#123; System.out.println(&quot;Drawing Circle[ color: green, radius: &quot; + radius +&quot;, x: &quot; +x+&quot;, &quot;+ y +&quot;]&quot;); &#125;&#125; 步骤 3 使用 DrawAPI 接口创建抽象类 Shape。 Shape.java 1234567public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 步骤 4 创建实现了 Shape 接口的实体类。 Circle.java 1234567891011121314public class Circle extends Shape &#123; private int x, y, radius; public Circle(int x, int y, int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; this.radius = radius; &#125; public void draw() &#123; drawAPI.drawCircle(radius,x,y); &#125;&#125; 步骤 5 使用 Shape 和 DrawAPI 类画出不同颜色的圆。 BridgePatternDemo.java 123456789public class BridgePatternDemo &#123; public static void main(String[] args) &#123; Shape redCircle = new Circle(100,100, 10, new RedCircle()); Shape greenCircle = new Circle(100,100, 10, new GreenCircle()); redCircle.draw(); greenCircle.draw(); &#125;&#125; 步骤 6 验证输出。 12Drawing Circle[ color: red, radius: 10, x: 100, 100]Drawing Circle[ color: green, radius: 10, x: 100, 100] 过滤器模式过滤器模式（Filter Pattern）或标准模式（Criteria Pattern）是一种设计模式，这种模式允许开发人员使用不同的标准来过滤一组对象，通过逻辑运算以解耦的方式把它们连接起来。这种类型的设计模式属于结构型模式，它结合多个标准来获得单一标准。实现我们将创建一个 Person 对象、Criteria 接口和实现了该接口的实体类，来过滤 Person 对象的列表。CriteriaPatternDemo，我们的演示类使用 Criteria 对象，基于各种标准和它们的结合来过滤 Person 对象的列表。 步骤 1 创建一个类，在该类上应用标准。 Person.java 12345678910111213141516171819202122public class Person &#123; private String name; private String gender; private String maritalStatus; public Person(String name,String gender,String maritalStatus)&#123; this.name = name; this.gender = gender; this.maritalStatus = maritalStatus; &#125; public String getName() &#123; return name; &#125; public String getGender() &#123; return gender; &#125; public String getMaritalStatus() &#123; return maritalStatus; &#125; &#125; 步骤 2 为标准（Criteria）创建一个接口。 Criteria.java 12345import java.util.List;public interface Criteria &#123; public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons);&#125; 步骤 3 创建实现了 Criteria 接口的实体类。 CriteriaMale.java 12345678910111213141516import java.util.ArrayList;import java.util.List;public class CriteriaMale implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; malePersons = new ArrayList&lt;Person&gt;(); for (Person person : persons) &#123; if(person.getGender().equalsIgnoreCase(&quot;MALE&quot;))&#123; malePersons.add(person); &#125; &#125; return malePersons; &#125;&#125; CriteriaFemale.java 12345678910111213141516import java.util.ArrayList;import java.util.List;public class CriteriaFemale implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; femalePersons = new ArrayList&lt;Person&gt;(); for (Person person : persons) &#123; if(person.getGender().equalsIgnoreCase(&quot;FEMALE&quot;))&#123; femalePersons.add(person); &#125; &#125; return femalePersons; &#125;&#125; CriteriaSingle.java 12345678910111213141516import java.util.ArrayList;import java.util.List;public class CriteriaSingle implements Criteria &#123; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; singlePersons = new ArrayList&lt;Person&gt;(); for (Person person : persons) &#123; if(person.getMaritalStatus().equalsIgnoreCase(&quot;SINGLE&quot;))&#123; singlePersons.add(person); &#125; &#125; return singlePersons; &#125;&#125; AndCriteria.java 123456789101112131415161718import java.util.List;public class AndCriteria implements Criteria &#123; private Criteria criteria; private Criteria otherCriteria; public AndCriteria(Criteria criteria, Criteria otherCriteria) &#123; this.criteria = criteria; this.otherCriteria = otherCriteria; &#125; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; firstCriteriaPersons = criteria.meetCriteria(persons); return otherCriteria.meetCriteria(firstCriteriaPersons); &#125;&#125; OrCriteria.java 12345678910111213141516171819202122232425import java.util.List;public class OrCriteria implements Criteria &#123; private Criteria criteria; private Criteria otherCriteria; public OrCriteria(Criteria criteria, Criteria otherCriteria) &#123; this.criteria = criteria; this.otherCriteria = otherCriteria; &#125; @Override public List&lt;Person&gt; meetCriteria(List&lt;Person&gt; persons) &#123; List&lt;Person&gt; firstCriteriaItems = criteria.meetCriteria(persons); List&lt;Person&gt; otherCriteriaItems = otherCriteria.meetCriteria(persons); for (Person person : otherCriteriaItems) &#123; if(!firstCriteriaItems.contains(person))&#123; firstCriteriaItems.add(person); &#125; &#125; return firstCriteriaItems; &#125;&#125; 步骤4 使用不同的标准（Criteria）和它们的结合来过滤 Person 对象的列表。 CriteriaPatternDemo.java 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.ArrayList; import java.util.List;public class CriteriaPatternDemo &#123; public static void main(String[] args) &#123; List&lt;Person&gt; persons = new ArrayList&lt;Person&gt;(); persons.add(new Person(&quot;Robert&quot;,&quot;Male&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;John&quot;,&quot;Male&quot;, &quot;Married&quot;)); persons.add(new Person(&quot;Laura&quot;,&quot;Female&quot;, &quot;Married&quot;)); persons.add(new Person(&quot;Diana&quot;,&quot;Female&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;Mike&quot;,&quot;Male&quot;, &quot;Single&quot;)); persons.add(new Person(&quot;Bobby&quot;,&quot;Male&quot;, &quot;Single&quot;)); Criteria male = new CriteriaMale(); Criteria female = new CriteriaFemale(); Criteria single = new CriteriaSingle(); Criteria singleMale = new AndCriteria(single, male); Criteria singleOrFemale = new OrCriteria(single, female); System.out.println(&quot;Males: &quot;); printPersons(male.meetCriteria(persons)); System.out.println(&quot;\nFemales: &quot;); printPersons(female.meetCriteria(persons)); System.out.println(&quot;\nSingle Males: &quot;); printPersons(singleMale.meetCriteria(persons)); System.out.println(&quot;\nSingle Or Females: &quot;); printPersons(singleOrFemale.meetCriteria(persons)); &#125; public static void printPersons(List&lt;Person&gt; persons)&#123; for (Person person : persons) &#123; System.out.println(&quot;Person : [ Name : &quot; + person.getName() +&quot;, Gender : &quot; + person.getGender() +&quot;, Marital Status : &quot; + person.getMaritalStatus() +&quot; ]&quot;); &#125; &#125; &#125; 步骤 5 验证输出。 123456789101112131415161718192021Males: Person : [ Name : Robert, Gender : Male, Marital Status : Single ]Person : [ Name : John, Gender : Male, Marital Status : Married ]Person : [ Name : Mike, Gender : Male, Marital Status : Single ]Person : [ Name : Bobby, Gender : Male, Marital Status : Single ]Females: Person : [ Name : Laura, Gender : Female, Marital Status : Married ]Person : [ Name : Diana, Gender : Female, Marital Status : Single ]Single Males: Person : [ Name : Robert, Gender : Male, Marital Status : Single ]Person : [ Name : Mike, Gender : Male, Marital Status : Single ]Person : [ Name : Bobby, Gender : Male, Marital Status : Single ]Single Or Females: Person : [ Name : Robert, Gender : Male, Marital Status : Single ]Person : [ Name : Diana, Gender : Female, Marital Status : Single ]Person : [ Name : Mike, Gender : Male, Marital Status : Single ]Person : [ Name : Bobby, Gender : Male, Marital Status : Single ]Person : [ Name : Laura, Gender : Female, Marital Status : Married ] 组合模式组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。这种模式创建了一个包含自己对象组的类。该类提供了修改相同对象组的方式。我们通过下面的实例来演示组合模式的用法。实例演示了一个组织中员工的层次结构。意图：将对象组合成树形结构以表示”部分-整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。主要解决：它在我们树型结构的问题中，模糊了简单元素和复杂元素的概念，客户程序可以向处理简单元素一样来处理复杂元素，从而使得客户程序与复杂元素的内部结构解耦。何时使用： 1、您想表示对象的部分-整体层次结构（树形结构）。 2、您希望用户忽略组合对象与单个对象的不同，用户将统一地使用组合结构中的所有对象。如何解决：树枝和叶子实现统一接口，树枝内部组合该接口。关键代码：树枝内部组合该接口，并且含有内部属性 List，里面放 Component。应用实例： 1、算术表达式包括操作数、操作符和另一个操作数，其中，另一个操作符也可以是操作树、操作符和另一个操作数。 2、在 JAVA AWT 和 SWING 中，对于 Button 和 Checkbox 是树叶，Container 是树枝。优点： 1、高层模块调用简单。 2、节点自由增加。缺点：在使用组合模式时，其叶子和树枝的声明都是实现类，而不是接口，违反了依赖倒置原则。使用场景：部分、整体场景，如树形菜单，文件、文件夹的管理。注意事项：定义时为具体类。实现我们有一个类 Employee，该类被当作组合模型类。CompositePatternDemo，我们的演示类使用 Employee 类来添加部门层次结构，并打印所有员工。 步骤 1 创建 Employee 类，该类带有 Employee 对象的列表。 Employee.java 1234567891011121314151617181920212223242526272829303132333435import java.util.ArrayList;import java.util.List;public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; //构造函数 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot;+ name +&quot;, dept : &quot;+ dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 步骤 2 使用 Employee 类来创建和打印员工的层次结构。 CompositePatternDemo.java 123456789101112131415161718192021222324252627282930313233public class CompositePatternDemo &#123; public static void main(String[] args) &#123; Employee CEO = new Employee(&quot;John&quot;,&quot;CEO&quot;, 30000); Employee headSales = new Employee(&quot;Robert&quot;,&quot;Head Sales&quot;, 20000); Employee headMarketing = new Employee(&quot;Michel&quot;,&quot;Head Marketing&quot;, 20000); Employee clerk1 = new Employee(&quot;Laura&quot;,&quot;Marketing&quot;, 10000); Employee clerk2 = new Employee(&quot;Bob&quot;,&quot;Marketing&quot;, 10000); Employee salesExecutive1 = new Employee(&quot;Richard&quot;,&quot;Sales&quot;, 10000); Employee salesExecutive2 = new Employee(&quot;Rob&quot;,&quot;Sales&quot;, 10000); CEO.add(headSales); CEO.add(headMarketing); headSales.add(salesExecutive1); headSales.add(salesExecutive2); headMarketing.add(clerk1); headMarketing.add(clerk2); //打印该组织的所有员工 System.out.println(CEO); for (Employee headEmployee : CEO.getSubordinates()) &#123; System.out.println(headEmployee); for (Employee employee : headEmployee.getSubordinates()) &#123; System.out.println(employee); &#125; &#125; &#125;&#125; 步骤 3 验证输出。 1234567Employee :[ Name : John, dept : CEO, salary :30000 ]Employee :[ Name : Robert, dept : Head Sales, salary :20000 ]Employee :[ Name : Richard, dept : Sales, salary :10000 ]Employee :[ Name : Rob, dept : Sales, salary :10000 ]Employee :[ Name : Michel, dept : Head Marketing, salary :20000 ]Employee :[ Name : Laura, dept : Marketing, salary :10000 ]Employee :[ Name : Bob, dept : Marketing, salary :10000 ] 装饰器模式装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。我们通过下面的实例来演示装饰器模式的用法。其中，我们将把一个形状装饰上不同的颜色，同时又不改变形状类。意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。何时使用：在不想增加很多子类的情况下扩展类。如何解决：将具体功能职责划分，同时继承装饰者模式。关键代码： 1、Component 类充当抽象角色，不应该具体实现。 2、修饰类引用和继承 Component 类，具体扩展类重写父类方法。应用实例： 1、孙悟空有 72 变，当他变成”庙宇”后，他的根本还是一只猴子，但是他又有了庙宇的功能。 2、不论一幅画有没有画框都可以挂在墙上，但是通常都是有画框的，并且实际上是画框被挂在墙上。在挂在墙上之前，画可以被蒙上玻璃，装到框子里；这时画、玻璃和画框形成了一个物体。优点：装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。缺点：多层装饰比较复杂。使用场景： 1、扩展一个类的功能。 2、动态增加功能，动态撤销。注意事项：可代替继承。实现我们将创建一个 Shape 接口和实现了 Shape 接口的实体类。然后我们创建一个实现了 Shape 接口的抽象装饰类 ShapeDecorator，并把 Shape 对象作为它的实例变量。RedShapeDecorator 是实现了 ShapeDecorator 的实体类。DecoratorPatternDemo，我们的演示类使用 RedShapeDecorator 来装饰 Shape 对象 步骤 1 创建一个接口。 Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2 创建实现接口的实体类。 Rectangle.java 1234567public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Shape: Rectangle&quot;); &#125;&#125; Circle.java 1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Shape: Circle&quot;); &#125;&#125; 步骤 3 创建实现了 Shape 接口的抽象装饰类。 ShapeDecorator.java 1234567891011public abstract class ShapeDecorator implements Shape &#123; protected Shape decoratedShape; public ShapeDecorator(Shape decoratedShape)&#123; this.decoratedShape = decoratedShape; &#125; public void draw()&#123; decoratedShape.draw(); &#125; &#125; 步骤 4 创建扩展了 ShapeDecorator 类的实体装饰类。 RedShapeDecorator.java 12345678910111213141516public class RedShapeDecorator extends ShapeDecorator &#123; public RedShapeDecorator(Shape decoratedShape) &#123; super(decoratedShape); &#125; @Override public void draw() &#123; decoratedShape.draw(); setRedBorder(decoratedShape); &#125; private void setRedBorder(Shape decoratedShape)&#123; System.out.println(&quot;Border Color: Red&quot;); &#125;&#125; 步骤 5 使用 RedShapeDecorator 来装饰 Shape 对象。 DecoratorPatternDemo.java 123456789101112131415161718public class DecoratorPatternDemo &#123; public static void main(String[] args) &#123; Shape circle = new Circle(); Shape redCircle = new RedShapeDecorator(new Circle()); Shape redRectangle = new RedShapeDecorator(new Rectangle()); System.out.println(&quot;Circle with normal border&quot;); circle.draw(); System.out.println(&quot;\nCircle of red border&quot;); redCircle.draw(); System.out.println(&quot;\nRectangle of red border&quot;); redRectangle.draw(); &#125;&#125; 步骤 6 验证输出。 12345678910Circle with normal borderShape: CircleCircle of red borderShape: CircleBorder Color: RedRectangle of red borderShape: RectangleBorder Color: Red 外观模式外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种类型的设计模式属于结构型模式，它向现有的系统添加一个接口，来隐藏系统的复杂性。这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。意图：为子系统中的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。主要解决：降低访问复杂系统的内部子系统时的复杂度，简化客户端与之的接口。何时使用： 1、客户端不需要知道系统内部的复杂联系，整个系统只需提供一个”接待员”即可。 2、定义系统的入口。如何解决：客户端不与系统耦合，外观类与系统耦合。关键代码：在客户端和复杂系统之间再加一层，这一层将调用顺序、依赖关系等处理好。应用实例： 1、去医院看病，可能要去挂号、门诊、划价、取药，让患者或患者家属觉得很复杂，如果有提供接待人员，只让接待人员来处理，就很方便。 2、JAVA 的三层开发模式。优点： 1、减少系统相互依赖。 2、提高灵活性。 3、提高了安全性。缺点：不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。使用场景： 1、为复杂的模块或子系统提供外界访问的模块。 2、子系统相对独立。 3、预防低水平人员带来的风险。注意事项：在层次化结构中，可以使用外观模式定义系统中每一层的入口。实现我们将创建一个 Shape 接口和实现了 Shape 接口的实体类。下一步是定义一个外观类 ShapeMaker。ShapeMaker 类使用实体类来代表用户对这些类的调用。FacadePatternDemo，我们的演示类使用 ShapeMaker 类来显示结果。 步骤 1 创建一个接口。 Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2 创建实现接口的实体类。 Rectangle.java 1234567public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125; Square.java 1234567public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Square::draw()&quot;); &#125;&#125; Circle.java 1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125; 步骤 3 创建一个外观类。 ShapeMaker.java 123456789101112131415161718192021public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 步骤 4 使用该外观类画出各种类型的形状。 FacadePatternDemo.java 123456789public class FacadePatternDemo &#123; public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125;&#125; 步骤 5 验证输出。 123Circle::draw()Rectangle::draw()Square::draw() 享元模式享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。这种类型的设计模式属于结构型模式，它提供了减少对象数量从而改善应用所需的对象结构的方式。享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。我们将通过创建 5 个对象来画出 20 个分布于不同位置的圆来演示这种模式。由于只有 5 种可用的颜色，所以 color 属性被用来检查现有的 Circle 对象。意图：运用共享技术有效地支持大量细粒度的对象。主要解决：在有大量对象时，有可能会造成内存溢出，我们把其中共同的部分抽象出来，如果有相同的业务请求，直接返回在内存中已有的对象，避免重新创建。何时使用： 1、系统中有大量对象。 2、这些对象消耗大量内存。 3、这些对象的状态大部分可以外部化。 4、这些对象可以按照内蕴状态分为很多组，当把外蕴对象从对象中剔除出来时，每一组对象都可以用一个对象来代替。 5、系统不依赖于这些对象身份，这些对象是不可分辨的。如何解决：用唯一标识码判断，如果在内存中有，则返回这个唯一标识码所标识的对象。关键代码：用 HashMap 存储这些对象。应用实例： 1、JAVA 中的 String，如果有则返回，如果没有则创建一个字符串保存在字符串缓存池里面。 2、数据库的数据池。优点：大大减少对象的创建，降低系统的内存，使效率提高。缺点：提高了系统的复杂度，需要分离出外部状态和内部状态，而且外部状态具有固有化的性质，不应该随着内部状态的变化而变化，否则会造成系统的混乱。使用场景： 1、系统有大量相似对象。 2、需要缓冲池的场景。注意事项： 1、注意划分外部状态和内部状态，否则可能会引起线程安全问题。 2、这些类必须有一个工厂对象加以控制。实现我们将创建一个 Shape 接口和实现了 Shape 接口的实体类 Circle。下一步是定义工厂类 ShapeFactory。ShapeFactory 有一个 Circle 的 HashMap，其中键名为 Circle 对象的颜色。无论何时接收到请求，都会创建一个特定颜色的圆。ShapeFactory 检查它的 HashMap 中的 circle 对象，如果找到 Circle 对象，则返回该对象，否则将创建一个存储在 hashmap 中以备后续使用的新对象，并把该对象返回到客户端。FlyWeightPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（red / green / blue/ black / white），以便获取它所需对象的颜色。 步骤 1 创建一个接口。 Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2 创建实现接口的实体类。 Circle.java 12345678910111213141516171819202122232425262728public class Circle implements Shape &#123; private String color; private int x; private int y; private int radius; public Circle(String color)&#123; this.color = color; &#125; public void setX(int x) &#123; this.x = x; &#125; public void setY(int y) &#123; this.y = y; &#125; public void setRadius(int radius) &#123; this.radius = radius; &#125; @Override public void draw() &#123; System.out.println(&quot;Circle: Draw() [Color : &quot; + color +&quot;, x : &quot; + x +&quot;, y :&quot; + y +&quot;, radius :&quot; + radius); &#125;&#125; 步骤 3 创建一个工厂，生成基于给定信息的实体类的对象。 ShapeFactory.java 12345678910111213141516import java.util.HashMap;public class ShapeFactory &#123; private static final HashMap&lt;String, Shape&gt; circleMap = new HashMap(); public static Shape getCircle(String color) &#123; Circle circle = (Circle)circleMap.get(color); if(circle == null) &#123; circle = new Circle(color); circleMap.put(color, circle); System.out.println(&quot;Creating circle of color : &quot; + color); &#125; return circle; &#125;&#125; 步骤 4 使用该工厂，通过传递颜色信息来获取实体类的对象。 FlyweightPatternDemo.java 123456789101112131415161718192021222324public class FlyweightPatternDemo &#123; private static final String colors[] = &#123; &quot;Red&quot;, &quot;Green&quot;, &quot;Blue&quot;, &quot;White&quot;, &quot;Black&quot; &#125;; public static void main(String[] args) &#123; for(int i=0; i &lt; 20; ++i) &#123; Circle circle = (Circle)ShapeFactory.getCircle(getRandomColor()); circle.setX(getRandomX()); circle.setY(getRandomY()); circle.setRadius(100); circle.draw(); &#125; &#125; private static String getRandomColor() &#123; return colors[(int)(Math.random()*colors.length)]; &#125; private static int getRandomX() &#123; return (int)(Math.random()*100 ); &#125; private static int getRandomY() &#123; return (int)(Math.random()*100); &#125;&#125; 步骤 5 验证输出。 12345678910111213141516171819202122232425Creating circle of color : BlackCircle: Draw() [Color : Black, x : 36, y :71, radius :100Creating circle of color : GreenCircle: Draw() [Color : Green, x : 27, y :27, radius :100Creating circle of color : WhiteCircle: Draw() [Color : White, x : 64, y :10, radius :100Creating circle of color : RedCircle: Draw() [Color : Red, x : 15, y :44, radius :100Circle: Draw() [Color : Green, x : 19, y :10, radius :100Circle: Draw() [Color : Green, x : 94, y :32, radius :100Circle: Draw() [Color : White, x : 69, y :98, radius :100Creating circle of color : BlueCircle: Draw() [Color : Blue, x : 13, y :4, radius :100Circle: Draw() [Color : Green, x : 21, y :21, radius :100Circle: Draw() [Color : Blue, x : 55, y :86, radius :100Circle: Draw() [Color : White, x : 90, y :70, radius :100Circle: Draw() [Color : Green, x : 78, y :3, radius :100Circle: Draw() [Color : Green, x : 64, y :89, radius :100Circle: Draw() [Color : Blue, x : 3, y :91, radius :100Circle: Draw() [Color : Blue, x : 62, y :82, radius :100Circle: Draw() [Color : Green, x : 97, y :61, radius :100Circle: Draw() [Color : Green, x : 86, y :12, radius :100Circle: Draw() [Color : Green, x : 38, y :93, radius :100Circle: Draw() [Color : Red, x : 76, y :82, radius :100Circle: Draw() [Color : Blue, x : 95, y :82, radius :100 代理模式在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。意图：为其他对象提供一种代理以控制对这个对象的访问。主要解决：在直接访问对象时带来的问题，比如说：要访问的对象在远程的机器上。在面向对象系统中，有些对象由于某些原因（比如对象创建开销很大，或者某些操作需要安全控制，或者需要进程外的访问），直接访问会给使用者或者系统结构带来很多麻烦，我们可以在访问此对象时加上一个对此对象的访问层。何时使用：想在访问一个类时做一些控制。如何解决：增加中间层。关键代码：实现与被代理类组合。应用实例： 1、Windows 里面的快捷方式。 2、猪八戒去找高翠兰结果是孙悟空变的，可以这样理解：把高翠兰的外貌抽象出来，高翠兰本人和孙悟空都实现了这个接口，猪八戒访问高翠兰的时候看不出来这个是孙悟空，所以说孙悟空是高翠兰代理类。 3、买火车票不一定在火车站买，也可以去代售点。 4、一张支票或银行存单是账户中资金的代理。支票在市场交易中用来代替现金，并提供对签发人账号上资金的控制。 5、spring aop。优点： 1、职责清晰。 2、高扩展性。 3、智能化。缺点： 1、由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 2、实现代理模式需要额外的工作，有些代理模式的实现非常复杂。使用场景：按职责来划分，通常有以下使用场景： 1、远程代理。 2、虚拟代理。 3、Copy-on-Write 代理。 4、保护（Protect or Access）代理。 5、Cache代理。 6、防火墙（Firewall）代理。 7、同步化（Synchronization）代理。 8、智能引用（Smart Reference）代理。注意事项： 1、和适配器模式的区别：适配器模式主要改变所考虑对象的接口，而代理模式不能改变所代理类的接口。 2、和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。实现我们将创建一个 Image 接口和实现了 Image 接口的实体类。ProxyImage 是一个代理类，减少 RealImage 对象加载的内存占用。ProxyPatternDemo，我们的演示类使用 ProxyImage 来获取要加载的 Image 对象，并按照需求进行显示。 步骤 1 创建一个接口。 Image.java 123public interface Image &#123; void display();&#125; 步骤 2 创建实现接口的实体类。 RealImage.java 123456789101112131415161718public class RealImage implements Image &#123; private String fileName; public RealImage(String fileName)&#123; this.fileName = fileName; loadFromDisk(fileName); &#125; @Override public void display() &#123; System.out.println(&quot;Displaying &quot; + fileName); &#125; private void loadFromDisk(String fileName)&#123; System.out.println(&quot;Loading &quot; + fileName); &#125;&#125; ProxyImage.java 1234567891011121314151617public class ProxyImage implements Image&#123; private RealImage realImage; private String fileName; public ProxyImage(String fileName)&#123; this.fileName = fileName; &#125; @Override public void display() &#123; if(realImage == null)&#123; realImage = new RealImage(fileName); &#125; realImage.display(); &#125;&#125; 步骤 3 当被请求时，使用 ProxyImage 来获取 RealImage 类的对象。 ProxyPatternDemo.java 123456789101112public class ProxyPatternDemo &#123; public static void main(String[] args) &#123; Image image = new ProxyImage(&quot;test_10mb.jpg&quot;); //图像将从磁盘加载 image.display(); System.out.println(&quot;&quot;); //图像将无法从磁盘加载 image.display(); &#125;&#125; 步骤 4 验证输出。 1234Loading test_10mb.jpgDisplaying test_10mb.jpgDisplaying test_10mb.jpg 责任链模式顾名思义，责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。这种类型的设计模式属于行为型模式。在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。意图：避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。主要解决：职责链上的处理者负责处理请求，客户只需要将请求发送到职责链上即可，无须关心请求的处理细节和请求的传递，所以职责链将请求的发送者和请求的处理者解耦了。何时使用：在处理消息的时候以过滤很多道。如何解决：拦截的类都实现统一接口。关键代码：Handler 里面聚合它自己，在 HanleRequest 里判断是否合适，如果没达到条件则向下传递，向谁传递之前 set 进去。应用实例： 1、红楼梦中的”击鼓传花”。 2、JS 中的事件冒泡。 3、JAVA WEB 中 Apache Tomcat 对 Encoding 的处理，Struts2 的拦截器，jsp servlet 的 Filter。优点： 1、降低耦合度。它将请求的发送者和接收者解耦。 2、简化了对象。使得对象不需要知道链的结构。 3、增强给对象指派职责的灵活性。通过改变链内的成员或者调动它们的次序，允许动态地新增或者删除责任。 4、增加新的请求处理类很方便。缺点： 1、不能保证请求一定被接收。 2、系统性能将受到一定影响，而且在进行代码调试时不太方便，可能会造成循环调用。 3、可能不容易观察运行时的特征，有碍于除错。使用场景： 1、有多个对象可以处理同一个请求，具体哪个对象处理该请求由运行时刻自动确定。 2、在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 3、可动态指定一组对象处理请求。注意事项：在 JAVA WEB 中遇到很多应用。实现 我们创建抽象类 AbstractLogger，带有详细的日志记录级别。然后我们创建三种类型的记录器，都扩展了 AbstractLogger。每个记录器消息的级别是否属于自己的级别，如果是则相应地打印出来，否则将不打印并把消息传给下一个记录器。 步骤 1 创建抽象的记录器类。 AbstractLogger.java 1234567891011121314151617181920212223242526public abstract class AbstractLogger &#123; public static int INFO = 1; public static int DEBUG = 2; public static int ERROR = 3; protected int level; //责任链中的下一个元素 protected AbstractLogger nextLogger; public void setNextLogger(AbstractLogger nextLogger)&#123; this.nextLogger = nextLogger; &#125; public void logMessage(int level, String message)&#123; if(this.level &lt;= level)&#123; write(message); &#125; if(nextLogger !=null)&#123; nextLogger.logMessage(level, message); &#125; &#125; abstract protected void write(String message); &#125; 步骤 2 创建扩展了该记录器类的实体类。 ConsoleLogger.java 1234567891011public class ConsoleLogger extends AbstractLogger &#123; public ConsoleLogger(int level)&#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;Standard Console::Logger: &quot; + message); &#125;&#125; ErrorLogger.java 1234567891011public class ErrorLogger extends AbstractLogger &#123; public ErrorLogger(int level)&#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;Error Console::Logger: &quot; + message); &#125;&#125; FileLogger.java 1234567891011public class FileLogger extends AbstractLogger &#123; public FileLogger(int level)&#123; this.level = level; &#125; @Override protected void write(String message) &#123; System.out.println(&quot;File::Logger: &quot; + message); &#125;&#125; 步骤 3 创建不同类型的记录器。赋予它们不同的错误级别，并在每个记录器中设置下一个记录器。每个记录器中的下一个记录器代表的是链的一部分。 ChainPatternDemo.java 123456789101112131415161718192021222324252627public class ChainPatternDemo &#123; private static AbstractLogger getChainOfLoggers()&#123; AbstractLogger errorLogger = new ErrorLogger(AbstractLogger.ERROR); AbstractLogger fileLogger = new FileLogger(AbstractLogger.DEBUG); AbstractLogger consoleLogger = new ConsoleLogger(AbstractLogger.INFO); errorLogger.setNextLogger(fileLogger); fileLogger.setNextLogger(consoleLogger); return errorLogger; &#125; public static void main(String[] args) &#123; AbstractLogger loggerChain = getChainOfLoggers(); loggerChain.logMessage(AbstractLogger.INFO, &quot;This is an information.&quot;); loggerChain.logMessage(AbstractLogger.DEBUG, &quot;This is an debug level information.&quot;); loggerChain.logMessage(AbstractLogger.ERROR, &quot;This is an error information.&quot;); &#125;&#125; 步骤 4 验证输出。 123456Standard Console::Logger: This is an information.File::Logger: This is an debug level information.Standard Console::Logger: This is an debug level information.Error Console::Logger: This is an error information.File::Logger: This is an error information.Standard Console::Logger: This is an error information. 命令模式命令模式（Command Pattern）是一种数据驱动的设计模式，它属于行为型模式。请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。意图：将一个请求封装成一个对象，从而使您可以用不同的请求对客户进行参数化。主要解决：在软件系统中，行为请求者与行为实现者通常是一种紧耦合的关系，但某些场合，比如需要对行为进行记录、撤销或重做、事务等处理时，这种无法抵御变化的紧耦合的设计就不太合适。何时使用：在某些场合，比如要对行为进行”记录、撤销/重做、事务”等处理，这种无法抵御变化的紧耦合是不合适的。在这种情况下，如何将”行为请求者”与”行为实现者”解耦？将一组行为抽象为对象，可以实现二者之间的松耦合。如何解决：通过调用者调用接受者执行命令，顺序：调用者→接受者→命令。关键代码：定义三个角色：1、received 真正的命令执行对象 2、Command 3、invoker 使用命令对象的入口应用实例：struts 1 中的 action 核心控制器 ActionServlet 只有一个，相当于 Invoker，而模型层的类会随着不同的应用有不同的模型类，相当于具体的 Command。优点： 1、降低了系统耦合度。 2、新的命令可以很容易添加到系统中去。缺点：使用命令模式可能会导致某些系统有过多的具体命令类。使用场景：认为是命令的地方都可以使用命令模式，比如： 1、GUI 中每一个按钮都是一条命令。 2、模拟 CMD。注意事项：系统需要支持命令的撤销(Undo)操作和恢复(Redo)操作，也可以考虑使用命令模式，见命令模式的扩展。实现我们首先创建作为命令的接口 Order，然后创建作为请求的 Stock 类。实体命令类 BuyStock 和 SellStock，实现了 Order 接口，将执行实际的命令处理。创建作为调用对象的类 Broker，它接受订单并能下订单。Broker 对象使用命令模式，基于命令的类型确定哪个对象执行哪个命令。CommandPatternDemo，我们的演示类使用 Broker 类来演示命令模式。 步骤 1 创建一个命令接口。 Order.java 123public interface Order &#123; void execute();&#125; 步骤 2 创建一个请求类。 Stock.java 1234567891011121314public class Stock &#123; private String name = &quot;ABC&quot;; private int quantity = 10; public void buy()&#123; System.out.println(&quot;Stock [ Name: &quot;+name+&quot;, Quantity: &quot; + quantity +&quot; ] bought&quot;); &#125; public void sell()&#123; System.out.println(&quot;Stock [ Name: &quot;+name+&quot;, Quantity: &quot; + quantity +&quot; ] sold&quot;); &#125;&#125; 步骤 3 创建实现了 Order 接口的实体类。 BuyStock.java 1234567891011public class BuyStock implements Order &#123; private Stock abcStock; public BuyStock(Stock abcStock)&#123; this.abcStock = abcStock; &#125; public void execute() &#123; abcStock.buy(); &#125;&#125; SellStock.java 1234567891011public class SellStock implements Order &#123; private Stock abcStock; public SellStock(Stock abcStock)&#123; this.abcStock = abcStock; &#125; public void execute() &#123; abcStock.sell(); &#125;&#125; 步骤 4 创建命令调用类。 Broker.java 1234567891011121314151617import java.util.ArrayList;import java.util.List; public class Broker &#123; private List&lt;Order&gt; orderList = new ArrayList&lt;Order&gt;(); public void takeOrder(Order order)&#123; orderList.add(order); &#125; public void placeOrders()&#123; for (Order order : orderList) &#123; order.execute(); &#125; orderList.clear(); &#125;&#125; 步骤 5 使用 Broker 类来接受并执行命令。 CommandPatternDemo.java 1234567891011121314public class CommandPatternDemo &#123; public static void main(String[] args) &#123; Stock abcStock = new Stock(); BuyStock buyStockOrder = new BuyStock(abcStock); SellStock sellStockOrder = new SellStock(abcStock); Broker broker = new Broker(); broker.takeOrder(buyStockOrder); broker.takeOrder(sellStockOrder); broker.placeOrders(); &#125;&#125; 步骤 6 验证输出。 12Stock [ Name: ABC, Quantity: 10 ] boughtStock [ Name: ABC, Quantity: 10 ] sold 解释器模式解释器模式（Interpreter Pattern）提供了评估语言的语法或表达式的方式，它属于行为型模式。这种模式实现了一个表达式接口，该接口解释一个特定的上下文。这种模式被用在 SQL 解析、符号处理引擎等。意图：给定一个语言，定义它的文法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子。主要解决：对于一些固定文法构建一个解释句子的解释器。何时使用：如果一种特定类型的问题发生的频率足够高，那么可能就值得将该问题的各个实例表述为一个简单语言中的句子。这样就可以构建一个解释器，该解释器通过解释这些句子来解决该问题。 如何解决：构件语法树，定义终结符与非终结符。 关键代码：构件环境类，包含解释器之外的一些全局信息，一般是 HashMap。 应用实例：编译器、运算表达式计算。 优点： 1、可扩展性比较好，灵活。 2、增加了新的解释表达式的方式。 3、易于实现简单文法。 缺点： 1、可利用场景比较少。 2、对于复杂的文法比较难维护。 3、解释器模式会引起类膨胀。 4、解释器模式采用递归调用方法。 使用场景： 1、可以将一个需要解释执行的语言中的句子表示为一个抽象语法树。 2、一些重复出现的问题可以用一种简单的语言来进行表达。 3、一个简单语法需要解释的场景。注意事项：可利用场景比较少，JAVA 中如果碰到可以用 expression4J 代替。 实现 **我们将创建一个接口 Expression 和实现了 Expression 接口的实体类。定义作为上下文中主要解释器的 TerminalExpression 类。其他的类 OrExpression、AndExpression 用于创建组合式表达式。InterpreterPatternDemo，我们的演示类使用 Expression 类创建规则和演示表达式的解析。 步骤 1 创建一个表达式接口。 Expression.java 123public interface Expression &#123; public boolean interpret(String context);&#125; 步骤 2 创建实现了上述接口的实体类。 TerminalExpression.java 12345678910111213141516public class TerminalExpression implements Expression &#123; private String data; public TerminalExpression(String data)&#123; this.data = data; &#125; @Override public boolean interpret(String context) &#123; if(context.contains(data))&#123; return true; &#125; return false; &#125;&#125; OrExpression.java 123456789101112131415public class OrExpression implements Expression &#123; private Expression expr1 = null; private Expression expr2 = null; public OrExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) || expr2.interpret(context); &#125;&#125; AndExpression.java 123456789101112131415public class AndExpression implements Expression &#123; private Expression expr1 = null; private Expression expr2 = null; public AndExpression(Expression expr1, Expression expr2) &#123; this.expr1 = expr1; this.expr2 = expr2; &#125; @Override public boolean interpret(String context) &#123; return expr1.interpret(context) &amp;&amp; expr2.interpret(context); &#125;&#125; 步骤 3InterpreterPatternDemo 使用 Expression 类来创建规则，并解析它们。 InterpreterPatternDemo.java 12345678910111213141516171819202122232425public class InterpreterPatternDemo &#123; //规则：Robert 和 John 是男性 public static Expression getMaleExpression()&#123; Expression robert = new TerminalExpression(&quot;Robert&quot;); Expression john = new TerminalExpression(&quot;John&quot;); return new OrExpression(robert, john); &#125; //规则：Julie 是一个已婚的女性 public static Expression getMarriedWomanExpression()&#123; Expression julie = new TerminalExpression(&quot;Julie&quot;); Expression married = new TerminalExpression(&quot;Married&quot;); return new AndExpression(julie, married); &#125; public static void main(String[] args) &#123; Expression isMale = getMaleExpression(); Expression isMarriedWoman = getMarriedWomanExpression(); System.out.println(&quot;John is male? &quot; + isMale.interpret(&quot;John&quot;)); System.out.println(&quot;Julie is a married women? &quot; + isMarriedWoman.interpret(&quot;Married Julie&quot;)); &#125;&#125; 步骤 4 验证输出。 12John is male? trueJulie is a married women? true 迭代器模式迭代器模式（Iterator Pattern）是 Java 和 .Net 编程环境中非常常用的设计模式。这种模式用于顺序访问集合对象的元素，不需要知道集合对象的底层表示。迭代器模式属于行为型模式。意图：提供一种方法顺序访问一个聚合对象中各个元素, 而又无须暴露该对象的内部表示。主要解决：不同的方式来遍历整个整合对象。何时使用：遍历一个聚合对象。如何解决：把在元素之间游走的责任交给迭代器，而不是聚合对象。关键代码：定义接口：hasNext, next。应用实例：JAVA 中的 iterator。优点： 1、它支持以不同的方式遍历一个聚合对象。 2、迭代器简化了聚合类。 3、在同一个聚合上可以有多个遍历。 4、在迭代器模式中，增加新的聚合类和迭代器类都很方便，无须修改原有代码。缺点：由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。使用场景： 1、访问一个聚合对象的内容而无须暴露它的内部表示。 2、需要为聚合对象提供多种遍历方式。 3、为遍历不同的聚合结构提供一个统一的接口。注意事项：迭代器模式就是分离了集合对象的遍历行为，抽象出一个迭代器类来负责，这样既可以做到不暴露集合的内部结构，又可让外部代码透明地访问集合内部的数据。实现我们将创建一个叙述导航方法的 Iterator 接口和一个返回迭代器的 Container 接口。实现了 Container 接口的实体类将负责实现 Iterator 接口。IteratorPatternDemo，我们的演示类使用实体类 NamesRepository 来打印 NamesRepository 中存储为集合的 Names。 步骤 1 创建接口。 Iterator.java 1234public interface Iterator &#123; public boolean hasNext(); public Object next();&#125; Container.java 123public interface Container &#123; public Iterator getIterator();&#125; 步骤 2 创建实现了 Container 接口的实体类。该类有实现了 Iterator 接口的内部类 NameIterator。 NameRepository.java 1234567891011121314151617181920212223242526272829public class NameRepository implements Container &#123; public String names[] = &#123;&quot;Robert&quot; , &quot;John&quot; ,&quot;Julie&quot; , &quot;Lora&quot;&#125;; @Override public Iterator getIterator() &#123; return new NameIterator(); &#125; private class NameIterator implements Iterator &#123; int index; @Override public boolean hasNext() &#123; if(index &lt; names.length)&#123; return true; &#125; return false; &#125; @Override public Object next() &#123; if(this.hasNext())&#123; return names[index++]; &#125; return null; &#125; &#125;&#125; 步骤 3 使用 NameRepository 来获取迭代器，并打印名字。 IteratorPatternDemo.java 1234567891011public class IteratorPatternDemo &#123; public static void main(String[] args) &#123; NameRepository namesRepository = new NameRepository(); for(Iterator iter = namesRepository.getIterator(); iter.hasNext();)&#123; String name = (String)iter.next(); System.out.println(&quot;Name : &quot; + name); &#125; &#125;&#125; 步骤 4 验证输出。 1234Name : RobertName : JohnName : JulieName : Lora 中介者模式中介者模式（Mediator Pattern）是用来降低多个对象和类之间的通信复杂性。这种模式提供了一个中介类，该类通常处理不同类之间的通信，并支持松耦合，使代码易于维护。中介者模式属于行为型模式。意图：用一个中介对象来封装一系列的对象交互，中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。主要解决：对象与对象之间存在大量的关联关系，这样势必会导致系统的结构变得很复杂，同时若一个对象发生改变，我们也需要跟踪与之相关联的对象，同时做出相应的处理。何时使用：多个类相互耦合，形成了网状结构。如何解决：将上述网状结构分离为星型结构。关键代码：对象 Colleague 之间的通信封装到一个类中单独处理。应用实例： 1、中国加入 WTO 之前是各个国家相互贸易，结构复杂，现在是各个国家通过 WTO 来互相贸易。 2、机场调度系统。 3、MVC 框架，其中C（控制器）就是 M（模型）和 V（视图）的中介者。优点： 1、降低了类的复杂度，将一对多转化成了一对一。 2、各个类之间的解耦。 3、符合迪米特原则。缺点：中介者会庞大，变得复杂难以维护。使用场景： 1、系统中对象之间存在比较复杂的引用关系，导致它们之间的依赖关系结构混乱而且难以复用该对象。 2、想通过一个中间类来封装多个类中的行为，而又不想生成太多的子类。注意事项：不应当在职责混乱的时候使用。实现我们通过聊天室实例来演示中介者模式。实例中，多个用户可以向聊天室发送消息，聊天室向所有的用户显示消息。我们将创建两个类 ChatRoom 和 User。User 对象使用 ChatRoom 方法来分享他们的消息。MediatorPatternDemo，我们的演示类使用 User 对象来显示他们之间的通信。 步骤 1 创建中介类。 ChatRoom.java 12345678import java.util.Date;public class ChatRoom &#123; public static void showMessage(User user, String message)&#123; System.out.println(new Date().toString() + &quot; [&quot; + user.getName() +&quot;] : &quot; + message); &#125;&#125; 步骤 2 创建 user 类。 User.java 12345678910111213141516171819public class User &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public User(String name)&#123; this.name = name; &#125; public void sendMessage(String message)&#123; ChatRoom.showMessage(this,message); &#125;&#125; 步骤 3 使用 User 对象来显示他们之间的通信。 MediatorPatternDemo.java 123456789public class MediatorPatternDemo &#123; public static void main(String[] args) &#123; User robert = new User(&quot;Robert&quot;); User john = new User(&quot;John&quot;); robert.sendMessage(&quot;Hi! John!&quot;); john.sendMessage(&quot;Hello! Robert!&quot;); &#125;&#125; 步骤 4 验证输出。 12Thu Jan 31 16:05:46 IST 2013 [Robert] : Hi! John!Thu Jan 31 16:05:46 IST 2013 [John] : Hello! Robert! 备忘录模式备忘录模式（Memento Pattern）保存一个对象的某个状态，以便在适当的时候恢复对象。备忘录模式属于行为型模式。意图：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。主要解决：所谓备忘录模式就是在不破坏封装的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样可以在以后将对象恢复到原先保存的状态。何时使用：很多时候我们总是需要记录一个对象的内部状态，这样做的目的就是为了允许用户取消不确定或者错误的操作，能够恢复到他原先的状态，使得他有”后悔药”可吃。如何解决：通过一个备忘录类专门存储对象状态。关键代码：客户不与备忘录类耦合，与备忘录管理类耦合。应用实例： 1、后悔药。 2、打游戏时的存档。 3、Windows 里的 ctri + z。 4、IE 中的后退。 4、数据库的事务管理。优点： 1、给用户提供了一种可以恢复状态的机制，可以使用户能够比较方便地回到某个历史的状态。 2、实现了信息的封装，使得用户不需要关心状态的保存细节。缺点：消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。使用场景： 1、需要保存/恢复数据的相关状态场景。 2、提供一个可回滚的操作。注意事项： 1、为了符合迪米特原则，还要增加一个管理备忘录的类。 2、为了节约内存，可使用原型模式+备忘录模式。实现备忘录模式使用三个类 Memento、Originator 和 CareTaker。Memento 包含了要被恢复的对象的状态。 Originator 创建并在 Memento 对象中存储状态。Caretaker 对象负责从 Memento 中恢复对象的状态。MementoPatternDemo，我们的演示类使用 CareTaker 和 Originator 对象来显示对象的状态恢复。 步骤 1 创建 Memento 类。 Memento.java 1234567891011public class Memento &#123; private String state; public Memento(String state)&#123; this.state = state; &#125; public String getState()&#123; return state; &#125; &#125; 步骤 2 创建 Originator 类。 Originator.java 12345678910111213141516171819public class Originator &#123; private String state; public void setState(String state)&#123; this.state = state; &#125; public String getState()&#123; return state; &#125; public Memento saveStateToMemento()&#123; return new Memento(state); &#125; public void getStateFromMemento(Memento Memento)&#123; state = Memento.getState(); &#125;&#125; 步骤 3 创建 CareTaker 类。 CareTaker.java 1234567891011121314import java.util.ArrayList;import java.util.List;public class CareTaker &#123; private List&lt;Memento&gt; mementoList = new ArrayList&lt;Memento&gt;(); public void add(Memento state)&#123; mementoList.add(state); &#125; public Memento get(int index)&#123; return mementoList.get(index); &#125;&#125; 步骤 4 使用 CareTaker 和 Originator 对象。 MementoPatternDemo.java 123456789101112131415161718public class MementoPatternDemo &#123; public static void main(String[] args) &#123; Originator originator = new Originator(); CareTaker careTaker = new CareTaker(); originator.setState(&quot;State #1&quot;); originator.setState(&quot;State #2&quot;); careTaker.add(originator.saveStateToMemento()); originator.setState(&quot;State #3&quot;); careTaker.add(originator.saveStateToMemento()); originator.setState(&quot;State #4&quot;); System.out.println(&quot;Current State: &quot; + originator.getState()); originator.getStateFromMemento(careTaker.get(0)); System.out.println(&quot;First saved State: &quot; + originator.getState()); originator.getStateFromMemento(careTaker.get(1)); System.out.println(&quot;Second saved State: &quot; + originator.getState()); &#125;&#125; 步骤 5 验证输出。 123Current State: State #4First saved State: State #2Second saved State: State #3 观察者模式当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。何时使用：一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知，进行广播通知。如何解决：使用面向对象技术，可以将这种依赖关系弱化。关键代码：在抽象类里有一个 ArrayList 存放观察者们。应用实例： 1、拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。 2、西游记里面悟空请求菩萨降服红孩儿，菩萨洒了一地水招来一个老乌龟，这个乌龟就是观察者，他观察菩萨洒水这个动作。优点： 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。缺点： 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。使用场景： 1、有多个子类共有的方法，且逻辑相同。 2、重要的、复杂的方法，可以考虑作为模板方法。注意事项： 1、JAVA 中已经有了对观察者模式的支持类。 2、避免循环引用。 3、如果顺序执行，某一观察者错误会导致系统卡壳，一般采用异步方式。实现观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。ObserverPatternDemo，我们的演示类使用 Subject 和实体类对象来演示观察者模式。 步骤 1 创建 Subject 类。 Subject.java 12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List;public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 步骤 2 创建 Observer 类。 Observer.java 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 步骤 3 创建实体观察者类。 BinaryObserver.java 12345678910111213public class BinaryObserver extends Observer&#123; public BinaryObserver(Subject subject)&#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println( &quot;Binary String: &quot; + Integer.toBinaryString( subject.getState() ) ); &#125;&#125; OctalObserver.java 12345678910111213public class OctalObserver extends Observer&#123; public OctalObserver(Subject subject)&#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println( &quot;Octal String: &quot; + Integer.toOctalString( subject.getState() ) ); &#125;&#125; HexaObserver.java 12345678910111213public class HexaObserver extends Observer&#123; public HexaObserver(Subject subject)&#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println( &quot;Hex String: &quot; + Integer.toHexString( subject.getState() ).toUpperCase() ); &#125;&#125; 步骤 4 使用 Subject 和实体观察者对象。 ObserverPatternDemo.java 1234567891011121314public class ObserverPatternDemo &#123; public static void main(String[] args) &#123; Subject subject = new Subject(); new HexaObserver(subject); new OctalObserver(subject); new BinaryObserver(subject); System.out.println(&quot;First state change: 15&quot;); subject.setState(15); System.out.println(&quot;Second state change: 10&quot;); subject.setState(10); &#125;&#125; 步骤 5 验证输出。 12345678First state change: 15Hex String: FOctal String: 17Binary String: 1111Second state change: 10Hex String: AOctal String: 12Binary String: 1010 状态模式在状态模式（State Pattern）中，类的行为是基于它的状态改变的。这种类型的设计模式属于行为型模式。在状态模式中，我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。介绍意图：允许对象在内部状态发生改变时改变它的行为，对象看起来好像修改了它的类。主要解决：对象的行为依赖于它的状态（属性），并且可以根据它的状态改变而改变它的相关行为。何时使用：代码中包含大量与对象状态有关的条件语句。如何解决：将各种具体的状态类抽象出来。关键代码：通常命令模式的接口中只有一个方法。而状态模式的接口中有一个或者多个方法。而且，状态模式的实现类的方法，一般返回值，或者是改变实例变量的值。也就是说，状态模式一般和对象的状态有关。实现类的方法有不同的功能，覆盖接口中的方法。状态模式和命令模式一样，也可以用于消除 if…else 等条件选择语句。应用实例： 1、打篮球的时候运动员可以有正常状态、不正常状态和超常状态。 2、曾侯乙编钟中，’钟是抽象接口’,’钟A’等是具体状态，’曾侯乙编钟’是具体环境（Context）。优点： 1、封装了转换规则。 2、枚举可能的状态，在枚举状态之前需要确定状态种类。 3、将所有与某个状态有关的行为放到一个类中，并且可以方便地增加新的状态，只需要改变对象状态即可改变对象的行为。 4、允许状态转换逻辑与状态对象合成一体，而不是某一个巨大的条件语句块。 5、可以让多个环境对象共享一个状态对象，从而减少系统中对象的个数。缺点： 1、状态模式的使用必然会增加系统类和对象的个数。 2、状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。 3、状态模式对”开闭原则”的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源代码，否则无法切换到新增状态，而且修改某个状态类的行为也需修改对应类的源代码。使用场景： 1、行为随状态改变而改变的场景。 2、条件、分支语句的代替者。注意事项：在行为受状态约束的时候使用状态模式，而且状态不超过 5 个。实现我们将创建一个 State 接口和实现了 State 接口的实体状态类。Context 是一个带有某个状态的类。StatePatternDemo，我们的演示类使用 Context 和状态对象来演示 Context 在状态改变时的行为变化。 步骤 1 创建一个接口。 State.java 123public interface State &#123; public void doAction(Context context);&#125; 步骤 2 创建实现接口的实体类。 StartState.java 1234567891011public class StartState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;Player is in start state&quot;); context.setState(this); &#125; public String toString()&#123; return &quot;Start State&quot;; &#125;&#125; StopState.java 1234567891011public class StopState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;Player is in stop state&quot;); context.setState(this); &#125; public String toString()&#123; return &quot;Stop State&quot;; &#125;&#125; 步骤 3 创建 Context 类。 Context.java 123456789101112131415public class Context &#123; private State state; public Context()&#123; state = null; &#125; public void setState(State state)&#123; this.state = state; &#125; public State getState()&#123; return state; &#125;&#125; 步骤 4 使用 Context 来查看当状态 State 改变时的行为变化。 StatePatternDemo.java 123456789101112131415public class StatePatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(); StartState startState = new StartState(); startState.doAction(context); System.out.println(context.getState().toString()); StopState stopState = new StopState(); stopState.doAction(context); System.out.println(context.getState().toString()); &#125;&#125; 步骤 5 验证输出。 1234Player is in start stateStart StatePlayer is in stop stateStop State 空对象模式在空对象模式（Null Object Pattern）中，一个空对象取代 NULL 对象实例的检查。Null 对象不是检查空值，而是反应一个不做任何动作的关系。这样的 Null 对象也可以在数据不可用的时候提供默认的行为。在空对象模式中，我们创建一个指定各种要执行的操作的抽象类和扩展该类的实体类，还创建一个未对该类做任何实现的空对象类，该空对象类将无缝地使用在需要检查空值的地方。实现我们将创建一个定义操作（在这里，是客户的名称）的 AbstractCustomer 抽象类，和扩展了 AbstractCustomer 类的实体类。工厂类 CustomerFactory 基于客户传递的名字来返回 RealCustomer 或 NullCustomer 对象。NullPatternDemo，我们的演示类使用 CustomerFactory 来演示空对象模式的用法。 步骤 1 创建一个抽象类。 AbstractCustomer.java 12345public abstract class AbstractCustomer &#123; protected String name; public abstract boolean isNil(); public abstract String getName();&#125; 步骤 2 创建扩展了上述类的实体类。 RealCustomer.java 12345678910111213141516public class RealCustomer extends AbstractCustomer &#123; public RealCustomer(String name) &#123; this.name = name; &#125; @Override public String getName() &#123; return name; &#125; @Override public boolean isNil() &#123; return false; &#125;&#125; NullCustomer.java 123456789101112public class NullCustomer extends AbstractCustomer &#123; @Override public String getName() &#123; return &quot;Not Available in Customer Database&quot;; &#125; @Override public boolean isNil() &#123; return true; &#125;&#125; 步骤 3 创建 CustomerFactory 类。 CustomerFactory.java 12345678910111213public class CustomerFactory &#123; public static final String[] names = &#123;&quot;Rob&quot;, &quot;Joe&quot;, &quot;Julie&quot;&#125;; public static AbstractCustomer getCustomer(String name)&#123; for (int i = 0; i &lt; names.length; i++) &#123; if (names[i].equalsIgnoreCase(name))&#123; return new RealCustomer(name); &#125; &#125; return new NullCustomer(); &#125;&#125; 步骤 4 使用 CustomerFactory，基于客户传递的名字，来获取 RealCustomer 或 NullCustomer 对象。 NullPatternDemo.java 123456789101112131415public class NullPatternDemo &#123; public static void main(String[] args) &#123; AbstractCustomer customer1 = CustomerFactory.getCustomer(&quot;Rob&quot;); AbstractCustomer customer2 = CustomerFactory.getCustomer(&quot;Bob&quot;); AbstractCustomer customer3 = CustomerFactory.getCustomer(&quot;Julie&quot;); AbstractCustomer customer4 = CustomerFactory.getCustomer(&quot;Laura&quot;); System.out.println(&quot;Customers&quot;); System.out.println(customer1.getName()); System.out.println(customer2.getName()); System.out.println(customer3.getName()); System.out.println(customer4.getName()); &#125;&#125; 步骤 5 验证输出。 12345CustomersRobNot Available in Customer DatabaseJulieNot Available in Customer Database 策略模式在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。意图：定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。何时使用：一个系统有许多许多类，而区分它们的只是他们直接的行为。如何解决：将这些算法封装成一个一个的类，任意地替换。关键代码：实现同一个接口。应用实例： 1、诸葛亮的锦囊妙计，每一个锦囊就是一个策略。 2、旅行的出游方式，选择骑自行车、坐汽车，每一种旅行方式都是一个策略。 3、JAVA AWT 中的 LayoutManager。优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。使用场景： 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。注意事项：如果一个系统的策略多于四个，就需要考虑使用混合模式，解决策略类膨胀的问题。实现 我们将创建一个定义活动的 Strategy 接口和实现了 Strategy 接口的实体策略类。Context 是一个使用了某种策略的类。StrategyPatternDemo，我们的演示类使用 Context 和策略对象来演示 Context 在它所配置或使用的策略改变时的行为变化。 步骤 1 创建一个接口。 Strategy.java 123public interface Strategy &#123; public int doOperation(int num1, int num2);&#125; 步骤 2 创建实现接口的实体类。 OperationAdd.java 123456public class OperationAdd implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 + num2; &#125;&#125; OperationSubstract.java 123456public class OperationSubstract implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 - num2; &#125;&#125; OperationMultiply.java 123456public class OperationMultiply implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 * num2; &#125;&#125; 步骤 3 创建 Context 类。 Context.java 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2)&#123; return strategy.doOperation(num1, num2); &#125;&#125; 步骤 4 使用 Context 来查看当它改变策略 Strategy 时的行为变化。 StrategyPatternDemo.java 123456789101112public class StrategyPatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(new OperationAdd()); System.out.println(&quot;10 + 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationSubstract()); System.out.println(&quot;10 - 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationMultiply()); System.out.println(&quot;10 * 5 = &quot; + context.executeStrategy(10, 5)); &#125;&#125; 步骤 5 验证输出。 12310 + 5 = 1510 - 5 = 510 * 5 = 50 模板模式在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。意图：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。主要解决：一些方法通用，却在每一个子类都重新写了这一方法。何时使用：有一些通用的方法。如何解决：将这些通用算法抽象出来。关键代码：在抽象类实现，其他步骤在子类实现。应用实例： 1、在造房子的时候，地基、走线、水管都一样，只有在建筑的后期才有加壁橱加栅栏等差异。 2、西游记里面菩萨定好的 81 难，这就是一个顶层的逻辑骨架。 3、spring 中对 Hibernate 的支持，将一些已经定好的方法封装起来，比如开启事务、获取 Session、关闭 Session 等，程序员不重复写那些已经规范好的代码，直接丢一个实体就可以保存。优点： 1、封装不变部分，扩展可变部分。 2、提取公共代码，便于维护。 3、行为由父类控制，子类实现。缺点：每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。使用场景： 1、有多个子类共有的方法，且逻辑相同。 2、重要的、复杂的方法，可以考虑作为模板方法。注意事项：为防止恶意操作，一般模板方法都加上 final 关键词。实现我们将创建一个定义操作的 Game 抽象类，其中，模板方法设置为 final，这样它就不会被重写。Cricket 和 Football 是扩展了 Game 的实体类，它们重写了抽象类的方法。TemplatePatternDemo，我们的演示类使用 Game 来演示模板模式的用法。 步骤 1 创建一个抽象类，它的模板方法被设置为 final。 Game.java 123456789101112131415161718public abstract class Game &#123; abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板 public final void play()&#123; //初始化游戏 initialize(); //开始游戏 startPlay(); //结束游戏 endPlay(); &#125;&#125; 步骤 2 创建扩展了上述类的实体类。 Cricket.java 1234567891011121314151617public class Cricket extends Game &#123; @Override void endPlay() &#123; System.out.println(&quot;Cricket Game Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Cricket Game Initialized! Start playing.&quot;); &#125; @Override void startPlay() &#123; System.out.println(&quot;Cricket Game Started. Enjoy the game!&quot;); &#125;&#125; Football.java 1234567891011121314151617public class Football extends Game &#123; @Override void endPlay() &#123; System.out.println(&quot;Football Game Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Football Game Initialized! Start playing.&quot;); &#125; @Override void startPlay() &#123; System.out.println(&quot;Football Game Started. Enjoy the game!&quot;); &#125;&#125; 步骤 3 使用 Game 的模板方法 play() 来演示游戏的定义方式。 TemplatePatternDemo.java 12345678910public class TemplatePatternDemo &#123; public static void main(String[] args) &#123; Game game = new Cricket(); game.play(); System.out.println(); game = new Football(); game.play(); &#125;&#125; 步骤 4 验证输出。 1234567Cricket Game Initialized! Start playing.Cricket Game Started. Enjoy the game!Cricket Game Finished!Football Game Initialized! Start playing.Football Game Started. Enjoy the game!Football Game Finished! 访问者模式在访问者模式（Visitor Pattern）中，我们使用了一个访问者类，它改变了元素类的执行算法。通过这种方式，元素的执行算法可以随着访问者改变而改变。这种类型的设计模式属于行为型模式。根据模式，元素对象已接受访问者对象，这样访问者对象就可以处理元素对象上的操作。意图：主要将数据结构与数据操作分离。 主要解决： 稳定的数据结构和易变的操作耦合问题。 何时使用： 需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作”污染”这些对象的类，使用访问者模式将这些封装到类中。 如何解决： 在被访问的类里面加一个对外提供接待访问者的接口。 关键代码： 在数据基础类里面有一个方法接受访问者，将自身引用传入访问者。 应用实例： 您在朋友家做客，您是访问者，朋友接受您的访问，您通过朋友的描述，然后对朋友的描述做出一个判断，这就是访问者模式。 优点： 1、符合单一职责原则。 2、优秀的扩展性。 3、灵活性。 缺点： 1、具体元素对访问者公布细节，违反了迪米特原则。 2、具体元素变更比较困难。 3、违反了依赖倒置原则，依赖了具体类，没有依赖抽象。 使用场景： 1、对象结构中对象对应的类很少改变，但经常需要在此对象结构上定义新的操作。 2、需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作”污染”这些对象的类，也不希望在增加新操作时修改这些类。 注意事项： 访问者可以对功能进行统一，可以做报表、UI、拦截器与过滤器。 实现我们将创建一个定义接受操作的 ComputerPart 接口。Keyboard、Mouse、Monitor 和 Computer 是实现了 ComputerPart 接口的实体类。我们将定义另一个接口 ComputerPartVisitor，它定义了访问者类的操作。Computer 使用实体访问者来执行相应的动作。VisitorPatternDemo，我们的演示类使用 Computer、ComputerPartVisitor 类来演示访问者模式的用法。 步骤 1 定义一个表示元素的接口。 ComputerPart.java 123public interface ComputerPart &#123; public void accept(ComputerPartVisitor computerPartVisitor);&#125; 步骤 2 创建扩展了上述类的实体类。 Keyboard.java 1234567public class Keyboard implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125; Monitor.java 1234567public class Monitor implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125; Mouse.java 1234567public class Mouse implements ComputerPart &#123; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; computerPartVisitor.visit(this); &#125;&#125; Computer.java 1234567891011121314151617public class Computer implements ComputerPart &#123; ComputerPart[] parts; public Computer()&#123; parts = new ComputerPart[] &#123;new Mouse(), new Keyboard(), new Monitor()&#125;; &#125; @Override public void accept(ComputerPartVisitor computerPartVisitor) &#123; for (int i = 0; i &lt; parts.length; i++) &#123; parts[i].accept(computerPartVisitor); &#125; computerPartVisitor.visit(this); &#125;&#125; 步骤 3 定义一个表示访问者的接口。 ComputerPartVisitor.java 123456public interface ComputerPartVisitor &#123; public void visit(Computer computer); public void visit(Mouse mouse); public void visit(Keyboard keyboard); public void visit(Monitor monitor);&#125; 步骤 4 创建实现了上述类的实体访问者。 ComputerPartDisplayVisitor.java 12345678910111213141516171819202122public class ComputerPartDisplayVisitor implements ComputerPartVisitor &#123; @Override public void visit(Computer computer) &#123; System.out.println(&quot;Displaying Computer.&quot;); &#125; @Override public void visit(Mouse mouse) &#123; System.out.println(&quot;Displaying Mouse.&quot;); &#125; @Override public void visit(Keyboard keyboard) &#123; System.out.println(&quot;Displaying Keyboard.&quot;); &#125; @Override public void visit(Monitor monitor) &#123; System.out.println(&quot;Displaying Monitor.&quot;); &#125;&#125; 步骤 5 使用 ComputerPartDisplayVisitor 来显示 Computer 的组成部分。 VisitorPatternDemo.java 1234567public class VisitorPatternDemo &#123; public static void main(String[] args) &#123; ComputerPart computer = new Computer(); computer.accept(new ComputerPartDisplayVisitor()); &#125;&#125; 步骤 6 验证输出。 1234Displaying Mouse.Displaying Keyboard.Displaying Monitor.Displaying Computer. MVC 模式MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。Model（模型） - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。View（视图） - 视图代表模型包含的数据的可视化。Controller（控制器） - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。实现我们将创建一个作为模型的 Student 对象。StudentView 是一个把学生详细信息输出到控制台的视图类，StudentController 是负责存储数据到 Student 对象中的控制器类，并相应地更新视图 StudentView。MVCPatternDemo，我们的演示类使用 StudentController 来演示 MVC 模式的用法。 步骤 1 创建模型。 Student.java 12345678910111213141516public class Student &#123; private String rollNo; private String name; public String getRollNo() &#123; return rollNo; &#125; public void setRollNo(String rollNo) &#123; this.rollNo = rollNo; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 步骤 2 创建视图。 StudentView.java 1234567public class StudentView &#123; public void printStudentDetails(String studentName, String studentRollNo)&#123; System.out.println(&quot;Student: &quot;); System.out.println(&quot;Name: &quot; + studentName); System.out.println(&quot;Roll No: &quot; + studentRollNo); &#125;&#125; 步骤 3 创建控制器。 StudentController.java 1234567891011121314151617181920212223242526272829public class StudentController &#123; private Student model; private StudentView view; public StudentController(Student model, StudentView view)&#123; this.model = model; this.view = view; &#125; public void setStudentName(String name)&#123; model.setName(name); &#125; public String getStudentName()&#123; return model.getName(); &#125; public void setStudentRollNo(String rollNo)&#123; model.setRollNo(rollNo); &#125; public String getStudentRollNo()&#123; return model.getRollNo(); &#125; public void updateView()&#123; view.printStudentDetails(model.getName(), model.getRollNo()); &#125; &#125; 步骤 4 使用 StudentController 方法来演示 MVC 设计模式的用法。 MVCPatternDemo.java 1234567891011121314151617181920212223242526public class MVCPatternDemo &#123; public static void main(String[] args) &#123; //从数据可获取学生记录 Student model = retriveStudentFromDatabase(); //创建一个视图：把学生详细信息输出到控制台 StudentView view = new StudentView(); StudentController controller = new StudentController(model, view); controller.updateView(); //更新模型数据 controller.setStudentName(&quot;John&quot;); controller.updateView(); &#125; private static Student retriveStudentFromDatabase()&#123; Student student = new Student(); student.setName(&quot;Robert&quot;); student.setRollNo(&quot;10&quot;); return student; &#125;&#125; 步骤 5 验证输出。 123456Student: Name: RobertRoll No: 10Student: Name: JohnRoll No: 10 业务代表模式业务代表模式（Business Delegate Pattern）用于对表示层和业务层解耦。它基本上是用来减少通信或对表示层代码中的业务层代码的远程查询功能。在业务层中我们有以下实体。客户端（Client） - 表示层代码可以是 JSP、servlet 或 UI java 代码。业务代表（Business Delegate） - 一个为客户端实体提供的入口类，它提供了对业务服务方法的访问。查询服务（LookUp Service） - 查找服务对象负责获取相关的业务实现，并提供业务对象对业务代表对象的访问。业务服务（Business Service） - 业务服务接口。实现了该业务服务的实体类，提供了实际的业务实现逻辑。实现我们将创建 Client、BusinessDelegate、BusinessService、LookUpService、JMSService 和 EJBService 来表示业务代表模式中的各种实体。BusinessDelegatePatternDemo，我们的演示类使用 BusinessDelegate 和 Client 来演示业务代表模式的用法。 步骤 1 创建 BusinessService 接口。 BusinessService.java 123public interface BusinessService &#123; public void doProcessing();&#125; 步骤 2 创建实体服务类。 EJBService.java 1234567public class EJBService implements BusinessService &#123; @Override public void doProcessing() &#123; System.out.println(&quot;Processing task by invoking EJB Service&quot;); &#125;&#125; JMSService.java 1234567public class JMSService implements BusinessService &#123; @Override public void doProcessing() &#123; System.out.println(&quot;Processing task by invoking JMS Service&quot;); &#125;&#125; 步骤 3 创建业务查询服务。 BusinessLookUp.java 123456789public class BusinessLookUp &#123; public BusinessService getBusinessService(String serviceType)&#123; if(serviceType.equalsIgnoreCase(&quot;EJB&quot;))&#123; return new EJBService(); &#125;else &#123; return new JMSService(); &#125; &#125;&#125; 步骤 4 创建业务代表。 BusinessDelegate.java 1234567891011121314public class BusinessDelegate &#123; private BusinessLookUp lookupService = new BusinessLookUp(); private BusinessService businessService; private String serviceType; public void setServiceType(String serviceType)&#123; this.serviceType = serviceType; &#125; public void doTask()&#123; businessService = lookupService.getBusinessService(serviceType); businessService.doProcessing(); &#125;&#125; 步骤 5 创建客户端。 Client.java 123456789101112public class Client &#123; BusinessDelegate businessService; public Client(BusinessDelegate businessService)&#123; this.businessService = businessService; &#125; public void doTask()&#123; businessService.doTask(); &#125;&#125; 步骤 6 使用 BusinessDelegate 和 Client 类来演示业务代表模式。 BusinessDelegatePatternDemo.java 1234567891011121314public class BusinessDelegatePatternDemo &#123; public static void main(String[] args) &#123; BusinessDelegate businessDelegate = new BusinessDelegate(); businessDelegate.setServiceType(&quot;EJB&quot;); Client client = new Client(businessDelegate); client.doTask(); businessDelegate.setServiceType(&quot;JMS&quot;); client.doTask(); &#125;&#125; 步骤 7 验证输出。 12Processing task by invoking EJB ServiceProcessing task by invoking JMS Service 组合实体模式组合实体模式（Composite Entity Pattern）用在 EJB 持久化机制中。一个组合实体是一个 EJB 实体 bean，代表了对象的图解。当更新一个组合实体时，内部依赖对象 beans 会自动更新，因为它们是由 EJB 实体 bean 管理的。以下是组合实体 bean 的参与者。组合实体（Composite Entity） - 它是主要的实体 bean。它可以是粗粒的，或者可以包含一个粗粒度对象，用于持续生命周期。粗粒度对象（Coarse-Grained Object） - 该对象包含依赖对象。它有自己的生命周期，也能管理依赖对象的生命周期。依赖对象（Dependent Object） - 依赖对象是一个持续生命周期依赖于粗粒度对象的对象。策略（Strategies） - 策略表示如何实现组合实体。实现我们将创建作为组合实体的 CompositeEntity 对象。CoarseGrainedObject 是一个包含依赖对象的类。CompositeEntityPatternDemo，我们的演示类使用 Client 类来演示组合实体模式的用法。 步骤 1 创建依赖对象。 DependentObject1.java 123456789101112public class DependentObject1 &#123; private String data; public void setData(String data)&#123; this.data = data; &#125; public String getData()&#123; return data; &#125;&#125; DependentObject2.java 123456789101112public class DependentObject2 &#123; private String data; public void setData(String data)&#123; this.data = data; &#125; public String getData()&#123; return data; &#125;&#125; 步骤 2 创建粗粒度对象。 CoarseGrainedObject.java 12345678910111213public class CoarseGrainedObject &#123; DependentObject1 do1 = new DependentObject1(); DependentObject2 do2 = new DependentObject2(); public void setData(String data1, String data2)&#123; do1.setData(data1); do2.setData(data2); &#125; public String[] getData()&#123; return new String[] &#123;do1.getData(),do2.getData()&#125;; &#125;&#125; 步骤 3 创建组合实体。 CompositeEntity.java 1234567891011public class CompositeEntity &#123; private CoarseGrainedObject cgo = new CoarseGrainedObject(); public void setData(String data1, String data2)&#123; cgo.setData(data1, data2); &#125; public String[] getData()&#123; return cgo.getData(); &#125;&#125; 步骤 4 创建使用组合实体的客户端类。 Client.java 12345678910111213public class Client &#123; private CompositeEntity compositeEntity = new CompositeEntity(); public void printData()&#123; for (int i = 0; i &lt; compositeEntity.getData().length; i++) &#123; System.out.println(&quot;Data: &quot; + compositeEntity.getData()[i]); &#125; &#125; public void setData(String data1, String data2)&#123; compositeEntity.setData(data1, data2); &#125;&#125; 步骤 5 使用 Client 来演示组合实体设计模式的用法。 CompositeEntityPatternDemo.java 123456789public class CompositeEntityPatternDemo &#123; public static void main(String[] args) &#123; Client client = new Client(); client.setData(&quot;Test&quot;, &quot;Data&quot;); client.printData(); client.setData(&quot;Second Test&quot;, &quot;Data1&quot;); client.printData(); &#125;&#125; 步骤 6 验证输出。 1234Data: TestData: DataData: Second TestData: Data1 数据访问对象模式数据访问对象模式（Data Access Object Pattern）或 DAO 模式用于把低级的数据访问 API 或操作从高级的业务服务中分离出来。以下是数据访问对象模式的参与者。数据访问对象接口（Data Access Object Interface） - 该接口定义了在一个模型对象上要执行的标准操作。数据访问对象实体类（Data Access Object concrete class） - 该类实现了上述的接口。该类负责从数据源获取数据，数据源可以是数据库，也可以是 xml，或者是其他的存储机制。模型对象/数值对象（Model Object/Value Object） - 该对象是简单的 POJO，包含了 get/set 方法来存储通过使用 DAO 类检索到的数据。实现 我们将创建一个作为模型对象或数值对象的 Student 对象。StudentDao 是数据访问对象接口。StudentDaoImpl 是实现了数据访问对象接口的实体类。DaoPatternDemo，我们的演示类使用 StudentDao 来演示数据访问对象模式的用法。 步骤 1 创建数值对象。 Student.java 12345678910111213141516171819202122232425public class Student &#123; private String name; private int rollNo; Student(String name, int rollNo)&#123; this.name = name; this.rollNo = rollNo; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getRollNo() &#123; return rollNo; &#125; public void setRollNo(int rollNo) &#123; this.rollNo = rollNo; &#125;&#125; 步骤 2 创建数据访问对象接口。 StudentDao.java 12345678import java.util.List;public interface StudentDao &#123; public List&lt;Student&gt; getAllStudents(); public Student getStudent(int rollNo); public void updateStudent(Student student); public void deleteStudent(Student student);&#125; 步骤 3 创建实现了上述接口的实体类。 StudentDaoImpl.java 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.ArrayList;import java.util.List;public class StudentDaoImpl implements StudentDao &#123; //列表是当作一个数据库 List&lt;Student&gt; students; public StudentDaoImpl()&#123; students = new ArrayList&lt;Student&gt;(); Student student1 = new Student(&quot;Robert&quot;,0); Student student2 = new Student(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; @Override public void deleteStudent(Student student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 @Override public List&lt;Student&gt; getAllStudents() &#123; return students; &#125; @Override public Student getStudent(int rollNo) &#123; return students.get(rollNo); &#125; @Override public void updateStudent(Student student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125; 步骤 4 使用 StudentDao 来演示数据访问对象模式的用法。 CompositeEntityPatternDemo.java 12345678910111213141516171819202122public class DaoPatternDemo &#123; public static void main(String[] args) &#123; StudentDao studentDao = new StudentDaoImpl(); //输出所有的学生 for (Student student : studentDao.getAllStudents()) &#123; System.out.println(&quot;Student: [RollNo : &quot; +student.getRollNo()+&quot;, Name : &quot;+student.getName()+&quot; ]&quot;); &#125; //更新学生 Student student =studentDao.getAllStudents().get(0); student.setName(&quot;Michael&quot;); studentDao.updateStudent(student); //获取学生 studentDao.getStudent(0); System.out.println(&quot;Student: [RollNo : &quot; +student.getRollNo()+&quot;, Name : &quot;+student.getName()+&quot; ]&quot;); &#125;&#125; 步骤 5 验证输出。 1234Student: [RollNo : 0, Name : Robert ]Student: [RollNo : 1, Name : John ]Student: Roll No 0, updated in the databaseStudent: [RollNo : 0, Name : Michael ] 前端控制器模式前端控制器模式（Front Controller Pattern）是用来提供一个集中的请求处理机制，所有的请求都将由一个单一的处理程序处理。该处理程序可以做认证/授权/记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。前端控制器（Front Controller） - 处理应用程序所有类型请求的单个处理程序，应用程序可以是基于 web 的应用程序，也可以是基于桌面的应用程序。调度器（Dispatcher） - 前端控制器可能使用一个调度器对象来调度请求到相应的具体处理程序。视图（View） - 视图是为请求而创建的对象。实现 我们将创建 FrontController、Dispatcher 分别当作前端控制器和调度器。HomeView 和 StudentView 表示各种为前端控制器接收到的请求而创建的视图。FrontControllerPatternDemo，我们的演示类使用 FrontController 来演示前端控制器设计模式。 步骤 1 创建视图。 HomeView.java 12345public class HomeView &#123; public void show()&#123; System.out.println(&quot;Displaying Home Page&quot;); &#125;&#125; StudentView.java 12345public class StudentView &#123; public void show()&#123; System.out.println(&quot;Displaying Student Page&quot;); &#125;&#125; 步骤 2 创建调度器 Dispatcher。 Dispatcher.java 12345678910111213141516public class Dispatcher &#123; private StudentView studentView; private HomeView homeView; public Dispatcher()&#123; studentView = new StudentView(); homeView = new HomeView(); &#125; public void dispatch(String request)&#123; if(request.equalsIgnoreCase(&quot;STUDENT&quot;))&#123; studentView.show(); &#125;else&#123; homeView.show(); &#125; &#125;&#125; 步骤 3 创建前端控制器 FrontController。 Context.java 1234567891011121314151617181920212223242526public class FrontController &#123; private Dispatcher dispatcher; public FrontController()&#123; dispatcher = new Dispatcher(); &#125; private boolean isAuthenticUser()&#123; System.out.println(&quot;User is authenticated successfully.&quot;); return true; &#125; private void trackRequest(String request)&#123; System.out.println(&quot;Page requested: &quot; + request); &#125; public void dispatchRequest(String request)&#123; //记录每一个请求 trackRequest(request); //对用户进行身份验证 if(isAuthenticUser())&#123; dispatcher.dispatch(request); &#125; &#125;&#125; 步骤 4 使用 FrontController 来演示前端控制器设计模式。 FrontControllerPatternDemo.java 1234567public class FrontControllerPatternDemo &#123; public static void main(String[] args) &#123; FrontController frontController = new FrontController(); frontController.dispatchRequest(&quot;HOME&quot;); frontController.dispatchRequest(&quot;STUDENT&quot;); &#125;&#125; 步骤 5 验证输出。 123456Page requested: HOMEUser is authenticated successfully.Displaying Home PagePage requested: STUDENTUser is authenticated successfully.Displaying Student Page 拦截过滤器模式拦截过滤器模式（Intercepting Filter Pattern）用于对应用程序的请求或响应做一些预处理/后处理。定义过滤器，并在把请求传给实际目标应用程序之前应用在请求上。过滤器可以做认证/授权/记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。过滤器（Filter） - 过滤器在请求处理程序执行请求之前或之后，执行某些任务。过滤器链（Filter Chain） - 过滤器链带有多个过滤器，并在 Target 上按照定义的顺序执行这些过滤器。Target - Target 对象是请求处理程序。过滤管理器（Filter Manager） - 过滤管理器管理过滤器和过滤器链。客户端（Client） - Client 是向 Target 对象发送请求的对象。实现我们将创建 FilterChain、FilterManager、Target、Client 作为表示实体的各种对象。AuthenticationFilter 和 DebugFilter 表示实体过滤器。InterceptingFilterDemo，我们的演示类使用 Client 来演示拦截过滤器设计模式。 步骤 1 创建过滤器接口 Filter。 Filter.java 123public interface Filter &#123; public void execute(String request);&#125; 步骤 2 创建实体过滤器。 AuthenticationFilter.java 12345public class AuthenticationFilter implements Filter &#123; public void execute(String request)&#123; System.out.println(&quot;Authenticating request: &quot; + request); &#125;&#125; DebugFilter.java 12345public class DebugFilter implements Filter &#123; public void execute(String request)&#123; System.out.println(&quot;request log: &quot; + request); &#125;&#125; 步骤 3 创建 Target。 Target.java 12345public class Target &#123; public void execute(String request)&#123; System.out.println(&quot;Executing request: &quot; + request); &#125;&#125; 步骤 4 创建过滤器链。 FilterChain.java 12345678910111213141516171819202122import java.util.ArrayList;import java.util.List;public class FilterChain &#123; private List&lt;Filter&gt; filters = new ArrayList&lt;Filter&gt;(); private Target target; public void addFilter(Filter filter)&#123; filters.add(filter); &#125; public void execute(String request)&#123; for (Filter filter : filters) &#123; filter.execute(request); &#125; target.execute(request); &#125; public void setTarget(Target target)&#123; this.target = target; &#125;&#125; 步骤 5 创建过滤管理器。 FilterManager.java 123456789101112131415public class FilterManager &#123; FilterChain filterChain; public FilterManager(Target target)&#123; filterChain = new FilterChain(); filterChain.setTarget(target); &#125; public void setFilter(Filter filter)&#123; filterChain.addFilter(filter); &#125; public void filterRequest(String request)&#123; filterChain.execute(request); &#125;&#125; 步骤 6 创建客户端 Client。 Client.java 1234567891011public class Client &#123; FilterManager filterManager; public void setFilterManager(FilterManager filterManager)&#123; this.filterManager = filterManager; &#125; public void sendRequest(String request)&#123; filterManager.filterRequest(request); &#125;&#125; 步骤 7 使用 Client 来演示拦截过滤器设计模式。 FrontControllerPatternDemo.java 1234567891011public class InterceptingFilterDemo &#123; public static void main(String[] args) &#123; FilterManager filterManager = new FilterManager(new Target()); filterManager.setFilter(new AuthenticationFilter()); filterManager.setFilter(new DebugFilter()); Client client = new Client(); client.setFilterManager(filterManager); client.sendRequest(&quot;HOME&quot;); &#125;&#125; 步骤 8 验证输出。 123Authenticating request: HOMErequest log: HOMEExecuting request: HOME 服务定位器模式服务定位器模式（Service Locator Pattern）用在我们想使用 JNDI 查询定位各种服务的时候。考虑到为某个服务查找 JNDI 的代价很高，服务定位器模式充分利用了缓存技术。在首次请求某个服务时，服务定位器在 JNDI 中查找服务，并缓存该服务对象。当再次请求相同的服务时，服务定位器会在它的缓存中查找，这样可以在很大程度上提高应用程序的性能。以下是这种设计模式的实体。服务（Service） - 实际处理请求的服务。对这种服务的引用可以在 JNDI 服务器中查找到。Context / 初始的 Context - JNDI Context 带有对要查找的服务的引用。服务定位器（Service Locator） - 服务定位器是通过 JNDI 查找和缓存服务来获取服务的单点接触。缓存（Cache） - 缓存存储服务的引用，以便复用它们。客户端（Client） - Client 是通过 ServiceLocator 调用服务的对象。实现我们将创建 ServiceLocator、InitialContext、Cache、Service 作为表示实体的各种对象。Service1 和 Service2 表示实体服务。ServiceLocatorPatternDemo，我们的演示类在这里是作为一个客户端，将使用 ServiceLocator 来演示服务定位器设计模式。 步骤 1 创建服务接口 Service。 Service.java 1234public interface Service &#123; public String getName(); public void execute();&#125; 步骤 2 创建实体服务。 Service1.java 12345678910public class Service1 implements Service &#123; public void execute()&#123; System.out.println(&quot;Executing Service1&quot;); &#125; @Override public String getName() &#123; return &quot;Service1&quot;; &#125;&#125; Service2.java 12345678910public class Service2 implements Service &#123; public void execute()&#123; System.out.println(&quot;Executing Service2&quot;); &#125; @Override public String getName() &#123; return &quot;Service2&quot;; &#125;&#125; 步骤 3 为 JNDI 查询创建 InitialContext。 InitialContext.java 123456789101112public class InitialContext &#123; public Object lookup(String jndiName)&#123; if(jndiName.equalsIgnoreCase(&quot;SERVICE1&quot;))&#123; System.out.println(&quot;Looking up and creating a new Service1 object&quot;); return new Service1(); &#125;else if (jndiName.equalsIgnoreCase(&quot;SERVICE2&quot;))&#123; System.out.println(&quot;Looking up and creating a new Service2 object&quot;); return new Service2(); &#125; return null; &#125;&#125; 步骤 4 创建缓存 Cache。 Cache.java 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.List;public class Cache &#123; private List&lt;Service&gt; services; public Cache()&#123; services = new ArrayList&lt;Service&gt;(); &#125; public Service getService(String serviceName)&#123; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(serviceName))&#123; System.out.println(&quot;Returning cached &quot;+serviceName+&quot; object&quot;); return service; &#125; &#125; return null; &#125; public void addService(Service newService)&#123; boolean exists = false; for (Service service : services) &#123; if(service.getName().equalsIgnoreCase(newService.getName()))&#123; exists = true; &#125; &#125; if(!exists)&#123; services.add(newService); &#125; &#125;&#125; 步骤 5 创建服务定位器。 ServiceLocator.java 123456789101112131415161718192021public class ServiceLocator &#123; private static Cache cache; static &#123; cache = new Cache(); &#125; public static Service getService(String jndiName)&#123; Service service = cache.getService(jndiName); if(service != null)&#123; return service; &#125; InitialContext context = new InitialContext(); Service service1 = (Service)context.lookup(jndiName); cache.addService(service1); return service1; &#125;&#125; 步骤 6 使用 ServiceLocator 来演示服务定位器设计模式。 ServiceLocatorPatternDemo.java 123456789101112public class ServiceLocatorPatternDemo &#123; public static void main(String[] args) &#123; Service service = ServiceLocator.getService(&quot;Service1&quot;); service.execute(); service = ServiceLocator.getService(&quot;Service2&quot;); service.execute(); service = ServiceLocator.getService(&quot;Service1&quot;); service.execute(); service = ServiceLocator.getService(&quot;Service2&quot;); service.execute(); &#125;&#125; 步骤 7 验证输出。 12345678Looking up and creating a new Service1 objectExecuting Service1Looking up and creating a new Service2 objectExecuting Service2Returning cached Service1 objectExecuting Service1Returning cached Service2 objectExecuting Service2 传输对象模式传输对象模式（Transfer Object Pattern）用于从客户端向服务器一次性传递带有多个属性的数据。传输对象也被称为数值对象。传输对象是一个具有 getter/setter 方法的简单的 POJO 类，它是可序列化的，所以它可以通过网络传输。它没有任何的行为。服务器端的业务类通常从数据库读取数据，然后填充 POJO，并把它发送到客户端或按值传递它。对于客户端，传输对象是只读的。客户端可以创建自己的传输对象，并把它传递给服务器，以便一次性更新数据库中的数值。以下是这种设计模式的实体。业务对象（Business Object） - 为传输对象填充数据的业务服务。传输对象（Transfer Object） - 简单的 POJO，只有设置/获取属性的方法。客户端（Client） - 客户端可以发送请求或者发送传输对象到业务对象。实现我们将创建一个作为业务对象的 StudentBO 和作为传输对象的 StudentVO，它们都代表了我们的实体。TransferObjectPatternDemo，我们的演示类在这里是作为一个客户端，将使用 StudentBO 和 Student 来演示传输对象设计模式。 步骤 1 创建传输对象。 StudentVO.java 12345678910111213141516171819202122232425public class StudentVO &#123; private String name; private int rollNo; StudentVO(String name, int rollNo)&#123; this.name = name; this.rollNo = rollNo; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getRollNo() &#123; return rollNo; &#125; public void setRollNo(int rollNo) &#123; this.rollNo = rollNo; &#125;&#125; 步骤 2 创建业务对象。 StudentBO.java 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.List;public class StudentBO &#123; //列表是当作一个数据库 List&lt;StudentVO&gt; students; public StudentBO()&#123; students = new ArrayList&lt;StudentVO&gt;(); StudentVO student1 = new StudentVO(&quot;Robert&quot;,0); StudentVO student2 = new StudentVO(&quot;John&quot;,1); students.add(student1); students.add(student2); &#125; public void deleteStudent(StudentVO student) &#123; students.remove(student.getRollNo()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, deleted from database&quot;); &#125; //从数据库中检索学生名单 public List&lt;StudentVO&gt; getAllStudents() &#123; return students; &#125; public StudentVO getStudent(int rollNo) &#123; return students.get(rollNo); &#125; public void updateStudent(StudentVO student) &#123; students.get(student.getRollNo()).setName(student.getName()); System.out.println(&quot;Student: Roll No &quot; + student.getRollNo() +&quot;, updated in the database&quot;); &#125;&#125; 步骤 3 使用 StudentBO 来演示传输对象设计模式。 TransferObjectPatternDemo.java 123456789101112131415161718192021public class TransferObjectPatternDemo &#123; public static void main(String[] args) &#123; StudentBO studentBusinessObject = new StudentBO(); //输出所有的学生 for (StudentVO student : studentBusinessObject.getAllStudents()) &#123; System.out.println(&quot;Student: [RollNo : &quot; +student.getRollNo()+&quot;, Name : &quot;+student.getName()+&quot; ]&quot;); &#125; //更新学生 StudentVO student =studentBusinessObject.getAllStudents().get(0); student.setName(&quot;Michael&quot;); studentBusinessObject.updateStudent(student); //获取学生 studentBusinessObject.getStudent(0); System.out.println(&quot;Student: [RollNo : &quot; +student.getRollNo()+&quot;, Name : &quot;+student.getName()+&quot; ]&quot;); &#125;&#125; 步骤 4 验证输出。 1234Student: [RollNo : 0, Name : Robert ]Student: [RollNo : 1, Name : John ]Student: Roll No 0, updated in the databaseStudent: [RollNo : 0, Name : Michael ] 相关资料本章列出了设计模式相关的网站、书籍和文章。 设计模式相关的网站 Wiki Page for Design Patterns) - 以一种非常通用的方式检查设计模式。Java Programming/Design Patterns - 一篇关于设计模式的好文章。The JavaTM Tutorials - 该 Java 教程是为那些想用 Java 编程语言创建应用程序的编程人员提供的实用指南。JavaTM 2 SDK, Standard Edition - JavaTM 2 SDK, Standard Edition 的官网。Java DesignPatterns - 关于设计模式的短文。 Java 设计模式有用的书籍Java Design PatternsHead First Design PatternsJava Design Pattern EssentialsDesign Patterns: Elements of Reusable Object-Oriented SoftwareDesign Patterns in Java(TM)Design Patterns Java Workbook 原文地址]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Singleton Pattern</tag>
        <tag>Factory Pattern</tag>
        <tag>Proxy Pattern</tag>
        <tag>MVC Pattern</tag>
        <tag>Intercepting Filter Pattern</tag>
        <tag>Iterator Pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库引擎]]></title>
    <url>%2F2017%2F08%2F06%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[数据库引擎 数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可控制访问权限并快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求。 使用数据库引擎创建用于联机事务处理或联机分析处理数据的关系数据库。这包括创建用于存储数据的表和用于查看、管理和保护数据安全的数据库对象（如索引、视图和存储过程）。 数据库引擎任务在数据库引擎文档中，各主题的顺序遵循用于实现使用数据库引擎进行数据存储的系统的任务的主要顺序。 设计并创建数据库以保存系统所需的关系或XML文档实现系统以访问和更改数据库中存储的数据。包括实现网站或使用数据的应用程序，还包括生成使用SQL Server工具和实用工具以使用数据的过程。为单位或客户部署实现的系统提供日常管理支持以优化数据库的性能 MySQL数据库引擎类别 你能用的数据库引擎取决于mysql在安装的时候是如何被编译的。要添加一个新的引擎，就必须重新编译MYSQL。在缺省情况下，MYSQL支持三个引擎：ISAM、MYISAM和HEAP。另外两种类型INNODB和BERKLEY（BDB），也常常可以使用。 ISAM ISAM是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到数据库被查询的次数要远大于更新的次数。因此，ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要不足之处在于，它不支持事务处理，也不能够容错：如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。 MYISAM MYISAM是MYSQL的ISAM扩展格式和缺省的数据库引擎。除了提供ISAM里所没有的索引和字段管理的功能，MYISAM还使用一种表格锁定的机制，来优化多个并发的读写操作。其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。MYISAM还有一些有用的扩展，例如用来修复数据库文件的MYISAMCHK工具和用来恢复浪费空间的MYISAMPACK工具。 MYISAM强调了快速读取操作，这可能就是为什么MYSQL受到了WEB开发如此青睐的主要原因：在WEB开发中你所进行的大量数据操作都是读取操作。所以，大多数虚拟主机提供商和INTERNET平台提供商只允许使用MYISAM格式。 HEAP HEAP允许只驻留在内存里的临时表格。驻留在内存里让HEAP要比ISAM和MYISAM都快，但是它所管理的数据是不稳定的，而且如果在关机之前没有进行保存，那么所有的数据都会丢失。在数据行被删除的时候，HEAP也不会浪费大量的空间。HEAP表格在你需要使用SELECT表达式来选择和操控数据的时候非常有用。要记住，在用完表格之后就删除表格。 INNODB和BERKLEYDB INNODB和BERKLEYDB（BDB）数据库引擎都是造就MYSQL灵活性的技术的直接产品，这项技术就是MYSQL++ API。在使用MYSQL的时候，你所面对的每一个挑战几乎都源于ISAM和MYISAM数据库引擎不支持事务处理也不支持外来键。尽管要比ISAM和MYISAM引擎慢很多，但是INNODB和BDB包括了对事务处理和外来键的支持，这两点都是前两个引擎所没有的。如前所述，如果你的设计需要这些特性中的一者或者两者，那你就要被迫使用后两个引擎中的一个了。 mysql数据引擎更换方式查看当前数据库支持的引擎和默认的数据库引擎1show engines; 我的查询结果如下： 更改数据库引擎更改方式1：修改配置文件my.ini修改my.ini，在[mysqld]后面添加default-storage-engine=InnoDB，重启服务，数据库默认的引擎修改为InnoDB 更改方式2:在建表的时候指定 建表时指定： 1234create table mytbl( id int primary key, name varchar(50) )type=MyISAM; 更改方式3：建表后更改 alter table mytbl2 type = InnoDB; 查看修改结果1234#方式1：show table status from mytest; #方式2：show create table table_name MyIASM 和 Innodb引擎详解Innodb引擎 Innodb引擎提供了对数据库ACID事务的支持，并且实现了SQL标准的四种隔离级别，关于数据库事务与其隔离级别的内容请见数据库事务与其隔离级别。该引擎还提供了行级锁和外键约束，它的设计目标是处理大容量数据库系统，它本身其实就是基于MySQL后台的完整数据库系统，MySQL运行时Innodb会在内存中建立缓冲池，用于缓冲数据和索引。但是该引擎不支持FULLTEXT类型的索引，而且它没有保存表的行数，当SELECT COUNT(*) FROM TABLE时需要扫描全表。当需要使用数据库事务时，该引擎当然是首选。由于锁的粒度更小，写操作不会锁定全表，所以在并发较高时，使用Innodb引擎会提升效率。但是使用行级锁也不是绝对的，如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表。 内存结构和特性 MySQL区别于其他数据库的最为重要的特点就是其插件式的表存储引擎。而在众多存储引擎中，InnoDB是最为常用的存储引擎。从MySQL5.5.8版本开始，InnoDB存储引擎是默认的存储引擎。 InnoDB存储引擎支持事务，其设计目标主要面向在线事务处理(OLTP)的应用。其特点是行锁设计、支持外键，并支持非锁定读，即默认读操作不会产生锁。 InnoDB通过使用多版本并发控制(MVCC)来获取高并发性，并且实现了SQL标准的4中隔离级别，默认为REPEATABLE级别。同时，使用一种被称为next-key-locking的策略来避免幻读现象的产生。除此之外，InnoDB存储引擎还提供了插入缓冲(insert buffer)、二次写(double write)、自适应哈希索引(adaptive hash index)、预读(read ahead)等高性能和高可用的功能。 上图详细显示了InnoDB存储引擎的体系架构，从图中可见，InnoDB存储引擎由内存池，后台线程和磁盘文件三大部分组成。接下来我们就来简单了解一下内存相关的概念和原理。 缓冲池 InnoDB存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理。但是由于CPU速度和磁盘速度之间的鸿沟，基于磁盘的数据库系统通常使用缓冲池记录来提高数据库的的整体性能。 在数据库中进行读取操作，首先将从磁盘中读到的页放在缓冲池中，下次再读相同的页中时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命中，直接读取该页。否则，读取磁盘上的页。 对于数据库中页的修改操作，则首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上。页从缓冲池刷新回磁盘的操作并不是在每次页发生更新时触发，而是通过一种称为CheckPoint的机制刷新回磁盘。 所以，缓冲池的大小直接影响着数据库的整体性能，可以通过配置参数innodb_buffer_pool_size来设置。 具体来看，缓冲池中缓存的数据页类型有：索引页、数据页、undo页、插入缓冲(insert buffer)、自适应哈希索引(adaptive hash index)、InnoDB存储的锁信息(lock info)和数据字典信息(data dictionary)。 在架构图上可以看到，InnoDB存储引擎的内存区域除了有缓冲池之外，还有重做日志缓冲和额外内存池。InnoDB存储引擎首先将重做日志信息先放到这个缓冲区中，然后按照一定频率将其刷新到重做日志文件中。重做日志缓冲一般不需要设置的很大，该值可由配置参数innodb_log_buffer_size控制。 数据页和索引页 Page是Innodb存储的最基本结构，也是Innodb磁盘管理的最小单位，与数据库相关的所有内容都存储在Page结构里。Page分为几种类型，数据页和索引页就是其中最为重要的两种类型。 插入缓冲(Insert Buffer) 我们都知道，在InnoDB引擎上进行插入操作时，一般需要按照主键顺序进行插入，这样才能获得较高的插入性能。当一张表中存在非聚簇的且不唯一的索引时，在插入时，数据页的存放还是按照主键进行顺序存放，但是对于非聚簇索引叶节点的插入不再是顺序的了，这时就需要离散的访问非聚簇索引页，由于随机读取的存在导致插入操作性能下降。 InnoDB为此设计了Insert Buffer来进行插入优化。对于非聚簇索引的插入或者更新操作，不是每一次都直接插入到索引页中，而是先判断插入的非聚集索引是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个Insert Buffer中。看似数据库这个非聚集的索引已经查到叶节点，而实际没有，这时存放在另外一个位置。然后再以一定的频率和情况进行Insert Buffer和非聚簇索引页子节点的合并操作。这时通常能够将多个插入合并到一个操作中，这样就大大提高了对于非聚簇索引的插入性能。 两次写(Double Write) 如果说Insert Buffer给InnoDB存储引擎带来了性能上的提升，那么Double Write带给InnoDB存储引擎的是数据页的可靠性。 如上图所示，Double Write由两部分组成，一部分是内存中的double write buffer，大小为2MB，另一部分是物理磁盘上共享表空间连续的128个页，大小也为2MB。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是通过memcpy函数将脏页先复制到内存中的该区域，之后通过doublewrite buffer再分两次，每次1MB顺序地写入共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘，避免操作系统缓冲写带来的问题。在完成doublewrite页的写入后，再讲doublewirite buffer中的页写入各个表空间文件中。 如果操作系统在将页写入磁盘的过程中发生了崩溃，在恢复过程中，InnoDB存储引擎可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件中，再应用重做日志。 重做日志(Redo Log Buffer) 当缓冲池中的页的版本比磁盘要新时，数据库需要将新版本的页从缓冲池刷新到磁盘。但是如果每次一个页发送变化，就进行刷新，那么性能开发是非常大的，于是InnoDB采用了Write Ahead Log策略，即当事务提交时，先写重做日志，然后再择时将脏页写入磁盘。如果发生宕机导致数据丢失，就通过重做日志进行数据恢复。 InnoDB存储引擎会首先将重做日志信息先放入重做日志缓冲中，然后再按照一定频率将其刷新到重做日志文件。重做日志缓冲一般不需要设置得很大，因为一般情况每一秒钟都会讲重做日志缓冲刷新到日志文件中。可通过配置参数innodb_log_buffer_size控制，默认为8MB。 除了每秒刷新机制之外，每次事务提交时重做日志缓冲也会刷新到日志中。InnoDB是事务的存储引擎，其通过Force Log at Commit机制实现事务的持久性，即当事务提交时，必须先将该事务的所有日志写入到重做日志文件进行持久化，然后事务的提交操作完成才算完成。InnoDB的写入机制大致入下图所示。 为了确保每次日志都写入到重做日志文件，在每次讲重做日志缓冲写入重做日志后，必须调用一次fsync操作，将缓冲文件从文件系统缓存中真正写入磁盘。 可以通过innodb_flush_log_at_trx_commit来控制重做日志刷新到磁盘的策略。该参数默认值为1，表示事务提交必须进行一次fsync操作，还可以设置为0和2。0表示事务提交时不进行写入重做日志操作，该操作只在主线程中完成，2表示提交时写入重做日志，但是只写入文件系统缓存，不进行fsync操作。由此可见，设置为0时，性能最高，但是丧失了事务的一致性。 自适应哈希索引(Adaptive Hash Index) InnoDB会根据访问的频率和模式，为热点页建立哈希索引，来提高查询效率。InnoDB存储引擎会监控对表上各个索引页的查询，如果观察到建立哈希索引可以带来速度上的提升，则建立哈希索引，所以叫做自适应哈希索引。 自适应哈希索引是通过缓冲池的B+树页构建而来，因此建立速度很快，而且不需要对整张数据表建立哈希索引。其有一个要求，即对这个页的连续访问模式必须是一样的，也就是说其查询的条件(WHERE)必须完全一样，而且必须是连续的。 锁信息(lock info) 我们都知道，InnoDB存储引擎会在行级别上对表数据进行上锁。不过InnoDB也会在数据库内部其他很多地方使用锁，从而允许对多种不同资源提供并发访问。数据库系统使用锁是为了支持对共享资源进行并发访问，提供数据的完整性和一致性。关于锁的具体知识我们之后再进行详细学习。 数据字典信息(Data Dictionary) InnoDB有自己的表缓存，可以称为表定义缓存或者数据字典。当InnoDB打开一张表，就增加一个对应的对象到数据字典。 数据字典是对数据库中的数据、库对象、表对象等的元信息的集合。在MySQL中，数据字典信息内容就包括表结构、数据库名或表名、字段的数据类型、视图、索引、表字段信息、存储过程、触发器等内容。MySQL INFORMATION_SCHEMA库提供了对数据局元数据、统计信息、以及有关MySQL server的访问信息（例如：数据库名或表名，字段的数据类型和访问权限等）。该库中保存的信息也可以称为MySQL的数据字典。 磁盘文件及落盘机制任何一个技术都有其底层的关键基础技术，这些关键技术很有可能也是其他技术的关键技术，学习这些底层技术，就可以一通百通，让你很快的掌握其他技术。如何在磁盘上存储数据，如何使用日志文件保证数据不丢失以及如何落盘，不仅是MySQL等数据库的关键技术，也是MQ消息队列或者其他中间件的关键技术之一。 上图详细显示了InnoDB存储引擎的体系架构，从图中可见，InnoDB存储引擎由内存池，后台线程和磁盘文件三大部分组成。接下来我们就来简单了解一下磁盘文件相关的概念和原理。 InnoDB的主要的磁盘文件主要分为三大块：一是系统表空间，二是用户表空间，三是redo日志文件和归档文件。二进制文件(binlog)等文件是MySQL Server层维护的文件，所以未列入InnoDB的磁盘文件中。 系统表空间和用户表空间 InnoDB系统表空间包含InnoDB数据字典(元数据以及相关对象)并且doublewrite buffer,change buffer,undo logs的存储区域。系统表空间也默认包含任何用户在系统表空间创建的表数据和索引数据。系统表空间是一个共享的表空间因为它是被多个表共享的 系统表空间是由一个或者多个数据文件组成。默认情况下,1个初始大小为10MB，名为ibdata1的系统数据文件在MySQL的data目录下被创建。用户可以使用innodb_data_file_path对数据文件的大小和数量进行配置。 innodb_data_file_path的格式如下： innodb_data_file_path=datafile1[,datafile2]... 用户可以通过多个文件组成一个表空间，同时制定文件的属性： innodb_data_file_path = /db/ibdata1:1000M;/dr2/db/ibdata2:1000M:autoextend 这里讲/db/ibdata1和/dr2/db/ibdata2两个文件组成系统表空间。如果这两个文件位于不同的磁盘上，磁盘的负载可能被平均，因此可以提高数据库的整体性能。两个文件的文件名之后都跟了属性，表示文件ibdata1的大小为1000MB，文件ibdata2的大小为1000MB，而且用完空间之后可以自动增长(autoextend)。 设置innodb_data_file_path参数之后，所以基于InnoDB存储引擎的表的数据都会记录到该系统表空间中，如果设置了参数innodb_file_per_table，则用户可以将每个基于InnoDB存储引擎的表产生一个独立的用户表空间。用户表空间的命名规则为：表名.ibd。 通过这种方式，用户不用将所有数据都存放于默认的系统表空间中，但是用户表空只存储该表的数据、索引和插入缓冲BITMAP等信息，其余信息还是存放在默认的表空间中。 上图显示InnoDB存储引擎对于文件的存储方式，其中frm文件是表结构定义文件，记录每个表的表结构定义。 重做日志文件和归档文件 默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，这就是InnoDB的重做日志文件(redo log fiel)，它记录了对于InnoDB存储引擎的事务日志。 当InnoDB的数据存储文件发生错误时，重做日志文件就能派上用场。InnoDB存储引擎可以使用重做日志文件将数据恢复为正确状态，以此来保证数据的正确性和完整性。 每个InnoDB存储引擎至少有1个重做日志文件组(group)，每个文件组下至少有2个重做日志文件，如默认的ib_logfile0和ib_logfile1。 为了得到更高的可靠性，用户可以设置多个镜像日志组，将不同的文件组放在不同的磁盘上，以此来提高重做日志的高可用性。 在日志组中每个重做日志文件的大小一致，并以循环写入的方式运行。InnoDB存储引擎先写入重做日志文件1，当文件被写满时，会切换到重做日志文件2，再当重做日志文件2也被写满时，再切换到重做日志文件1。 用户可以使用innodb_log_file_size来设置重做日志文件的大小，这对InnoDB存储引擎的性能有着非常大的影响。 如果重做日志文件设置的太大，数据丢失时，恢复时可能需要很长的时间；另一方面，如果设置的太小，重做日志文件太小会导致依据checkpoint的检查需要频繁刷新脏页到磁盘中，导致性能的抖动。 重做日志相关和Checkpoint的机制可以阅读我之前文章的相应章节。MySQL探秘(三):InnoDB的内存结构和特性 重做日志的落盘机制 InnoDB对于数据文件和日志文件的刷盘遵守WAL(Write ahead redo log) 和Force-log-at-commit两种规则，二者保证了事务的持久性。WAL要求数据的变更写入到磁盘前，首先必须将内存中的日志写入到磁盘；Force-log-at-commit要求当一个事务提交时，所有产生的日志都必须刷新到磁盘上，如果日志刷新成功后，缓冲池中的数据刷新到磁盘前数据库发生了宕机，那么重启时，数据库可以从日志中恢复数据。 如上图所示，InnoDB在缓冲池中变更数据时，会首先将相关变更写入重做日志缓冲中，然后再按时或者当事务提交时写入磁盘，这符合Force-log-at-commit原则；当重做日志写入磁盘后，缓冲池中的变更数据才会依据checkpoint机制择时写入到磁盘中，这符合WAL原则。 在checkpoint择时机制中，就有重做日志文件写满的判断，所以，如前文所述，如果重做日志文件太小，经常被写满，就会频繁导致checkpoint将更改的数据写入磁盘，导致性能抖动。 操作系统的文件系统是带有缓存的，当InnoDB向磁盘写入数据时，有可能只是写入到了文件系统的缓存中，没有真正的“落袋为安”。 InnoDB的innodb_flush_log_at_trx_commit属性可以控制每次事务提交时InnoDB的行为。当属性值为0时，事务提交时，不会对重做日志进行写入操作，而是等待主线程按时写入；当属性值为1时，事务提交时，会将重做日志写入文件系统缓存，并且调用文件系统的fsync，将文件系统缓冲中的数据真正写入磁盘存储，确保不会出现数据丢失；当属性值为2时，事务提交时，也会将日志文件写入文件系统缓存，但是不会调用fsync，而是让文件系统自己去判断何时将缓存写入磁盘。日志的刷盘机制如下图所示。 innodb_flush_log_at_commit是InnoDB性能调优的一个基础参数，涉及InnoDB的写入效率和数据安全。当参数值为0时，写入效率最高，但是数据安全最低；参数值为1时，写入效率最低，但是数据安全最高；参数值为2时，二者都是中等水平。一般建议将该属性值设置为1，以获得较高的数据安全性，而且也只有设置为1，才能保证事务的持久性。 锁的类型和状态查询锁是数据库系统区分于文件系统的一个关键特性。数据库使用锁来支持对共享资源进行并发访问，提供数据的完整性和一致性。此外，数据库事务的隔离性也是通过锁实现的。InnoDB在此方面一直优于其他数据库引擎。InnoDB会在行级别上对表数据上锁，而MyISAM只能在表级别上锁，二者性能差异可想而知。 InnoDB存储引擎中的锁 InnoDB存储引擎实现了如下两种标准的行级锁： 共享锁(S Lock)，允许事务读取一行 排他锁(X Lock)，允许事务删除或更新一行数据 如果一个事务T1已经获取了行r的共享锁，那么另外一个事务T2可以立刻获得行r的共享锁，因为读取并不会改变数据，可以进行并发的读取操作；但若其他的事务T3想要获取行r的排他锁，则必须等待事务T1和T2释放行r上的共享锁之后才能继续，因为获取排他锁一般是为了改变数据，所以不能同时进行读取或则其他写入操作。 X S X 不兼容 不兼容 S 不兼容 兼容 InnoDB存储引擎支持多粒度锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB存储引擎支持一种称为意向锁的锁方式。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁。 InnoDB存储引擎的意向锁即为表级别的锁。设计目的主要是为了在一个事务中揭示下一行将被请求的锁类型。其支持两种意向锁： 意向共享锁(IS Lock)，事务想要获得一张表中某几行的共享锁 意向排他锁(IX Lock)，事务想要获得一张表中某几行的排他锁 需要注意的是意向锁是表级别的锁，它不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突。故表级别的意向锁和表级别的锁的兼容性如下表所示。 IS IX S X IS 兼容 兼容 兼容 不兼容 IX 兼容 兼容 不兼容 不兼容 S 兼容 不兼容 兼容 不兼容 X 不兼容 不兼容 不兼容 不兼容 向一个表添加表级X锁的时候(执行ALTER TABLE, DROP TABLE, LOCK TABLES等操作)，如果没有意向锁的话，则需要遍历所有整个表判断是否有行锁的存在，以免发生冲突。如果有了意向锁，只需要判断该意向锁与即将添加的表级锁是否兼容即可。因为意向锁的存在代表了，有行级锁的存在或者即将有行级锁的存在，因而无需遍历整个表，即可获取结果。 如果将上锁的对象看成一棵树，那么对最下层的对象上锁，也就是对最细粒度的对象进行上锁，那么首先需要对粗粒度的对象上锁。如上图所示，如果需要对表1的记录m行上X锁，那么需要先对表1加意向IX锁，然后对记录m上X锁。如果其中任何一个部分导致等待，那么该操作需要等待粗粒度锁的完成。 InnoDB锁相关状态查询 用户可以使用INFOMATION_SCHEMA库下的INNODB_TRX、INNODB_LOCKS和INNODB_LOCK_WAITS表来监控当前事务并分析可能出现的锁问题。INNODB_TRX的定义如下表所示，其由8个字段组成。 trx_id：InnoDB存储引擎内部唯一的事务ID trx_state：当前事务的状态 trx_started：事务的开始时间 trx_request_lock_id：等待事务的锁ID。如果trx_state的状态为LOCK WAIT,那么该字段代表当前事务等待之前事务占用的锁资源ID trx_wait_started：事务等待的时间 trx_weight：事务的权重，反映了一个事务修改和锁住的行数，当发生死锁需要回滚时，会选择该数值最小的进行回滚 trx_mysql_thread_id：线程ID，SHOW PROCESSLIST 显示的结果 trx_query：事务运行的SQL语句 mysql&gt; SELECT * FROM information_schema.INNODB_TRX\G; ************** 1.row *********************** trx_id: 7311F4 trx_state: LOCK WAIT trx_started: 2010-01-04 10:49:33 trx_requested_lock_id: 7311F4:96:3:2 trx_wait_started: 2010-01-04 10:49:33 trx_weight: 2 trx_mysql_thread_id: 471719 trx_query: select * from parent lock in share mode INNODB_TRX表只能显示当前运行的InnoDB事务，并不能直接判断锁的一些情况。如果需要查看锁，则还需要访问表INNODB_LOCKS，该表的字段组成如下表所示。 lock_id：锁的ID lock_trx_id：事务的ID lock_mode：锁的模式 lock_type：锁的类型，表锁还是行锁 lock_table：要加锁的表 lock_index：锁住的索引 lock_space：锁住的space id lock_page：事务锁定页的数量，若是表锁，则该值为NULL lock_rec：事务锁定行的数量，如果是表锁，则该值为NULL lock_data： mysql&gt; SELECT * FROM information_schema.INNODB_LOCKS\G; ***************** 1.row *********************** lock_id: 7311F4:96:3:2 lock_trx_id: 7311F4 lock_mode: S lock_type: RECORD lock_type: &#39;mytest&#39;.&#39;parent&#39; lock_index: &#39;PRIMARY&#39; lock_space: 96 lock_page: 3 lock_rec: 2 lock_data: 1 通过表INNODB_LOCKS查看每张表上锁的情况后，用户就可以来判断由此引发的等待情况。当时当事务量非常大，其中锁和等待也时常发生，这个时候就不那么容易判断。但是通过表INNODB_LOCK_WAITS，可以很直观的反应当前事务的等待。表INNODB_LOCK_WAITS由四个字段组成，如下表所示。 requesting_trx_id：申请锁资源的事务ID requesting_lock_id：申请的锁的ID blocking_trx_id：阻塞的事务ID blocking_lock_id：阻塞的锁的ID mysql&gt; SELECT * FROM information_schema.INNODB_LOCK_WAITS\G; ******************1.row************************** requesting_trx_id: 7311F4 requesting_lock_id: 7311F4:96:3:2 blocking_trx_id: 730FEE blocking_lock_id: 730FEE:96:3:2 通过上述的SQL语句，用户可以清楚直观地看到哪个事务阻塞了另一个事务，然后使用上述的事务ID和锁ID，去INNODB_TRX和INNDOB_LOCKS表中查看更加详细的信息。 一致性非锁定读一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过多版本控制(MVVC)读取当前数据库中行数据的方式。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB会去读取行的一个快照。 上图直观地展现了InnoDB一致性非锁定读的机制。之所以称其为非锁定读，是因为不需要等待行上排他锁的释放。快照数据是指该行的之前版本的数据，每行记录可能有多个版本，一般称这种技术为行多版本技术。 由此带来的并发控制，称之为多版本并发控制(Multi Version Concurrency Control, MVVC)。InnoDB是通过undo log来实现MVVC。undo log本身用来在事务中回滚数据，因此快照数据本身是没有额外开销。此外，读取快照数据是不需要上锁的，因为没有事务需要对历史的数据进行修改操作。 一致性非锁定读是InnoDB默认的读取方式，即读取不会占用和等待行上的锁。但是并不是在每个事务隔离级别下都是采用此种方式。此外，即使都是使用一致性非锁定读，但是对于快照数据的定义也各不相同。 在事务隔离级别READ COMMITTED和REPEATABLE READ下，InnoDB使用一致性非锁定读。然而，对于快照数据的定义却不同。在READ COMMITTED事务隔离级别下，一致性非锁定读总是读取被锁定行的最新一份快照数据。而在REPEATABLE READ事务隔离级别下，则读取事务开始时的行数据版本。 我们下面举个例子来详细说明一下上述的情况。 session Amysql&gt; BEGIN; mysql&gt; SELECT * FROM test WHERE id = 1; 我们首先在会话A中显示地开启一个事务，然后读取test表中的id为1的数据，但是事务并没有结束。于此同时，用户在开启另一个会话B，这样可以模拟并发的操作，然后对会话B做出如下的操作： session Bmysql&gt; BEGIN; mysql&gt; UPDATE test SET id = 3 WHERE id = 1; 在会话B的事务中，将test表中id为1的记录修改为id=3，但是事务同样也没有提交，这样id=1的行其实加了一个排他锁。由于InnoDB在READ COMMITTED和REPEATABLE READ事务隔离级别下使用一致性非锁定读，这时如果会话A再次读取id为1的记录，仍然能够读取到相同的数据。此时，READ COMMITTED和REPEATABLE READ事务隔离级别没有任何区别。 如上图所示，当会话B提交事务后，会话A再次运行SELECT * FROM test WHERE id = 1的SQL语句时，两个事务隔离级别下得到的结果就不一样了。 对于READ COMMITTED的事务隔离级别，它总是读取行的最新版本，如果行被锁定了，则读取该行版本的最新一个快照。因为会话B的事务已经提交，所以在该隔离级别下上述SQL语句的结果集是空的。 对于REPEATABLE READ的事务隔离级别，总是读取事务开始时的行数据，因此，在该隔离级别下，上述SQL语句仍然会获得相同的数据。 MVVC 我们首先来看一下wiki上对MVVC的定义： Multiversion concurrency control (MCC or MVCC), is a concurrency control method commonly used by database management systems to provide concurrent access to the database and in programming languages to implement transactional memory. 由定义可知，MVVC是用于数据库提供并发访问控制的并发控制技术。数据库的并发控制机制有很多，最为常见的就是锁机制。锁机制一般会给竞争资源加锁，阻塞读或者写操作来解决事务之间的竞争条件，最终保证事务的可串行化。 而MVVC则引入了另外一种并发控制，它让读写操作互不阻塞，每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回，由此解决了事务的竞争条件。 考虑一个现实场景。管理者要查询所有用户的存款总额，假设除了用户A和用户B之外，其他用户的存款总额都为0，A、B用户各有存款1000，所以所有用户的存款总额为2000。但是在查询过程中，用户A会向用户B进行转账操作。转账操作和查询总额操作的时序图如下图所示。 如果没有任何的并发控制机制，查询总额事务先读取了用户A的账户存款，然后转账事务改变了用户A和用户B的账户存款，最后查询总额事务继续读取了转账后的用户B的账号存款，导致最终统计的存款总额多了100元，发生错误。 使用锁机制可以解决上述的问题。查询总额事务会对读取的行加锁，等到操作结束后再释放所有行上的锁。因为用户A的存款被锁，导致转账操作被阻塞，直到查询总额事务提交并将所有锁都释放。 但是这时可能会引入新的问题，当转账操作是从用户B向用户A进行转账时会导致死锁。转账事务会先锁住用户B的数据，等待用户A数据上的锁，但是查询总额的事务却先锁住了用户A数据，等待用户B的数据上的锁。 使用MVVC机制也可以解决这个问题。查询总额事务先读取了用户A的账户存款，然后转账事务会修改用户A和用户B账户存款，查询总额事务读取用户B存款时不会读取转账事务修改后的数据，而是读取本事务开始时的数据副本(在REPEATABLE READ隔离等级下)。 MVCC使得数据库读不会对数据加锁，普通的SELECT请求不会加锁，提高了数据库的并发处理能力。借助MVCC，数据库可以实现READ COMMITTED，REPEATABLE READ等隔离级别，用户可以查看当前数据的前一个或者前几个历史版本，保证了ACID中的I特性（隔离性) InnoDB的MVVC实现 多版本并发控制仅仅是一种技术概念，并没有统一的实现标准， 其的核心理念就是数据快照，不同的事务访问不同版本的数据快照，从而实现不同的事务隔离级别。虽然字面上是说具有多个版本的数据快照，但这并不意味着数据库必须拷贝数据，保存多份数据文件，这样会浪费大量的存储空间。InnoDB通过事务的undo日志巧妙地实现了多版本的数据快照。 数据库的事务有时需要进行回滚操作，这时就需要对之前的操作进行undo。因此，在对数据进行修改时，InnoDB会产生undo log。当事务需要进行回滚时，InnoDB可以利用这些undo log将数据回滚到修改之前的样子。 根据行为的不同 undo log 分为两种 insert undo log和update undo log。 insert undo log 是在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对于其它事务此记录是不可见的，所以 insert undo log 可以在事务提交后直接删除而不需要进行 purge 操作。 update undo log 是 update 或 delete 操作中产生的 undo log，因为会对已经存在的记录产生影响，为了提供 MVCC机制，因此 update undo log 不能在事务提交时就进行删除，而是将事务提交时放到入 history list 上，等待 purge 线程进行最后的删除操作。 为了保证事务并发操作时，在写各自的undo log时不产生冲突，InnoDB采用回滚段的方式来维护undo log的并发写入和持久化。回滚段实际上是一种 Undo 文件组织方式。 InnoDB行记录有三个隐藏字段：分别对应该行的rowid、事务号db_trx_id和回滚指针db_roll_ptr，其中db_trx_id表示最近修改的事务的id，db_roll_ptr指向回滚段中的undo log。如下图所示。 当事务2使用UPDATE语句修改该行数据时，会首先使用排他锁锁定改行，将该行当前的值复制到undo log中，然后再真正地修改当前行的值，最后填写事务ID，使用回滚指针指向undo log中修改前的行。如下图所示。 当事务3进行修改与事务2的处理过程类似，如下图所示。 REPEATABLE READ隔离级别下事务开始后使用MVVC机制进行读取时，会将当时活动的事务id记录下来，记录到Read View中。READ COMMITTED隔离级别下则是每次读取时都创建一个新的Read View。 Read View是InnoDB中用于判断记录可见性的数据结构，记录了一些用于判断可见性的属性。 low_limit_id：某行记录的db_trx_id &lt; 该值，则该行对于当前Read View是一定可见的 up_limit_id：某行记录的db_trx_id &gt;= 该值，则该行对于当前read view是一定不可见的 low_limit_no：用于purge操作的判断 rw_trx_ids：读写事务数组 Read View创建后，事务再次进行读操作时比较记录的db_trx_id和Read View中的low_limit_id，up_limit_id和读写事务数组来判断可见性。 如果该行中的db_trx_id等于当前事务id，说明是事务内部发生的更改，直接返回该行数据。否则的话，如果db_trx_id小于up_limit_id，说明是事务开始前的修改，则该记录对当前Read View是可见的，直接返回该行数据。 如果db_trx_id大于或者等于low_limit_id，则该记录对于该Read View一定是不可见的。如果db_trx_id位于[up_limit_id, low_limit_id)范围内，需要在活跃读写事务数组(rw_trx_ids)中查找db_trx_id是否存在，如果存在，记录对于当前Read View是不可见的。 如果记录对于Read View不可见，需要通过记录的DB_ROLL_PTR指针遍历undo log，构造对当前Read View可见版本数据。 简单来说，Read View记录读开始时及其之后，所有的活动事务，这些事务所做的修改对于Read View是不可见的。除此之外，所有其他的小于创建Read View的事务号的所有记录均可见。 ACID A 事务的原子性(Atomicity)：指一个事务要么全部执行,要么不执行.也就是说一个事务不可能只执行了一半就停止了.比如你从取款机取钱,这个事务可以分成两个步骤:1划卡,2出钱.不可能划了卡,而钱却没出来.这两步必须同时完成.要么就不完成.C 事务的一致性(Consistency)：指事务的运行并不改变数据库中数据的一致性.例如,完整性约束了a+b=10,一个事务改变了a,那么b也应该随之改变.I 独立性(Isolation）:事务的独立性也有称作隔离性,是指两个以上的事务不会出现交错执行的状态.因为这样可能会导致数据不一致.D 持久性(Durability）:事务的持久性是指事务执行成功以后,该事务所对数据库所作的更改便是持久的保存在数据库之中，不会无缘无故的回滚. MyIASM引擎 MyIASM是MySQL默认的引擎，但是它没有提供对数据库事务的支持，也不支持行级锁和外键，因此当INSERT(插入)或UPDATE(更新)数据时即写操作需要锁定整个表，效率便会低一些。不过和Innodb不同，MyIASM中存储了表的行数，于是SELECT COUNT(*) FROM TABLE时只需要直接读取已经保存好的值而不需要进行全表扫描。如果表的读操作远远多于写操作且不需要数据库事务的支持，那么MyIASM也是很好的选择。 两种引擎的选择 大尺寸的数据集趋向于选择InnoDB引擎，因为它支持事务处理和故障恢复,支持行级锁和外键。数据库的大小决定了故障恢复的时间长短，InnoDB可以利用事务日志进行数据恢复，这会比较快。主键查询在InnoDB引擎下也会相当快，不过需要注意的是如果主键太长也会导致性能问题，关于这个问题我会在下文中讲到。大批的INSERT语句(在每个INSERT语句中写入多行，批量插入)在MyISAM下会快一些，但是UPDATE语句在InnoDB下则会更快一些，尤其是在并发量大的时候。 Index——索引 索引（Index）是帮助MySQL高效获取数据的数据结构。MyIASM和Innodb都使用了树这种数据结构做为索引。下面我接着讲这两种引擎使用的索引结构，讲到这里，首先应该谈一下B-Tree和B+Tree。 MyIASM引擎的索引结构 MyISAM引擎的索引结构为B+Tree，其中B+Tree的数据域存储的内容为实际数据的地址，也就是说它的索引和实际的数据是分开的，只不过是用索引指向了实际的数据，这种索引就是所谓的非聚集索引。如下图所示： 这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 Innodb引擎的索引结构 与MyISAM引擎的索引结构同样也是B+Tree，但是Innodb的索引文件本身就是数据文件，即B+Tree的数据域存储的就是实际的数据，这种索引就是聚集索引。这个索引的key就是数据表的主键，因此InnoDB表数据文件本身就是主索引。 并且和MyISAM不同，InnoDB的辅助索引数据域存储的也是相应记录主键的值而不是地址，所以当以辅助索引查找时，会先根据辅助索引找到主键，再根据主键索引找到实际的数据。所以Innodb不建议使用过长的主键，否则会使辅助索引变得过大。建议使用自增的字段作为主键，这样B+Tree的每一个结点都会被顺序的填满，而不会频繁的分裂调整，会有效的提升插入数据的效率。 两者区别： 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引： 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调(可能是指“非递增”的意思)的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调(可能是指“非递增”的意思)的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 MyISAM适合： (1)做很多count 的计算；(2)读多写少，插入不频繁，查询非常频繁；(3)没有事务。 InnoDB适合： (1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况指定数据引擎的创建；(3)支持行级锁，外键。]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>ISAM</tag>
        <tag>MyIsam</tag>
        <tag>HEAP</tag>
        <tag>Innodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程之线程池ThreadPoolExecutor]]></title>
    <url>%2F2017%2F08%2F06%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0ThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[ThreadPoolExecutor当前越来越多的系统使用多线程来处理任务，但是为每一个任务创建线程并不是合理的方案，原因有2点：一是创建线程的开销很大，一个任务一个线程的方式会有性能上的损失；二是可能导致线程数量的膨胀，不但不易于线程的管理，还可能导致内存被消耗完，导致out of memory（OOM）,从而使系统崩溃。为了解决这个问题，线程池应运而生。线程池有两个作用：一个是限制线程的数量，不会导致线程的膨胀；二是线程复用，线程执行完一个人任务之后，可以接着执行下一个任务，减少了创建线程的开销。 java中一个运用非常普遍的线程池是ThreadPoolExecutor。下面来探究下ThreadPoolExecutor的功能和实现原理。ThreadPoolExecutor的功能 自定义线程池的核心线程数和最大线程数。如果当前池中的线程数小于核心线程数，则直接为任务创建新线程来执行，如果前池中的线程数大于核心线程数，则把任务放入任务队列中，等待线程池中已有的线程去执行。如果任务队列满了，但是池中的线程数小于最大线程数，则创建新线程执行任务。如果任务队列满了，池中的线程数等于最大线程数，那么执行拒绝任务策略。 可配置拒绝任务策略，ThreadPoolExecutor自带了四种拒绝策略：丢弃当前将要加入队列的任务本身（DiscardPolicy），丢弃任务队列中最旧任务（DiscardOldestPolicy），抛出异常的方式（AbortPolicy），将任务交由调用者线程去执行（CallerRunsPolicy），除了自带的策略之外，用户还可以自定义策略。 线程声明周期管理。如果线程空闲时间超过了配置的时间keepAliveTime，则线程将被销毁。 配置线程工厂，用户可以自定义创建线程的工厂。 配置阻塞队列类型。 线程池生命周期管理。可以强制shutdown线程池，也可以优雅shutdown线程池。 为了实现上面的配置管理。ThreadPoolExecutor提供了不同的创建线程池的构造方法，用户可以根据自身实际情况选择。 ThreadPoolExecutor实现原理 ThreadPoolExecutor的属性 属性名 属性说明 volatile int runState runState主要提供了生命周期的控制，下面是主要的状态：RUNNING：0。接收新任务以及处理队列中的任务SHUTDOWN：1。不再接收新任务，但是处理队列中的任务STOP：2。不再接收新任务，也不处理队列中的任务，同时中断正在执行的任务TERMINATED：3。跟STOP相同，同时所有的线程都终止了。 BlockingQueue workQueue 任务队列 ReentrantLock mainLock 为poolSize, corePoolSize,maximumPoolSize, runState, and workers属性的set提供同步。 HashSet workers 保存线程池中所有的工作线程，只有获得mainLock锁才能访问 volatile long keepAliveTime 空闲线程的最大存活时间 volatile boolean allowCoreThreadTimeOut 核心线程是否也支持最大存活时间管理 volatile int corePoolSize 线程池核心线程数 volatile int maximumPoolSize 线程池最大线程数 volatile int poolSize 线程池当前线程数 int largestPoolSize 线程池峰值线程数 long completedTaskCount 线程池总共处理的任务数 volatile RejectedExecutionHandler handler 任务拒绝策略 volatile ThreadFactory threadFactory 创建线程工厂 ThreadPoolExecutor的属性，基本上大部分都是构造函数中可配置的，也说明了ThreadPoolExecutor的灵活性。不过通过上面的表大家可能会有点疑惑：怎么没有保存Thread对象集合的属性？不要急，大家应该发现了里面有个HashSet workers属性。这个集合里Worker对象是ThreadPoolExecutor定义的一个内部类，它包含了thread对象。现在我们来看下Worker对象包含的属性。 Worker对象的属性 属性名称 属性说明 inal ReentrantLock runLock 这个锁的作用是保护取消worker线程的中断，而不是中断正在执行的任务。 Runnable firstTask 由于线程池创建现在的时候都是为某个任务创建，所以该属性就是记录该刚线程创建时执行的任务 long completedTasks 这个线程执行的任务数 Thread thread 本worker运行的线程 volatile boolean hasRun 本worker对象运行的线程是否执行过该worker的run方法。只有hasRun为true时worker的线程才能被中断。 ThreadPoolExecutor的任务处理流程 前面介绍了ThreadPoolExecutor的属性以及需要用到的内部类，素材有了，那么下面来看看是如何来把素材加工成成品的吧。在用户创建完线程池之后，需要把任务提交给线程池，线程池提供了submit和execute方法来提交任务，而submit方法最终还是调用的execute方法，它只是把任务封装成futuretask，以便获得任务的返回值。对于没有返回值的任务直接用execute提交就可以了，如果有返回值的任务，用submit提交更好。所以提交任务的核心还是execute方法。现在就来看看execute的实现代码： 1.public void execute(Runnable command) { 2. if (command == null) 3. throw new NullPointerException(); 4. if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) { 5. if (runState == RUNNING &amp;&amp; workQueue.offer(command)) { 6. if (runState != RUNNING || poolSize == 0) 7. ensureQueuedTaskHandled(command); 8. } 9. else if (!addIfUnderMaximumPoolSize(command)) 10. reject(command); // is shutdown or saturated 11. } 12. } 这段代码的主要逻辑如下： 1.如果当选线程数大于等于核心线程数，则直接把任务放到任务队列里，等待已有的线程去执行它。如果当前选线程数小于核心线程数，则为该任务创建新的线程去执行它，这个的功能的实现方法是addIfUnderCorePoolSize(command)如下： 123456789101112private boolean addIfUnderCorePoolSize(Runnable firstTask) &#123; Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (poolSize &lt; corePoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); &#125; finally &#123; mainLock.unlock(); &#125; return t != null; &#125; 可以发现，这段源码是如果发现小于corePoolSize就会调用addThread()方法创建一个新的线程，并且调用线程的start()方法将线程运行起来。只有没有创建成功Thread才会返回false，也就是当当前的poolSize &gt; corePoolSize的时候，或线程池已经不是在running状态的时候才会出现。execute对poolSize和corePoolSize的比较只是粗略判断，而addIfUnderCorePoolSize（）内部是加锁后判定的，以得到更为准确的结果，而外部初步判定如果是大于了，就没有必要进入这段有锁的代码了。 2.如果addIfUnderCorePoolSize返回false，说明没有为任务创建线程（原因可能是线程池不是RUNNING状态，或者poolsize大于corepoolsize了）。则需要把任务存放到任务队列中。 3.在任务放到队列之前，先初步判断下此时线程池的状态。如果是running才接受新任务，否则addIfUnderMaximumPoolSize方法精确线程池状态。 4.如果任务可以添加到任务队列，则判调用队列的offer方法，往队列末尾加入任务。由于队列是一个自定义的阻塞队列，可以是有界也可以是无界的。如果加入队列成功，还有先判断下runState != RUNNING || poolSize == 0。前面判断了状态之后为什么还要判断呢？这是因为有时间差，状态随时可以发生改变。记住了这一点在看这样一堆状态判断就不会难以理解了。好了，如果线程池不是RUNNING状态或线程池里没有线程了，则执行ensureQueuedTaskHandled方法处理任务如下： 12345678910111213141516171819private void ensureQueuedTaskHandled(Runnable command) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); boolean reject = false; Thread t = null; try &#123; int state = runState; if (state != RUNNING &amp;&amp; workQueue.remove(command)) reject = true; else if (state &lt; STOP &amp;&amp; poolSize &lt; Math.max(corePoolSize, 1) &amp;&amp; !workQueue.isEmpty()) t = addThread(null); &#125; finally &#123; mainLock.unlock(); &#125; if (reject) reject(command); &#125; 这段代码是处理拒绝任务的。这里也会加锁来锁定当前的状态和工作队列。如果状态确实不等于running，则把任务从任务列表中移除并执行拒绝策略。如果任务remove失败，并且当前状态为running和shutdown状态，任务队列不为空，并且poolSize小于Math.max(corePoolSize, 1)。则调用addThread为线程池创建一个新线程。但是这个任务并没有直接给新线程执行。为什么要判断poolSize小于Math.max(corePoolSize, 1)，因为corePoolSize可以设置为0.当corePoolSize=0时，需要至少有1个线程去执行任务。前面的几个方法中出现了几次创建addThread的方法，现在来看看这个方法做了哪些事情： 1234567891011121314151617181920212223private Thread addThread(Runnable firstTask) &#123; Worker w = new Worker(firstTask); Thread t = threadFactory.newThread(w); boolean workerStarted = false; if (t != null) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); w.thread = t; workers.add(w); int nt = ++poolSize; if (nt &gt; largestPoolSize) largestPoolSize = nt; try &#123; t.start(); workerStarted = true; &#125; finally &#123; if (!workerStarted) workers.remove(w); &#125; &#125; return t; &#125; 这个方法的参数名firstTask可能比较难理解。这里详细说明一下：首先在线程池中，一个新线程的创建大多数情况都是为执行某个任务而创建的，这个任务不会加入任务队列，而是通过firstTask传给为他而建的新线程去执行。所以这个任务也就是这个线程执行的第一个任务。如果firstTask设为null，那么线程将去执行任务队列中的任务。下面来分析这个方法的功能，首先先创建一个worker对象，把firstTask初始化这个worker对象。然后通过线程工厂创建一个线程，并检查这个线程的状态，同时跟新线程池的峰值线程数的值。需要注意的是，这个线程不是属于某个具体任务的，而是属于这个worker的，即该线程不是执行某个任务的run，而是执行这个worker的run。最后把worker对象添加到worker队列里面。所以发到这里可以明白了ThreadPoolExecutor为什么没有thread的集合属性了。5.第4点阐述了任务加入队列成功的情况，但是如果队列满了加入队列也可能失败。这时候会去尝试创建新线程来执行该任务。即执行addIfUnderMaximumPoolSize方法。这个方法与addIfUnderCorePoolSize基本一致，只是后者是拿poolSize跟corePoolSize比较，而前者是拿poolSize跟maximumPoolSize比较。如果addIfUnderMaximumPoolSize方法为任务创建线程失败，则执行拒绝策略来处理这个任务。到目前为止，前面讲的5个步骤将了一个任务提交给线程池之后是如何处理的。但是细心的用户可能发现，里面缺失了非常重要的一个功能：任务被添加到任务队列之后是如何被线程池处理掉的？线程处理完它的首个任务之后是如何获取新任务的呢？线程池是不是有类似Timer一样的守护进程不断扫描线程队列和等待队列？还是利用某种锁机制，实现类似wait和notify实现的？ 别急。下面来揭开它的神秘面纱。前面提到了ThreadPoolExecutor的内部类Worker，也在介绍addThread方法的时候提到了线程池的线程是和Worker对象绑定在一起的。所以现在来看看Worker类做了什么事情？通过代码发现Worker的定义也是一个Runnable。addthread方法中调用了这个Worker的start()方法，也就是线程的启动方法，其实也就是调用了Worker的run()方法。现在来看看worker的run方法做了什么事情： 1234567891011121314public void run() &#123; try &#123; hasRun = true; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); &#125; &#125; &#125; woker的run方法主要是通过while循环不断调用getTask()方法去获取任务。然后执行runTask(task)方法来执行任务，最后调用workerDone()方法来执行一些清除操作。 runTask(task)其实做的事情很简单：它的核心就是调用任务的run方法来执行真正的用户任务，除此之外还执行了任务执行前后需要的一些操作，以及统计一下这个worker完成的任务数。这个方法不需要深究，代码也比较简单： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private void runTask(Runnable task) &#123; final ReentrantLock runLock = this.runLock; runLock.lock(); try &#123; if ((runState &gt;= STOP || (Thread.interrupted() &amp;&amp; runState &gt;= STOP)) &amp;&amp; hasRun) thread.interrupt(); boolean ran = false; beforeExecute(thread, task); try &#123; task.run(); ran = true; afterExecute(task, null); ++completedTasks; &#125; catch (RuntimeException ex) &#123; if (!ran) afterExecute(task, ex); throw ex; &#125; &#125; finally &#123; runLock.unlock(); &#125; &#125; ``` worker的润方法真正的核心是如果不断获取任务的。所以这里必须认真解读下getTask()方法，下面是getTask()的代码： ``` Runnable getTask() &#123; for (;;) &#123; try &#123; int state = runState; if (state &gt; SHUTDOWN) return null; Runnable r; if (state == SHUTDOWN) // Help drain queue r = workQueue.poll(); else if (poolSize &gt; corePoolSize || allowCoreThreadTimeOut) r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS); else r = workQueue.take(); if (r != null) return r; if (workerCanExit()) &#123; if (runState &gt;= SHUTDOWN) // Wake up others interruptIdleWorkers(); return null; &#125; // Else retry &#125; catch (InterruptedException ie) &#123; // On interruption, re-check runState &#125; &#125; &#125; 你会发现getTask()方法是从workQueue队列中，也就是等待队列中获取一个任务出来并返回！如果没有获得任务，则通过 interruptIdleWorkers()方法去关闭空闲时间超过阈值的空闲线程。 至此，完整的ThreadPoolExecutor线程池处理任务的原理就解读完毕了。其他的一些诸如关闭线程池和获取线程池的状态和统计信息等的接口都比较简单，这里就不一一解释了。 常见线程池1. newSingleThreadExecutor。只有一个线程的线程池，即corePoolSize和maximumPoolSize都等于1。2. newCachedThreadPool创建一个可根据需要创建新线程的线程池，但是在以前构造的线程可用时将重用它们。对于执行很多短期异步任务的程序而言，这些线程池通常可提高程序性能。调用 execute 将重用以前构造的线程（如果线程可用）。如果现有线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。因此，长时间保持空闲的线程池不会使用任何资源。在newCachedThreadPool构造参数中，corePoolSize=0，maximumPoolSize=Integer.MAX_VALUE，即可与无限制的创建线程。但是它使用的是阻塞队列是SynchronousQueue。这个队列比较奇葩，虽然他是无界的，但是里面只能有一个元素。在添加一个任务的时候，必须要有一个线程正在等待一个任务。即通过这个阻塞队列，既可以保证任务能够马上得到线程去运行，同时又能重用已有的空闲线程。3. newFixedThreadPool线程数固定的线程池，即corePoolSize=maximumPoolSize。当线程数达到了corePoolSize时，不能创建新的线程了，所以新的任务只能放到任务队列中，因此这个线程池用的阻塞队列是无界队列LinkedBlockingQueue。4. ScheduledThreadPoolExecutor可以执行延迟固定时间的任务，也可以执行定时任务的线程池。ScheduledThreadPoolExecutor的底层不是基于ThreadPoolExecutor实现的，它有一个自己的实现类。 其实我们的要求很简单，希望线程池能跟连接池一样，能设置最小线程数、最大线程数，当最小数&lt;任务&lt;最大数时，应该分配新的线程处理；当任务&gt;最大数时，应该等待有空闲线程再处理该任务。但线程池的设计思路是，任务应该放到Queue中，当Queue放不下时再考虑用新线程处理，如果Queue满且无法派生新线程，就拒绝该任务。设计导致“先放等执行”、“放不下再执行”、“拒绝不等待”。所以，根据不同的Queue参数，要提高吞吐量不能一味地增大maximumPoolSize。当然，要达到我们的目标，必须对线程池进行一定的封装，幸运的是ThreadPoolExecutor中留了足够的自定义接口以帮助我们达到目标。我们封装的方式是： 以SynchronousQueue作为参数，使maximumPoolSize发挥作用，以防止线程被无限制的分配，同时可以通过提高maximumPoolSize来提高系统吞吐量 自定义一个RejectedExecutionHandler，当线程数超过maximumPoolSize时进行处理，处理方式为隔一段时间检查线程池是否可以执行新Task，如果可以把拒绝的Task重新放入到线程池，检查的时间依赖keepAliveTime的大小。 线程池的两个核心队列 线程等待池，即线程队列BlockingQueue。 任务处理池（PoolWorker），即正在工作的Thread列表（HashSet）。线程池的核心参数： 核心池大小（corePoolSize），即固定大小，设定好之后，线程池的稳定峰值，达到这个值之后池的线程数大小不会释放。 最大处理线程池数（maximumPoolSize），当线程池里面的线程数超过corePoolSize，小于maximumPoolSize时会动态创建与回收线程池里面的线程池资源。 线程池的运行机制： 举个例子。假如有一个工厂，工厂里面有10个人，每个工人同时只能做一件事情。因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做；当10个工人都有任务时，如果还来任务，就把任务进行排队等待。如果说新任务数目增长的速度远远大于工作做任务的速度，那么此时工厂的主管可能就需要采取补救措施了，比如重新招4个工人进来；然后就将任务分配给这4个刚招进来的工人处理。如果说这14个工人做任务的速度还是不够，此时工厂主管就要考虑不再接受新的任务或者抛弃前面的一些任务了。当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管就要考虑辞掉4个临时工了，只保持原来10个工人，比较额外的工人是需要花费的。而这个例子中永远等待干活的10个工人机制就是workerQueue。这个栗子中的corePoolSize就是10，而maximumPoolSize就是14（10+4）。也就是说corePoolSize就是线程池的大小，maximumPoolSize在我看来就是一种线程池任务超过负荷的一种补救措施，即任务量突然过大时的一种补救措施。再看看下面图好好理解一下。工人永远在等待干活，就像workerQueue永远在循环干活一样，除非，整个线程池停止了。 线程池里面的线程的时序图如下图所示： 自定义线程池与ExecutorService自定义线程池需要用到ThreadFactory，本节将通过创建一个线程的例子对ExecutorService及其参数进行详细讲解。1.认识ExecutorService家族 ExecutorService家族成员如下所示： 上图中主要元素说明如下：Executor：线程池的顶级接口，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。ExecutorService：真正线程池接口。这个接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等。ThreadPoolExecutor：ExecutorService的默认实现，继承了类AbstractExecutorService。ScheduledExecutorService：与Timer/TimerTask类似，解决那些需要任务重复执行的问题。ScheduledThreadPoolExecutor：继承ThreadPoolExecutor的ScheduledExecutorService接口实现，周期性任务调度的类实现。Executors是个线程工厂类，方便我们快速地创建线程池。 2.利用ThreadFactory创建一个线程Java.util.concurrent.ThreadFactory提供了一个创建线程的工厂的接口。ThreadFactory源码如下： 1234public interface ThreadFactory&#123; @override public Thread newThread(Runnable r);&#125; 我们可以看到上面的接口类中有一个newThread()的方法，为此我们自己手动定义一个线程工厂类，有木有激动啊，呵呵，下面我们就手动写一个自己的线程工厂类吧！ 123456public class MyThreadFactory implements ThreadFactory&#123; @Override public Thread newThread(Runnable r)&#123; return new Thread(r); &#125;&#125; 上面已经创建好了我们自己的线程工厂类，但是啥都没有做，就是直接new了一个Thread就返回回去了，我们一般在创建线程的时候，都需要定义其线程的名字，因为我们在定义了线程的名字之后就能在出现问题的时候根据监视工具来查找错误的来源，所以我们来看下官方实现的ThreadFactory吧！这个类在java.util.concurrent.Executors类中的静态类中DefaultThreadFactory 12345678910111213141516171819202122/*** The default thread factory*/static class DefaultThreadFactory implements ThreadFactory&#123; private static final AtomicInteger poolNumber=new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber=new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory()&#123; SecurityManager s=System.getSecurityManager(); group=(s!=null)?s.getThreadGroup():Thread.currentThread().getThreadGroup(); namePrefix=&quot;pool-&quot;+poolNumber.getAndIncrement()+&quot;-thread-&quot;; &#125; public Thread newThread(Runnable r)&#123; Thread t=new Thread(group,r,namePrefix+threadNumber.getAndIncrement(),0); if((t.isDaemon()) t.setDaemon(false); if(t.getPriority()!=Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; &#125;&#125; 3.了解线程池的拒绝策略(RejectExecutionHandler)当调用ThreadPoolExecutor的execute方法时，而此时线程池处于一个饱和的状态，并且任务队列也已经满了那么就需要做丢弃处理，RejectExecutionHandler就是这样的一个处理接口类。 12345678910111213141516171819public interface RejectedExecutionHandler &#123; /** * Method that may be invoked by a &#123;@link ThreadPoolExecutor&#125; when * &#123;@link ThreadPoolExecutor#execute execute&#125; cannot accept a * task. This may occur when no more threads or queue slots are * available because their bounds would be exceeded, or upon * shutdown of the Executor. * * &lt;p&gt;In the absence of other alternatives, the method may throw * an unchecked &#123;@link RejectedExecutionException&#125;, which will be * propagated to the caller of &#123;@code execute&#125;. * * @param r the runnable task requested to be executed * @param executor the executor attempting to execute this task * @throws RejectedExecutionException if there is no remedy */ void rejectedExecution(Runnable r, ThreadPoolExecutor executor);&#125; 在JDK里面有4种拒绝策略，如下图所示： AbortPolicy：一言不合就抛异常（默认使用策略）。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，直接丢弃。 来看下源码吧： 123456789101112131415161718192021222324AbortPolicy : 一言不合就抛异常的 /** * A handler for rejected tasks that throws a * &#123;@code RejectedExecutionException&#125;. */ public static class AbortPolicy implements RejectedExecutionHandler &#123; /** * Creates an &#123;@code AbortPolicy&#125;. */ public AbortPolicy() &#123; &#125; /** * Always throws RejectedExecutionException. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task * @throws RejectedExecutionException always. */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; &#125; CallerRunsPolicy：调用者所在线程来运行任务 12345678910111213141516171819202122232425/** * A handler for rejected tasks that runs the rejected task * directly in the calling thread of the &#123;@code execute&#125; method, * unless the executor has been shut down, in which case the task * is discarded. */public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code CallerRunsPolicy&#125;. */ public CallerRunsPolicy() &#123; &#125; /** * Executes task r in the caller&apos;s thread, unless the executor * has been shut down, in which case the task is discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125;&#125; DiscardOldestPolicy :丢弃队列里面最近的一个任务,并执行当前任务 123456789101112131415161718192021222324252627/** * A handler for rejected tasks that discards the oldest unhandled * request and then retries &#123;@code execute&#125;, unless the executor * is shut down, in which case the task is discarded. */public static class DiscardOldestPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code DiscardOldestPolicy&#125; for the given executor. */ public DiscardOldestPolicy() &#123; &#125; /** * Obtains and ignores the next task that the executor * would otherwise execute, if one is immediately available, * and then retries execution of task r, unless the executor * is shut down, in which case task r is instead discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125; &#125;&#125; DiscardPolicy : 不处理，直接丢弃 12345678910111213141516171819/** * A handler for rejected tasks that silently discards the * rejected task. */ public static class DiscardPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code DiscardPolicy&#125;. */ public DiscardPolicy() &#123; &#125; /** * Does nothing, which has the effect of discarding task r. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125; &#125; 思考问题：为什么有任务拒绝的情况发生呢：这里先假设有一个前提：线程池里面有一个任务队列，用于缓存所有待处理的任务，正在处理的任务将从任务队列中移除。因此，在任务队列长度有限的情况下，就会出现现任务的拒绝情况，需要一种策略来处理发生这种已满无法加入的情况。另外，在线程池关闭的时候，也需要对任务加入队列操作进行额外的协调处理。 4.ThreadPoolExecutor详解 ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要想透彻的了解Java线程池，必须先了解这个大BOSS，下面来看下其源码： 4种构造方法： 12345678910111213141516171819202122232425262728public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,Executors.defaultThreadFactory(), defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,threadFactory, defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125;public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory,RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 通过源码我们清楚的看到，最终构造函数调用了最后一个构造函数，后面的那个构造函数才是真正的构造函数，接下来研究一下参数。 int corePoolSize：核心池大小，这个参数跟后面讲的线程池原理有很大的关系。在创建了线程池之后，默认情况下，线程池中并没有任何线程，而是等待所有的任务到来之时才进行创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法 ，从这个两个方法的名字可以知道是预创建线程的意思，即在没有任务来临之前先创建好corePoolSize个线程或者一个线程。默认情况下，在创建好线程池之后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数量达到corePoolSize后，就会把达到的任务放到缓存队列中去。 int maximumPoolSize：线程池最大线程数量，这是个非常重要的参数，它表示在线程池中最多能创建线程的数量；在corePoolSize和maximumPoolSize的线程数会被自动释放，而小于corePoolSize的则不会。 long keepAliveTime：表示线程没有执行任务时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会生效,直到线程池数量不大于corePoolSize，即只有当线程池数量大于corePoolSize数量，超出这个数量的线程一旦到达keepAliveTime就会终止。但是如果调用了allowCoreThreadTimeout(boolean)方法，即使线程池的线程数量不大于corePoolSize，线程也会在keepAliveTime之后就终止，知道线程池的数量为0为止。 TimeUnit unit：参数keepAliveTime的时间单位，一个时间单位枚举类。 BlockingQueue workQueue：一个阻塞队列，用来存储等待执行任务的队列，这个参数选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列就是（ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue；）。 ThreadFactory ThreadFactory：线程工厂，主要用来创建线程；可以是一个自定义的线程工厂，默认就是Executors.defaultThreadFactory()。用来在线程池中创建线程。 RejectedExecutionHandler handler：表示当拒绝处理任务时的策略，也是可以自定义的，默认是我们前面的4种取值： ThreadPoolExecutor.AbortPolicy（默认的，一言不合即抛异常的） ThreadPoolExecutor.DiscardPolicy（一言不合就丢弃任务） ThreadPoolExecutor.DiscardOldestPolicy（一言不合就把最近的任务给抛弃，然后执行当前任务） ThreadPoolExecutor.CallerRunsPolicy（由调用者所在线程来执行任务）所以想自定义线程池就可以从上面的几个参数入手。接下来具体看下代码,了解一下实现原理： // 默认异常处理机制 private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); //任务缓存队列，用来存放等待执行的任务 private final BlockingQueue workQueue; //线程池的主要状态锁，对线程状态（比如线程大小、runState等）的改变都需要这个锁 private final ReentrantLock mainLock = new ReentrantLock(); //用来存放工作集 private final HashSet workers = new HashSet(); //volatile 可变变量关键字，写的时候用mainLock做锁，读的时候无锁，高性能 private volatile long keepAliveTime; //是否允许核心线程超时 private volatile boolean allowCoreThreadTimeOut; //核心线程数量 private volatile int corePoolSize; //线程最大线程数量 private volatile int maximumPoolSize; //任务拒绝策略 private volatile RejectedExcutionHandler handler;结合之前的知识，大概就能猜出里面是怎么实现的了，具体可以参考一下JDK的源代码，这样我们就能做到了解原理又会用了。 5.自定义实现一个简单的Web请求连接池我们来自定义一个简单的Web请求线程池。模仿Web服务的需求场景说明如下： 服务器可容纳的最小请求数是多少。 可以动态扩充的请求数大小是多少。 多久回收多余线程数即请求数。 用户访问量打了怎么处理。 线程队列机制采取有优先级的排队的执行机制。根据上面的场景，看下这个线程池如何编写？ 1234567public class MyExecutors extends Executors&#123; //利用默认线程工厂和PriorityBlockingQueue队列机制，当然了，我们还可以自定义ThreadFactory和继承queue进行自定义扩展 public static ExecutorService newMyWebThreadPool(int minSpareThreads,int maxThreads,int maxIdleTime)&#123; return new ThreadPoolExecutor(minSpareThread,maxThreads,maxIdleTime,TimeUnit.MILLISECONDS， new PriorityBlockingQueue&lt;Runnable&gt;()); &#125;&#125; 6.线程池在工作中的错误使用(1)分不清楚线程是单例还是多对象。(2)线程池数量设置很大。(3)注意死锁问题 连接池（org.apache.commons.dbcp.BasicDataSource）在使用org.apache.commons.dbcp.BasicDataSource的时候，因为之前采用了默认配置，所以当访问量大时，通过JMX观察到很多Tomcat线程都阻塞在BasicDataSource使用的Apache ObjectPool的锁上，直接原因当时是因为BasicDataSource连接池的最大连接数设置的太小，默认的BasicDataSource配置，仅使用8个最大连接。我还观察到一个问题，当较长的时间不访问系统，比如2天，DB上的Mysql会断掉所以的连接，导致连接池中缓存的连接不能用。为了解决这些问题，我们充分研究了BasicDataSource，发现了一些优化的点： Mysql默认支持100个链接，所以每个连接池的配置要根据集群中的机器数进行，如有2台服务器，可每个设置为60 initialSize：参数是一直打开的连接数 minEvictableIdleTimeMillis：该参数设置每个连接的空闲时间，超过这个时间连接将被关闭 timeBetweenEvictionRunsMillis：后台线程的运行周期，用来检测过期连接 maxActive：最大能分配的连接数 maxIdle：最大空闲数，当连接使用完毕后发现连接数大于maxIdle，连接将被直接关闭。只有initialSize &lt; x &lt; maxIdle的连接将被定期检测是否超期。这个参数主要用来在峰值访问时提高吞吐量。 initialSize是如何保持的？经过研究代码发现，BasicDataSource会关闭所有超期的连接，然后再打开initialSize数量的连接，这个特性与minEvictableIdleTimeMillis、timeBetweenEvictionRunsMillis一起保证了所有超期的initialSize连接都会被重新连接，从而避免了Mysql长时间无动作会断掉连接的问题。]]></content>
      <categories>
        <category>java</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>ThreadPoolExecutor</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程的实现方式]]></title>
    <url>%2F2017%2F08%2F05%2F%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[线程实现方式 继承java.lang.Thread类 实现java.lang.Runnable接口 区别：第一种是继承，第二种是实现好处： 在实际开发中通常以实现Runnable接口为主，因为实现Runnable接口相比继承Thread类可以避免继承的局限，一个类可以继承多个接口，适合于资源的共享示例1：1234567891011121314151617181920212223241.class ThreadTest extends Thread&#123; 2. public void run()&#123; 3. private int tickets = 100; 4. while(true)&#123; 5. if(ticket &gt; 0)&#123; 6. System.out.println(Thread.currentThread().getName() + 7. &quot;is saling ticket&quot; + ticket--); 8. &#125;else&#123; 9. break; 10. &#125; 11. &#125; 12. &#125; 13.&#125; #main测试类: 1.public class ThreadDome1&#123; 2. public static void main(String[] args)&#123; 3. ThreadTest t = new ThreadTest(); 4. t.start(); 5. t.start(); 6. t.start(); 7. t.start(); 8. &#125; 9.&#125; 【说明】一个线程对象只能启动一个线程，无论你调用多少遍start()方法，结果只有一个线程。 示例2：123456789101112131415161718192021221.public class ThreadDemo1&#123; 2. public static void main(String[] args)&#123; 3. new ThreadTest().start(); 4. new ThreadTest().start(); 5. new ThreadTest().start(); 6. new ThreadTest().start(); 7. &#125; 8.&#125; 1.class ThreadTest extends Thread&#123; 2. public void run()&#123; 3. private int tickets = 100; 4. while(true)&#123; 5. if(ticket &gt; 0)&#123; 6. System.out.println(Thread.currentThread().getName() + 7. &quot; is saling ticket&quot; + ticket--); 8. &#125;else&#123; 9. break; 10. &#125; 11. &#125; 12. &#125; 13.&#125; 【说明】创建了四个ThreadTest对象，就等于创建了四个资源，每个资源都有100张票，每个线程都在独自处理各自的资源。 示例4： 1234567891011121314151617181920211.public class ThreadDemo1&#123; 2. public static void main(String[] args)&#123; 3. ThreadTest t = new ThreadTest(); 4. new Thread(t).start(); 5. new Thread(t).start(); 6. new Thread(t).start(); 7. new Thread(t).start(); 8. &#125; 9.&#125; 1.class ThreadTest implements Runnable&#123; 2. public void run()&#123; 3. private int tickets = 100; //局部变量，如果是成员变量，则所有线程则共享它，会出现问题 4. while(true)&#123; 5. if(tickets &gt; 0)&#123; 6. System.out.println(Thread.currentThread().getName() + 7. &quot; is saling ticket &quot; + tickets--); 8. &#125; 9. &#125; 10. &#125; 11.&#125; 【注意】创建了四个线程，每个线程调用的是同一个ThreadTest对象中的run()方法，访问的是同一个对象中的变量（tickets）的实例 如果一个变量是成员变量，那么多个线程对同一个对象的成员变量进行操作时，它们对该成员变量是彼此影响的，也就是说一个线程对成员变量的改变会影响到另一个线程。 如果一个变量是局部变量，那么每个线程都会有一个该局部变量的拷贝（即便是同一个对象中的方法的局部变量，也会对每一个线程有一个拷贝），一个线程对该局部变量的改变不会影响到其他线程。]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>thread</tag>
        <tag>runnable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm调优]]></title>
    <url>%2F2017%2F08%2F05%2Fjvm%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[JVM内存模型1.根据Java虚拟机规范，JVM将内存划分为：New（年轻代）Tenured（年老代）永久代（Perm） 其中New和Tenured属于堆内存，堆内存会从JVM启动参数（-Xmx:3G）指定的内存中分配，Perm不属于堆内存，有虚拟机直接分配，但可以通过-XX:PermSize -XX:MaxPermSize 等参数调整其大小。 年轻代（New）：年轻代用来存放JVM刚分配的Java对象年老代（Tenured)：年轻代中经过垃圾回收没有回收掉的对象将被Copy到年老代永久代（Perm）：永久代存放Class、Method元信息，其大小跟项目的规模、类、方法的量有关，一般设置为128M就足够，设置原则是预留30%的空间。 New又分为几个部分：Eden：Eden用来存放JVM刚分配的对象Survivor1Survivro2：两个Survivor空间一样大，当Eden中的对象经过垃圾回收没有被回收掉时，会在两个Survivor之间来回Copy，当满足某个条件，比如Copy次数，就会被Copy到Tenured。显然，Survivor只是增加了对象在年轻代中的逗留时间，增加了被垃圾回收的可能性。 各代如何设置比例堆大小设置JVM 中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。我在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。典型设置：- java -Xmx3550m -Xms3550m -Xmn2g -Xss128k- -Xmx3550m：设置JVM最大可用内存为3550M。- -Xms3550m：设置JVM促使内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。- -Xmn2g：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。- -Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。- java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0- -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5- -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6- -XX:MaxPermSize=16m:设置持久代大小为16m。- -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 如果程序确实需要大量的线程，现有的设置不能达到要求，那么可以通过修改MaxProcessMemory，JVMMemory，ThreadStackSize这三个因素，来增加能创建的线程数：a. MaxProcessMemory 使用64位JVM 经测试在，64位jvm下生成的线程数不受上述公式的制约，值为63260，这个数值应该足够用了， 测试数据如下, (指令|能启动的线程数量) java -Xms512M -Xmx2G -cp . TestNativeOutOfMemoryError | 63389java -Xms512M -Xmx8G -cp . TestNativeOutOfMemoryError | 63260java -Xms512M -Xmx16G -cp . TestNativeOutOfMemoryError | 63385java -Xms512M -Xmx32G -cp . TestNativeOutOfMemoryError | 63380java -Xms512M -Xmx64G -cp . TestNativeOutOfMemoryError |63387 b. JVMMemory 减少JVMMemory的分配(即减少xms/xmx的大小)c. ThreadStackSize 减小单个线程的栈大小 (-Xss) 垃圾回收算法垃圾回收算法可以分为三类，都基于标记-清除（复制）算法：Serial算法（单线程）并行算法并发算法JVM会根据机器的硬件配置对每个内存代选择适合的回收算法，比如，如果机器多于1个核，会对年轻代选择并行算法，关于选择细节请参考JVM调优文档。 稍微解释下的是，并行算法是用多线程进行垃圾回收，回收期间会暂停程序的执行，而并发算法，也是多线程回收，但期间不停止应用执行。所以，并发算法适用于交互性高的一些程序。经过观察，并发算法会减少年轻代的大小，其实就是使用了一个大的年老代，这反过来跟并行算法相比吞吐量相对较低。 还有一个问题是，垃圾回收动作何时执行？ 当年轻代内存满时，会引发一次普通GC，该GC仅回收年轻代。需要强调的时，年轻代满是指Eden代满，Survivor满不会引发GC 当年老代满时会引发Full GC，Full GC将会同时回收年轻代、年老代 当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载另一个问题是，何时会抛出OutOfMemoryException，并不是内存被耗空的时候才抛出 JVM98%的时间都花费在内存回收 每次回收的内存小于2%满足这两个条件将触发OutOfMemoryException，这将会留给系统一个微小的间隙以做一些Down之前的操作，比如手动打印Heap Dump。 内存泄漏及解决方法1.系统崩溃前的一些现象： 每次垃圾回收的时间越来越长，由之前的10ms延长到50ms左右，FullGC的时间也有之前的0.5s延长到4、5s FullGC的次数越来越多，最频繁时隔不到1分钟就进行一次FullGC 年老代的内存越来越大并且每次FullGC后年老代没有内存被释放之后系统会无法响应新的请求，逐渐到达OutOfMemoryError的临界值。 2.生成堆的dump文件 通过JMX的MBean生成当前的Heap信息，大小为一个3G（整个堆的大小）的hprof文件，如果没有启动JMX可以通过Java的jmap命令来生成该文件。 3.分析dump文件 下面要考虑的是如何打开这个3G的堆信息文件，显然一般的Window系统没有这么大的内存，必须借助高配置的Linux。当然我们可以借助X-Window把Linux上的图形导入到Window。我们考虑用下面几种工具打开该文件： Visual VM IBM HeapAnalyzer JDK 自带的Hprof工具 使用这些工具时为了确保加载速度，建议设置最大内存为6G。使用后发现，这些工具都无法直观地观察到内存泄漏，Visual VM虽能观察到对象大小，但看不到调用堆栈；HeapAnalyzer虽然能看到调用堆栈，却无法正确打开一个3G的文件。因此，我们又选用了Eclipse专门的静态内存分析工具：Mat。 4.分析内存泄漏 通过Mat我们能清楚地看到，哪些对象被怀疑为内存泄漏，哪些对象占的空间最大及对象的调用关系。针对本案，在ThreadLocal中有很多的JbpmContext实例，经过调查是JBPM的Context没有关闭所致。 另，通过Mat或JMX我们还可以分析线程状态，可以观察到线程被阻塞在哪个对象上，从而判断系统的瓶颈。 5.回归问题 Q：为什么崩溃前垃圾回收的时间越来越长？ A:根据内存模型和垃圾回收算法，垃圾回收分两部分：内存标记、清除（复制），标记部分只要内存大小固定时间是不变的，变的是复制部分，因为每次垃圾回收都有一些回收不掉的内存，所以增加了复制量，导致时间延长。所以，垃圾回收的时间也可以作为判断内存泄漏的依据 Q：为什么Full GC的次数越来越多？ A：因此内存的积累，逐渐耗尽了年老代的内存，导致新对象分配没有更多的空间，从而导致频繁的垃圾回收 Q:为什么年老代占用的内存越来越大？ A:因为年轻代的内存无法被回收，越来越多地被Copy到年老代 性能调优 除了上述内存泄漏外，我们还发现CPU长期不足3%，系统吞吐量不够，针对8core×16G、64bit的Linux服务器来说，是严重的资源浪费。 在CPU负载不足的同时，偶尔会有用户反映请求的时间过长，我们意识到必须对程序及JVM进行调优。从以下几个方面进行： 线程池：解决用户响应时间长的问题 连接池 JVM启动参数：调整各代的内存比例和垃圾回收算法，提高吞吐量 程序算法：改进程序逻辑算法提高性能]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web.xml加载顺序]]></title>
    <url>%2F2017%2F08%2F05%2Fweb-xml%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[web.xml 的加载顺序:ServletContext-&gt; context-param -&gt;listener -&gt; filter -&gt; servlet，而同个类型之间的实际程序调用的时候的顺序是根据对应的 mapping 的顺序进行调用的。ServletContext即Servlet上下文对象，该对象表示当前的web应用环境信息，一个Web应用只会创建一个ServletContext对象。 Web容器启动的时候，它会为每个Web应用程序都创建一个对应的ServletContext对象，它代表当前的web应用。[注意]由于一个Web应用中的所有Servlet共享一个ServletContext对象，所以多个Servlet通过ServletContext对象实现数据共享，ServletContext对象通常称为Context域对象。]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>web.xml加载顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sping之IOC，AOP]]></title>
    <url>%2F2017%2F08%2F05%2Fsping%E4%B9%8BIOC%EF%BC%8CAOP%2F</url>
    <content type="text"><![CDATA[前言Spring AOP和IOC个人理解IOC inversion of control 控制反转将new对象的权力由调用者转移到spring容器（即xml文件），Struts2与Spring整合（scope=”prototype”）由spring来维护struts的生命周期，在启动web容器时spring容器创建action实例对象，又分两种方式：第一种xml方式 需要set方法为被调用的属性赋值，xml中需要ref注入被调要的对象。第二种注解方式 不需要set方法为被调用属性赋值，但需要在action层service层dao层的类上对应写上@Controller,@Service,@Repository通过在属性上加上@Resource(name=&quot;&quot;)来为属性赋值，这一步相当于xml方式的ref。事务管理器 123&lt;bean id=&quot;txManager&quot;class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 相当于切面需要注入sessionFactory AOP Aspect Oriented Programming 面向切面编程通过代理的方式在需要的时候通过切入点给指定位置的程序添加逻辑代码或功能。声明事务处理分为两种方式：第一种xml方式: 需要在xml中配置事务的通知&lt;tx:advice&gt;里面放增删改查等方法的isolation=&quot;DEFAULT&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;&lt;/tx:advice&gt;）用切面关联通知，然后再用通知关联切入点&lt;aop:config&gt;（即事务操作业务层）切入地点是所有service 包及其子包下类的所有方法。第二种注解方式: 使用注解的方式配置声明式事务处理，在Service层类中，@Transcational(事务处理的) &lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt;不需要关联通知也不需要通知关联切入点 spring的基本框架主要包含六大模块：DAO、ORM、AOP、JEE、WEB、CORE Spring DAO：Spring提供了对JDBC的操作支持：JdbcTemplate模板工具类 。Spring ORM：Spring可以与ORM框架整合。例如Spring整合Hibernate框架，其中Spring还提供 HibernateDaoSupport工具类，简化了Hibernate的操作 。Spring WEB：Spring提供了对Struts、Springmvc的支持，支持WEB开发。与此同时Spring自身也提供了基于MVC的解决方案 。Spring AOP：Spring提供面向切面的编程，可以给某一层提供事务管理，例如在Service层添加事物控制 。Spring JEE：J2EE开发规范的支持，例如EJB 。8Spring Core：提供IOC容器对象的创建和处理依赖对象关系 。 Spring下IOC容器和DI(依赖注入Dependency injection) IOC容器：就是具有依赖注入功能的容器，是可以创建对象的容器，IOC容器负责实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。通常new一个实例，控制权由程序员控制，而”控制反转”是指new实例工作不由程序员来做而是交给Spring容器来做。。在Spring中BeanFactory是IOC容器的实际代表者。 DI(依赖注入Dependency injection) ：在容器创建对象后，处理对象的依赖关系。依赖注入spring的注入方式： set注入方式静态工厂注入方式构造方法注入方式基于注解的方式 1、 set注入方式：控制层代码：1234private OrderServiceImp orderService; public void setOrderService(OrderServiceImp orderService) &#123; this.orderService = orderService;&#125; Spring配置XML文件：其中配置声明OrderAction类存在属性orderService。程式运行时候，会将已经实例化的orderService对象调用setOrderService方式注入。1234&lt;bean name=&quot;orderAction&quot; class=&quot;com.pec.action.OrderAction&quot;&gt; &lt;property name=&quot;orderService&quot; ref=&quot;orderService&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean name=&quot;orderService&quot; class=&quot;com.pec.service.imp.OrderServiceImp&quot;&gt;&lt;/bean&gt; 2、 构造器注入方式：控制层代码：1234private OrderServiceImp orderService; public OrderAction(OrderServiceImp orderService) &#123; this.orderService = orderService; &#125; Spring配置XML文件：1234&lt;bean name=&quot;orderAction&quot; class=&quot;com.pec.action.OrderAction&quot;&gt; &lt;constructor-arg ref=&quot;orderService&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;bean name=&quot;orderService&quot; class=&quot;com.pec.service.imp.OrderServiceImp&quot;&gt;&lt;/bean&gt; 3、基于注解的方式 （推荐使用，比较便捷少配置）控制层代码：1@Autowired //@Resourceprivate OrderServiceImp orderService; 服务层代码： @Service(&quot;orderService&quot;) public class OrderServiceImp implements IOrderService { @Autowired private JavaOrderMDaoImp javaOrderMDao; @Autowired private JavaOrderDDaoImp javaOrderDDao; @Override public List&lt;JavaOrderMList&gt; findOrderM(OrderSearch search) { return javaOrderMDao.findJavaOrderM(search); } @Override public List&lt;JavaOrderDList&gt; findOrderD(OrderSearch search) { return javaOrderDDao.findJavaOrderD(search); } } DAO层代码：1234@Repository(&quot;javaOrderMDao&quot;)public class JavaOrderMDaoImp extends BaseHibernateDAO&lt;JavaOrderM, Serializable&gt; implements IJavaOrderMDao &#123;...&#125;@Repository(&quot;javaOrderDDao&quot;)public class JavaOrderDDaoImp extendsBaseHibernateDAO&lt;JavaOrderD, Serializable&gt; implements IJavaOrderDDao &#123;...&#125; 【注意点】⑴ 持久层DAO层注解Repository中规定了名称，在Service层中声明名称必须一致。⑵ 服务层Service层注解Service中规定了名称，在控制层中声明的名称必须一致。⑶ 注解方式注入依赖注解： @Component 把对象加入ioc容器，对象引用名称是类名，第一个字母小写@Component(“name”) 把指定名称的对象，加入ioc容器@Repository 主要用于标识加入容器的对象是一个持久层的组件(类)@Service 主要用于标识加入容器的对象是一个业务逻辑层的组件@Controller 主要用于标识加入容器的对象是一个控制层的组件@Resource 注入属性(DI), 会从容器中找对象注入到@Resource修饰的对象上@Autowired 注入属性(DI), 会从容器中找对象注入到@Autowired修饰的对象上 开启注解&lt;mvc:annotation-driven/&gt;静态资源由WEB服务器默认的Servlet来处理 ，必须和一起&lt;mvc:default-servlet-handler/&gt;包扫描路径&lt;context:component-scan base-package=&quot;com.fh.controller&quot; /&gt; 注解可以简化配置，提升开发效率，但是也不利于后期维护。 @Autowired与@Resource都可以用来装配bean. 都可以写在字段上,或写在setter方法上。 @Autowired默认按类型装配（这个注解是属业spring的），默认情况下必须要求依赖对象必须存在，如果要允许null 值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下： 12@Autowired() @Qualifier(&quot;baseDao&quot;) private BaseDao baseDao; @Resource（这个注解属于J2EE的），默认安照名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。 当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。 12@Resource(name=&quot;baseDao&quot;) private BaseDao baseDao; 我喜欢用 @Resource注解在字段上，且这个注解是属于J2EE的，减少了与spring的耦合。最重要的这样代码看起就比较优雅。 Spring面向切面编程(AOP)和事务管理配置AOP就是纵向的编程，如业务1和业务2都需要一个共同的操作，与其往每个业务中都添加同样的代码，不如写一遍代码，让两个业务共同使用这段代码。在日常有订单管理、商品管理、资金管理、库存管理等业务，都会需要到类似日志记录、事务控制、权限控制、性能统计、异常处理及事务处理等。AOP把所有共有代码全部抽取出来，放置到某个地方集中管理，然后在具体运行时，再由容器动态织入这些共有代码。 AOP涉及名称：切面（Aspect）：其实就是共有功能的实现。如日志切面、权限切面、事务切面等。在实际应用中通常是一个存放共有功能实现的普通Java类，之所以能被AOP容器识别成切面，是在配置中指定的。通知（Advice）：是切面的具体实现。以目标方法为参照点，根据放置的地方不同，可分为前置通知（Before）、后置通知（AfterReturning）、异常通知（AfterThrowing）、最终通知（After）与环绕通知（Around）5种。在实际应用中通常是切面类中的一个方法，具体属于哪类通知，同样是在配置中指定的。连接点（Joinpoint）：就是程序在运行过程中能够插入切面的地点。例如，方法调用、异常抛出或字段修改等，但Spring只支持方法级的连接点。切入点（Pointcut）：用于定义通知应该切入到哪些连接点上。不同的通知通常需要切入到不同的连接点上，这种精准的匹配是由切入点的正则表达式来定义的。目标对象（Target）：就是那些即将切入切面的对象，也就是那些被通知的对象。这些对象中已经只剩下干干净净的核心业务逻辑代码了，所有的共有功能代码等待AOP容器的切入。代理对象（Proxy）：将通知应用到目标对象之后被动态创建的对象。可以简单地理解为，代理对象的功能等于目标对象的核心业务逻辑功能加上共有功能。代理对象对于使用者而言是透明的，是程序运行过程中的产物。织入（Weaving）：将切面应用到目标对象从而创建一个新的代理对象的过程。这个过程可以发生在编译期、类装载期及运行期，当然不同的发生点有着不同的前提条件。譬如发生在编译期的话，就要求有一个支持这种AOP实现的特殊编译器；发生在类装载期，就要求有一个支持AOP实现的特殊类装载器；只有发生在运行期，则可直接通过Java语言的反射机制与动态代理机制来动态实现。 Spring配置文件中关于事务配置总是由三个组成部分，分别是DataSource、TransactionManager和代理机制这三部分，无论哪种配置方式，一般变化的只是代理机制这部分。 DataSource、TransactionManager这两部分只是会根据数据访问方式有所变化，比如使用hibernate进行数据访问时，DataSource实际为SessionFactory，TransactionManager的实现为HibernateTransactionManager。根据代理机制的不同，总结了五种Spring事务的配置方式，配置文件如下：第一种方式：每个Bean都有一个代理 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDaoTarget&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; class=&quot;org.springframework.transaction.interceptor.TransactionProxyFactoryBean&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;property name=&quot;target&quot; ref=&quot;userDaoTarget&quot; /&gt; &lt;property name=&quot;proxyInterfaces&quot; value=&quot;com.bluesky.spring.dao.GeneratorDao&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 第二种方式：所有Bean共享一个代理基类 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;transactionBase&quot; class=&quot;org.springframework.transaction.interceptor.TransactionProxyFactoryBean&quot; lazy-init=&quot;true&quot; abstract=&quot;true&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDaoTarget&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; parent=&quot;transactionBase&quot; &gt; &lt;property name=&quot;target&quot; ref=&quot;userDaoTarget&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 第三种方式：使用拦截器 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;transactionInterceptor&quot; class=&quot;org.springframework.transaction.interceptor.TransactionInterceptor&quot;&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator&quot;&gt; &lt;property name=&quot;beanNames&quot;&gt; &lt;list&gt; &lt;value&gt;*Dao&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;interceptorNames&quot;&gt; &lt;list&gt; &lt;value&gt;transactionInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDao&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 第四种方式：使用tx标签配置的拦截器 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd&quot;&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package=&quot;com.bluesky&quot; /&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;interceptorPointCuts&quot; expression=&quot;execution(* com.bluesky.spring.dao.*.*(..))&quot; /&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;interceptorPointCuts&quot; /&gt; &lt;/aop:config&gt; &lt;/beans&gt; 第五种方式：全注解 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd&quot;&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package=&quot;com.bluesky&quot; /&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 此时在DAO上需加上@Transactional注解，如下： 123456789101112131415161718192021package com.bluesky.spring.dao;import java.util.List;import org.hibernate.SessionFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.orm.hibernate3.support.HibernateDaoSupport;import org.springframework.stereotype.Component;import com.bluesky.spring.domain.User;@Transactional@Component(&quot;userDao&quot;)public class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; public List&lt;User&gt; listUsers() &#123; return this.getSession().createQuery(&quot;from User&quot;).list(); &#125; &#125;]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>IOC</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务的传播属性和隔离级别]]></title>
    <url>%2F2017%2F08%2F05%2Fspring%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E5%B1%9E%E6%80%A7%E5%92%8C%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[spring事务传播属性Propagation（事务的传播属性） ：key属性确定代理应该给哪个方法增加事务行为。这样的属性最重要的部份是传播行为。有以下选项可供使用：PROPAGATION_REQUIRED—支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。PROPAGATION_SUPPORTS—支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY—支持当前事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW—新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED—以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。PROPAGATION_NEVER—以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_REQUIRED加入当前正要执行的事务不在另外一个事务里，那么就起一个新的事务比如说，ServiceB.methodB的事务级别定义为PROPAGATION_REQUIRED, 那么由于执行ServiceA.methodA的时候，ServiceA.methodA已经起了事务，这时调用ServiceB.methodB，ServiceB.methodB看到自己已经运行在ServiceA.methodA的事务内部，就不再起新的事务。而假如ServiceA.methodA运行的时候发现自己没有在事务中，他就会为自己分配一个事务。这样，在ServiceA.methodA或者在ServiceB.methodB内的任何地方出现异常，事务都会被回滚。即使ServiceB.methodB的事务已经被提交，但是ServiceA.methodA在接下来fail要回滚，ServiceB.methodB也要回滚 PROPAGATION_SUPPORTS如果当前在事务中，即以事务的形式运行，如果当前不再一个事务中，那么就以非事务的形式运行 PROPAGATION_MANDATORY必须在一个事务中运行。也就是说，他只能被一个父事务调用。否则，他就要抛出异常 PROPAGATION_REQUIRES_NEW这个就比较绕口了。 比如我们设计ServiceA.methodA的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别为PROPAGATION_REQUIRES_NEW，那么当执行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会起一个新的事务，等待ServiceB.methodB的事务完成以后，他才继续执行。他与PROPAGATION_REQUIRED 的事务区别在于事务的回滚程度了。因为ServiceB.methodB是新起一个事务，那么就是存在两个不同的事务。如果ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚，ServiceB.methodB是不会回滚的。如果ServiceB.methodB失败回滚，如果他抛出的异常被ServiceA.methodA捕获，ServiceA.methodA事务仍然可能提交。 PROPAGATION_NOT_SUPPORTED当前不支持事务。比如ServiceA.methodA的事务级别是PROPAGATION_REQUIRED ，而ServiceB.methodB的事务级别是PROPAGATION_NOT_SUPPORTED ，那么当执行到ServiceB.methodB时，ServiceA.methodA的事务挂起，而他以非事务的状态运行完，再继续ServiceA.methodA的事务。 PROPAGATION_NEVER不能在事务中运行。假设ServiceA.methodA的事务级别是PROPAGATION_REQUIRED， 而ServiceB.methodB的事务级别是PROPAGATION_NEVER ，那么ServiceB.methodB就要抛出异常了。 PROPAGATION_NESTED理解Nested的关键是savepoint。他与PROPAGATION_REQUIRES_NEW的区别是，PROPAGATION_REQUIRES_NEW另起一个事务，将会与他的父事务相互独立，而Nested的事务和他的父事务是相依的，他的提交是要等和他的父事务一块提交的。也就是说，如果父事务最后回滚，他也要回滚的。而Nested事务的好处是他有一个savepoint。 12345678910111213ServiceA &#123;/*** 事务属性配置为 PROPAGATION_REQUIRED*/void methodA() &#123;try &#123;//savepointServiceB.methodB(); //PROPAGATION_NESTED 级别&#125; catch (SomeException) &#123;// 执行其他业务, 如 ServiceC.methodC();&#125;&#125;&#125; 也就是说ServiceB.methodB失败回滚，那么ServiceA.methodA也会回滚到savepoint点上，ServiceA.methodA可以选择另外一个分支，比如ServiceC.methodC，继续执行，来尝试完成自己的事务。但是这个事务并没有在EJB标准中定义。 Spring事务的隔离级别 ISOLATION_DEFAULT： 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别. 另外四个与JDBC的隔离级别相对应 ISOLATION_READ_UNCOMMITTED： 这是事务最低的隔离级别，它充许令外一个事务可以看到这个事务未提交的数据。 这种隔离级别会产生脏读，不可重复读和幻像读。 ISOLATION_READ_COMMITTED： 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据 ISOLATION_REPEATABLE_READ： 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。 ISOLATION_SERIALIZABLE: 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。 事务的四个特性 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 以上介绍完事务的四大特性(简称ACID)，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题： 脏读: 指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据， 那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的。 不可重复读: 指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。 幻觉读: 指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 除了防止脏读，不可重复读外，还避免了幻像读，需要设置事务隔离级别]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>事务传播属性</tag>
        <tag>隔离级别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐观锁与悲观锁及应用举例]]></title>
    <url>%2F2017%2F08%2F05%2F%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E6%82%B2%E8%A7%82%E9%94%81%E5%8F%8A%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[悲观锁 正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）的修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 以常用的mysql InnoDB存储引擎为例：加入商品表items表中有一个字段status，status=1表示该商品未被下单，status=2表示该商品已经被下单，那么我们对每个商品下单前必须确保此商品的status=1。假设有一件商品，其id为10000；如果不使用锁，那么操作方法如下: 123456//查出商品状态select status from items where id=10000;//根据商品信息生成订单insert into orders(id,item_id) values(null,10000);//修改商品状态为2update Items set status=2 where id=10000; 上述场景在高并发环境下可能出现问题：前面已经提到只有商品的status=1是才能对它进行下单操作，上面第一步操作中，查询出来的商品status为1。但是当我们执行第三步update操作的时候，有可能出现其他人先一步对商品下单把Item的status修改为2了，但是我们并不知道数据已经被修改了，这样就可能造成同一个商品被下单2次，使得数据不一致。所以说这种方式是不安全的。使用悲观锁来实现：在上面的场景中，商品信息从查询出来到修改，中间有一个处理订单的过程，使用悲观锁的原理就是，当我们在查询出items信息后就把当前的数据锁定，直到我们修改完毕后再解锁。那么在这个过程中，因为items被锁定了，就不会出现有第三者来对其进行修改了。注：要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。我们可以使用命令设置MySQL为非autocommit模式：123456789101112set autocommit=0;设置完autocommit后，我们就可以执行我们的正常业务了。具体如下：//开始事务begin;/begin work;/start transaction; (三者选一就可以)//查询出商品信息select status from items where id=10000 for update;//根据商品信息生成订单insert into orders (id,item_id) values (null,10000);//修改商品status为2update items set status=2 where id=10000;//提交事务commit;/commit work; 注：上面的begin/commit为事务的开始和结束，因为在前一步我们关闭了mysql的autocommit，所以需要手动控制事务的提交，在这里就不细表了。上面的第一步我们执行了一次查询操作：select status from items where id=10000 for update;与普通查询不一样的是，我们使用了select…for update的方式，这样就通过数据库实现了悲观锁。此时在items表中，id为10000的 那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。注：需要注意的是，在事务中，只有SELECT … FOR UPDATE 或LOCK IN SHARE MODE 同一笔数据时会等待其它事务结束后才执行，一般SELECT … 则不受此影响。拿上面的实例来说，当我执行select status from items where id=10000 for update;后。我在另外的事务中如果再次执行select status from items where id=10000 for update;则第二个事务会一直等待第一个事务的提交，此时第二个查询处于阻塞的状态，但是如果我是在第二个事务中执行select status from items where id=10000;则能正常查询出数据，不会受第一个事务的影响。上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认Row-Level Lock，所以只有明确地指定主键，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。除了主键外，使用索引也会影响数据库的锁定级别。悲观锁并不是适用于任何场景，它也有它存在的一些不足，因为悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。如果加锁的时间过长，其他用户长时间无法访问，影响了程序的并发访问性，同时这样对数据库性能开销影响也很大，特别是对长事务而言，这样的开销往往无法承受。 乐观锁（ Optimistic Locking ）相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。那么我们如何实现乐观锁呢，一般来说有以下2种方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值+1。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。如果更新操作顺序执行，则数据的版本（version）依次递增，不会产生冲突。但是如果发生有不同的业务操作对同一版本的数据进行修改，那么，先提交的操作（图中B）会把数据version更新为2，当A在B之后提交更新时发现数据的version已经被修改了，那么A的更新操作会失败。 乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似（其实不用新加字段，用需要修改的字段作为条件进行修改操作即可），也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。以mysql InnoDB存储引擎为例，还是拿之前的例子商品表items表中有一个字段status，status=1表示该商品未被下单，status=2表示该商品已经被下单，那么我们对每个商品下单前必须确保此商品的status=1。假设有一件商品，其id为10000；下单操作包括3步骤： 12345//查询出商品信息select (status,version) from items where id=#&#123;id&#125;//根据商品信息生成订单//修改商品status为2update items set status=2,version=version+1 where id=#&#123;id&#125; and version=#&#123;version&#125;; 为了使用乐观锁，我们需要首先修改items表，增加一个version字段，数据默认version可设为1；其实我们周围的很多产品都有乐观锁的使用，比如我们经常使用的分布式存储引擎Tair，Tair中存储的每个数据都有版本号，版本号在每次更新后都会递增，相应的，在Tair put接口中也有此version参数，这个参数是为了解决并发更新同一个数据而设置的，这其实就是乐观锁；很多情况下，更新数据是先get，修改get回来的数据，然后put回系统。如果有多个客户端get到同一份数据，都对其修改并保存，那么先保存的修改就会被后到达的修改覆盖，从而导致数据一致性问题,在大部分情况下应用能够接受，但在少量特殊情况下，这个是我们不希望发生的。比如系统中有一个值”1”, 现在A和B客户端同时都取到了这个值。之后A和B客户端都想改动这个值，假设A要改成12，B要改成13，如果不加控制的话，无论A和B谁先更新成功，它的更新都会被后到的更新覆盖。Tair引入的乐观锁机制避免了这样的问题。刚刚的例子中，假设A和B同时取到数据，当时版本号是10，A先更新，更新成功后，值为12，版本为11。当B更新的时候，由于其基于的版本号是10，此时服务器会拒绝更新，返回version error，从而避免A的更新被覆盖。B可以选择get新版本的value，然后在其基础上修改，也可以选择强行更新。当然了，乐观锁也是要精心挑选的，主要的目的就是避免锁的失败率过高又要规避ABA问题。关于锁力度太大导致接口操作失败率过高。商品库存扣减时，尤其是在秒杀、聚划算这种高并发的场景下，若采用version号作为乐观锁，则每次只有一个事务能更新成功，业务感知上就是大量操作失败。若挑选以库存数作为乐观锁123456update item set quantity=quantity-#sub_quantity# where item_id = #id# and quantity-#sub_quantity# &gt; 0 通过挑选乐观锁，可以减小锁力度，从而提升吞吐乐观锁需要灵活运用,现在互联网高并发的架构中，受到fail-fast思路的影响，悲观锁已经非常少见了。]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>乐观锁</tag>
        <tag>悲观锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql事务隔离级别]]></title>
    <url>%2F2017%2F08%2F04%2Fmysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[mysql事务隔离级别第1级别：Read Uncommitted(读取未提交内容)1.所有事务都可以看到其他未提交事务的执行结果2.本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少3.该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据 #首先，修改隔离级别 set tx_isolation=&#39;READ-UNCOMMITTED&#39;; select @@tx_isolation; | @@tx_isolation | | READ-UNCOMMITTED | #事务A：启动一个事务 start transaction; select * from tx; | id | num | | 1 | 1 | | 2 | 2 | | 3 | 3 | #事务B：也启动一个事务(那么两个事务交叉了) #在事务B中执行更新语句，且不提交 start transaction; update tx set num=10 where id=1; select * from tx; | id | num | | 1 | 10 | | 2 | 2 | | 3 | 3 | #事务A：那么这时候事务A能看到这个更新了的数据吗? select * from tx; | id | num | | 1 | 10 | ---&gt;可以看到！说明我们读到了事务B还没有提交的数据 | 2 | 2 | | 3 | 3 | #事务B：事务B回滚,仍然未提交 rollback; select * from tx; | id | num | | 1 | 1 | | 2 | 2 | | 3 | 3 | #事务A：在事务A里面看到的也是B没有提交的数据 select * from tx; | id | num | | 1 | 1 | ---&gt;脏读意味着我在这个事务中(A中)，事务B虽然没有提交，但它任何一条数据变化，我都可以看到！ | 2 | 2 | | 3 | 3 | 第2级别：Read Committed(读取提交内容)1.这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）2.它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变3.这种隔离级别出现的问题是——不可重复读(Nonrepeatable Read)：不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。 ——&gt;导致这种情况的原因可能有： (1)有一个交叉的事务有新的commit，导致了数据的改变; (2)一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit #首先修改隔离级别 set tx_isolation=&#39;read-committed&#39;; select @@tx_isolation; | @@tx_isolation | | READ-COMMITTED | #事务A：启动一个事务 start transaction; select * from tx; | id | num | | 1 | 1 | | 2 | 2 | | 3 | 3 | #事务B：也启动一个事务(那么两个事务交叉了) #在这事务中更新数据，且未提交 start transaction; update tx set num=10 where id=1; select * from tx; | id | num | | 1 | 10 | | 2 | 2 | | 3 | 3 | #事务A：这个时候我们在事务A中能看到数据的变化吗? select * from tx; | id | num | | 1 | 1 |---&gt;并不能看到！ | 2 | 2 | | 3 | 3 | |——&gt;相同的select语句，结果却不一样 #事务B：如果提交了事务B呢? commit; #事务A: select * from tx; | id | num | | 1 | 10 |---&gt;因为事务B已经提交了，所以在A中我们看到了数据变化 | 2 | 2 | | 3 | 3 | 第3级别：Repeatable Read(可重读)1.这是MySQL的默认事务隔离级别2.它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行3.此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行4.InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题 #首先，更改隔离级别 set tx_isolation=&#39;repeatable-read&#39;; select @@tx_isolation; | @@tx_isolation | +------+------+ | REPEATABLE-READ | +------+------+ #事务A：启动一个事务 start transaction; select * from tx; | id | num | +------+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +------+------+ #事务B：开启一个新事务(那么这两个事务交叉了) #在事务B中更新数据，并提交 start transaction; update tx set num=10 where id=1; select * from tx; +------+------+ | id | num | +------+------+ | 1 | 10 | | 2 | 2 | | 3 | 3 | +------+------+ commit; #事务A：这时候即使事务B已经提交了,但A能不能看到数据变化？ select * from tx;+------+------+ | id | num | +------+------+ | 1 | 1 | ---&gt;还是看不到的！(这个级别2不一样，也说明级别3解决了不可重复读问题) | 2 | 2 | | 3 | 3 | +------+------+ #事务A：只有当事务A也提交了，它才能够看到数据变化 commit;select * from tx; +------+------+ | id | num | +------+------+ | 1 | 10 | | 2 | 2 | | 3 | 3 | +------+------+ 第4级别：Serializable(可串行化)1.这是最高的隔离级别2.它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。3.在这个级别，可能导致大量的超时现象和锁竞争 #首先修改隔离界别 set tx_isolation=&#39;serializable&#39;; select @@tx_isolation; +----------------+ | @@tx_isolation | +----------------+ | SERIALIZABLE | +----------------+ #事务A：开启一个新事务 start transaction; #事务B：在A没有commit之前，这个交叉事务是不能更改数据的 start transaction; insert tx values(&#39;4&#39;,&#39;4&#39;); ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionupdate tx set num=10 where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>事务隔离级别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equals、==和hashCode]]></title>
    <url>%2F2017%2F08%2F01%2Fequals%E3%80%81-%E5%92%8ChashCode%2F</url>
    <content type="text"><![CDATA[引言equals：是否同一个对象实例。注意，是“实例”。比如String s = new String(“test”); s.equals(s), 这就是同一个对象实例的比较； 等号(==)：对比对象实例的内存地址（也即对象实例的ID），来判断是否是同一对象实例；又可以说是判断对象实例是否物理相等； Hashcode：我觉得可以这样理解：并不是对象的内存地址，而是利用hash算法，对对象实例的一种描述符（或者说对象存储位置的hash算法映射）——对象实例的哈希码。 对于==，比较的是值是否相等 如果作用于基本数据类型的变量，则直接比较其存储的 “值”是否相等； 如果作用于引用类型的变量，则比较的是所指向的对象的地址 对于equals方法，注意：equals方法不能作用于基本数据类型的变量，equals继承Object类，比较的是是否是同一个对象 如果没有对equals方法进行重写，则比较的是引用类型的变量所指向的对象的地址； 诸如String、Date等类对equals方法进行了重写的话，比较的是所指向的对象的内容。 ==比较的是对象的地址String重写的equals比较的是字符串的内容值String重写的hashCode已经不是对象内存地址的hash码，是根据内容产生的，因为a、b是两个完全不同的对象，也满足这条规律“equals相等的两个对象，hashCode也相等”。System.identityHashCode是未被重写的获取对象内存地址hash码的函数，new出来的String对象的内存地址是不一样的，所以hash值也不一样 示例12345678910111213141516171819202122232425public class Test &#123; public static void main(String[] args) &#123; String a=new String(&quot;foo&quot;); String b=new String(&quot;foo&quot;); String c=&quot;hello&quot;; String d=&quot;hello&quot;; System.out.println(&quot;memory address hashcode a:&quot;+System.identityHashCode(a)); System.out.println(&quot;memory address hashcode a:&quot;+System.identityHashCode(b)); System.out.println(&quot;String hashcode a: &quot;+a.hashCode()); System.out.println(&quot;String hashcode a: &quot;+b.hashCode()); System.out.println(&quot;a==b: &quot;+(a==b)); System.out.println(&quot;a.equals(b): &quot;+a.equals(b)); System.out.println(&quot;&quot;); System.out.println(&quot;memory address hashcode c:&quot;+System.identityHashCode(c)); System.out.println(&quot;memory address hashcode d:&quot;+System.identityHashCode(d)); System.out.println(&quot;String hashcode c: &quot;+c.hashCode()); System.out.println(&quot;String hashcode d: &quot;+d.hashCode()); System.out.println(&quot;c==d: &quot;+(c==d)); System.out.println(&quot;c.equals(d): &quot;+c.equals(d)); &#125;&#125; 结果：123456789101112memory address hashcode a:8222510memory address hashcode a:18581223String hashcode a: 101574String hashcode a: 101574a==b: falsea.equals(b): truememory address hashcode c:3526198memory address hashcode d:3526198String hashcode c: 99162322String hashcode d: 99162322c==d: truec.equals(d): true 从Java集合的常用需求为什么需要使用HashcodeJava中的集合（Collection）有两类，一类是List，再有一类是Set。前者集合内的元素是有序的，元素可以重复；后者元素无序，但元素不可重复。那么这里就有一个比较严重的问题了：要想保证元素不重复，可两个元素是否重复应该依据什么来判断呢？这就是 Object.equals方法了。但是，如果每增加一个元素就检查一次，那么当元素很多时，后添加到集合中的元素比较的次数就非常多了。也就是说，如果集合中现在已经有1000个元素，那么第1001个元素加入集合时，它就要调用1000次equals方法。这显然会大大降低效率。 于是，Java采用了哈希表的原理。哈希算法也称为散列算法，是将数据依特定算法直接指定到一个地址上。可以这样简单理解，hashCode方法实际上返回的就是对象存储位置的映像。 这样一来，当集合要添加新的元素时，先调用这个元素的hashCode方法，就能定位到它应该放置的bucket存储位置。如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了；如果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就表示发生冲突了，散列表对于冲突有具体的解决办法，但最终还会将新元素保存在适当的位置。这样一来，实际调用equals方法的次数就大大降低了，几乎只需要一两次。 简单归纳，hashmap的深入理解： HashMap的数据结构是基于数组和链表的。（以数组存储元素，如有hash相同的元素，在数组结构中，创建链表结构，再把hash相同的元素放到链表的下一个节点） hashMap的结构类似这样 元素0—&gt;[hashCode=0, key.value=x1的数据] 元素1—&gt;[hashCode=1, key.value=y1的数据] 。。。。。。 元素n—&gt;[hashCode=n, key.value=z1的数据] 假设没有hashCode=1的元素加入，但是有两个hashCode=0的数据，它的结构就变成这样 元素0—&gt;[hashCode=0, key.value=x1的数据].next—&gt;[hashCode=0, key.value=x2的数据] 元素1—&gt;[null] …… 元素n—&gt;[hashCode=n, key.value=z1的数据] put和get都首先会调用hashcode方法，去查找相关的key，当有冲突时，再调用equals（这也是为什么刚开始就重温hashcode和equals的原因）！HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 重写 equals 的时候必须重写 hashCodeSUN官方的文档中规定”如果重定义equals方法，就必须重定义hashCode方法,以便用户可以将对象插入到散列(哈希)表中” 那么 SUN 公司是出于什么考虑做了这个规定呢？ 在集合框架中的HashSet，HashTable和HashMap都使用哈希表的形式存储数据，而hashCode计算出来的哈希码便是它们的身份证。哈希码的存在便可以： 快速定位对象，提高哈希表集合的性能。只有当哈希表中对象的索引即hashCode和对象的属性即equals同时相等时，才能够判断两个对象相等。从上面可以看出，哈希码主要是为哈希表服务的，其实如果不需要使用哈希表，也可以不重写hashCode。但是SUN公司应该是出于对程序扩展性的考虑（万一以后需要将对象放入哈希表集合中），才会规定重写equals的同时需要重写hashCode，以避免后续开发不必要的麻烦。 重写equals的注意事项 Java语言规范要求equals需要具有如下的特性： 自反性：对于任何非空引用 x，x.equals() 应该返回 true。对称性：对于任何引用 x 和 y，当且仅当 y.equals(x) 返回 true，x.equals(y) 也应该返回 true。传递性：对于任何引用 x、y 和 z，如果 x.equals(y)返回 true，y.equals(z) 也应返回同样的结果。一致性：如果 x 和 y 引用的对象没有发生变化，反复调用 x.equals(y) 应该返回同样的结果。对于任意非空引用 x，x.equals(null) 应该返回 false。在对象比较时，我们应该如何编写出一个符合特性的 equals 方法呢，《Core Java》中提出了如下建议： 显式参数命名为 otherObject，稍后将它转换成另一个叫做 other 的变量。检测 this 与 otherObject 是否引用同一个对象： if (this == otherObject) return true;计算这个等式可以避免一个个比较类中的域，实现优化。 检测 otherObject 是否为 null，如果为 null，返回 false。进行非空校验是十分重要的。 比较 this 与 otherObject 是否属于同一个类。 如果每个子类都重写了 equals，使用 getClass 检验：12if (getClass() != otherObject.getClass()) return false; 如果所有子类都使用同一个 equals，就用 instanceof 检验：12if (!(otherObject instanceof ClassName)) return false; 将 otherObject 转换为相应的类型变量。1ClassName other = (ClassName) otherObject; 现在可以对所有需要比较的域进行比较了。 基本类型使用 == 比较对象使用 equals 比较数组类型的域可以使用静态方法 Arrays.equals检测相应数组元素是否相等如果所有域匹配，则返回 true注意：子类重写父类 equals 方法时，必须完全覆盖父类方法，不能因为类型错误或者其他原因定义了一个完全无关的方法。可以使用 @Override 注解对覆盖父类的方法进行标记，这样编译器便会检测到覆盖过程中的错误。 重写 hashCode 的注意事项散列码（hash code）是由对象导出的一个整型值。散列码没有规律，在不同的对象中通过不同的算法生成，Java中生成 hashCode 的策略为（以下说明均摘自 Java API 8）： String 类的 hashCode 根据其字符串内容，使用算法计算后返回哈希码。1Returns a hash code for this string. The hash code for a String object is computed as s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] Integer 类返回的哈希码为其包含的整数数值。1Returns: a hash code value for this object, equal to the primitive int value represented by this Integer object. Object 类的 hashCode 返回对象的内存地址经过处理后的数值。1Returns a hash code value for the object. This method is supported for the benefit of hash tables such as those provided by HashMap. 在自己的类中想要重写 hashCode 的话一般怎么做呢？建议合理地组合实例域的散列码，让各个不同对象产生的散列码更加均匀。例如我们现在有一个 Cat 对象，它有 name、size 和 color 三个不同域，那么可以重写 hashCode 方法如下： 12345678910class Cat &#123; ...... public int hashCode() &#123; //hashCode是可以返回负值的 return 6 * name.hashCode() + 8 * new Double(size).hashCode() + 10 * color.hashCode(); &#125; ......&#125; 当然还有更好的做法，我们可以直接调用静态方法 Objects.hash 并提供多个参数。这个方法会对各个参数调用 Object.hashCode，并组合返回的散列码。故以上的方法可以缩写为：123public int hashCode() &#123; return Objects.hash(name, size, color);&#125; 【注意】 equals与hashCode的定义必须一致，两个对象equals为true，就必须有相同的hashCode。例如：如果定义的equals比较的是小猫的 name，那么hashCode就需要散列该 name，而不是小猫的 color 或 size。]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>==</tag>
        <tag>equals</tag>
        <tag>hashCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap工作原理]]></title>
    <url>%2F2017%2F07%2F28%2FHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[引言HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ HashMap“你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？”你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者知道HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。我们可以使用CocurrentHashMap来代替Hashtable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看这篇博客查看Hashtable和ConcurrentHashMap的区别。我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念HashMap中解决碰撞的方法equals()和hashCode()的应用，以及它们在HashMap中的重要性不可变对象的好处HashMap多线程的条件竞争重新调整HashMap的大小 HashMap的工作原理HashMap 里面是一个数组，然后数组中每个元素是一个单向链表,存储的是Entry对象，包含四个属性：key, value, hash 值和用于单向链表的 next。capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍。loadFactor：负载因子，默认为 0.75。threshold：扩容的阈值，等于 capacity * loadFactor HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。 简单归纳，hashmap的深入理解： HashMap的数据结构是基于数组和链表的。（以数组存储元素，如有hash相同的元素，在数组结构中，创建链表结构，再把hash相同的元素放到链表的下一个节点） hashMap的结构类似这样 元素0—&gt;[hashCode=0, key.value=x1的数据] 元素1—&gt;[hashCode=1, key.value=y1的数据] 。。。。。。 元素n—&gt;[hashCode=n, key.value=z1的数据] 假设没有hashCode=1的元素加入，但是有两个hashCode=0的数据，它的结构就变成这样 元素0—&gt;[hashCode=0, key.value=x1的数据].next—&gt;[hashCode=0, key.value=x2的数据] 元素1—&gt;[null] …… 元素n—&gt;[hashCode=n, key.value=z1的数据] put和get都首先会调用hashcode方法，去查找相关的key，当有冲突时，再调用equals（这也是为什么刚开始就重温hashcode和equals的原因）！ put过程public V put(K key, V value) { // 当插入第一个元素的时候，需要先初始化数组大小 if (table == EMPTY_TABLE) { inflateTable(threshold); } // 如果 key 为 null，感兴趣的可以往里看，最终会将这个 entry 放到 table[0] 中 if (key == null) return putForNullKey(value); // 1. 求 key 的 hash 值 int hash = hash(key); // 2. 找到对应的数组下标 int i = indexFor(hash, table.length); // 3. 遍历一下对应下标处的链表，看是否有重复的 key 已经存在， // 如果有，直接覆盖，put 方法返回旧值就结束了 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 4. 不存在重复的 key，将此 entry 添加到链表中，细节后面说 addEntry(hash, key, value, i); return null; } 数组初始化在第一个元素插入 HashMap 的时候做一次数组的初始化，就是先确定初始的数组大小，并计算数组扩容的阈值。 private void inflateTable(int toSize) { // 保证数组大小一定是 2 的 n 次方。 // 比如这样初始化：new HashMap(20)，那么处理成初始数组大小是 32 int capacity = roundUpToPowerOf2(toSize); // 计算扩容阈值：capacity * loadFactor threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); // 算是初始化数组吧 table = new Entry[capacity]; initHashSeedAsNeeded(capacity); //ignore } 这里有一个将数组大小保持为 2 的 n 次方的做法，Java7 和 Java8 的 HashMap 和 ConcurrentHashMap 都有相应的要求，只不过实现的代码稍微有些不同，后面再看到的时候就知道了。 计算具体数组位置这个简单，我们自己也能 YY 一个：使用 key 的 hash 值对数组长度进行取模就可以了。 static int indexFor(int hash, int length) { // assert Integer.bitCount(length) == 1 : &quot;length must be a non-zero power of 2&quot;; return hash &amp; (length-1); } 这个方法很简单，简单说就是取 hash 值的低 n 位。如在数组长度为 32 的时候，其实取的就是 key 的 hash 值的低 5 位，作为它在数组中的下标位置。 添加节点到链表中找到数组下标后，会先进行 key 判重，如果没有重复，就准备将新值放入到链表的表头。 void addEntry(int hash, K key, V value, int bucketIndex) { // 如果当前 HashMap 大小已经达到了阈值，并且新值要插入的数组位置已经有元素了，那么要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { // 扩容，后面会介绍一下 resize(2 * table.length); // 扩容以后，重新计算 hash 值 hash = (null != key) ? hash(key) : 0; // 重新计算扩容后的新的下标 bucketIndex = indexFor(hash, table.length); } // 往下看 createEntry(hash, key, value, bucketIndex); } // 这个很简单，其实就是将新值放到链表的表头，然后 size++ void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++; } 这个方法的主要逻辑就是先判断是否需要扩容，需要的话先扩容，然后再将这个新的数据插入到扩容后的数组的相应位置处的链表的表头。 数组扩容前面我们看到，在插入新值的时候，如果当前的 size 已经达到了阈值，并且要插入的数组位置上已经有元素，那么就会触发扩容，扩容后，数组大小为原来的 2 倍。 当hashmap中的元素越来越多的时候，碰撞的几率也就越来越高（因为数组的长度是固定的），所以为了提高查询的效率，就要对hashmap的数组进行扩容，数组扩容这个操作也会出现在ArrayList中，所以这是一个通用的操作，很多人对它的性能表示过怀疑，不过想想我们的“均摊”原理，就释然了，而在hashmap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。 那么hashmap什么时候进行扩容呢？当hashmap中的元素个数超过数组大小 * loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过16 * 0.75=12的时候，就把数组的大小扩展为2 * 16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知hashmap中元素的个数，那么预设元素的个数能够有效的提高hashmap的性能。比如说，我们有1000个元素new HashMap(1000), 但是理论上来讲new HashMap(1024)更合适，即使是1000，hashmap也自动会将其设置为1024。 但是new HashMap(1024)还不是更合适的，因为0.75 * 1000 &lt; 1000, 也就是说为了让0.75 * size &gt; 1000, 我们必须这样new HashMap(2048)才最合适，既考虑了&amp;的问题，也避免了resize的问题。 void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } // 新的数组 Entry[] newTable = new Entry[newCapacity]; // 将原来数组中的值迁移到新的更大的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } 扩容就是用一个新的大数组替换原来的小数组，并将原来数组中的值迁移到新的数组中。 由于是双倍扩容，迁移过程中，会将原来 table[i] 中的链表的所有节点，分拆到新的数组的 newTable[i] 和 newTable[i + oldLength] 位置上。如原来数组长度是 16，那么扩容后，原来 table[0] 处的链表中的所有元素会被分配到新数组中 newTable[0] 和 newTable[16] 这两个位置。代码比较简单，这里就不展开了。 get过程相对于 put 过程，get 过程是非常简单的。根据 key 计算 hash 值。找到相应的数组下标：hash &amp; (length – 1)。遍历该数组位置处的链表，直到找到相等(==或equals)的 key。 public V get(Object key) { // 之前说过，key 为 null 的话，会被放到 table[0]，所以只要遍历下 table[0] 处的链表就可以了 if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); } getEntry(key): final Entry&lt;K,V&gt; getEntry(Object key) { return null; } int hash = (key == null) ? 0 : hash(key); // 确定数组下标，然后从头开始遍历链表，直到找到为止 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } Java7 ConcurrentHashMapConcurrentHashMap 和 HashMap 思路是差不多的，但是因为它支持并发操作，所以要复杂一些。 整个 ConcurrentHashMap 由一个个 Segment 组成，Segment 代表”部分“或”一段“的意思，所以很多地方都会将其描述为分段锁。注意，行文中，我很多地方用了“槽”来代表一个 segment。 简单理解就是，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 首先如何从ConcurrentHashMap定位到HashEntry。在HashMap的原理分析部分说过，对于一个Hash的数据结构来说，为了减少浪费的空间和快速定位数据，那么就需要数据在Hash上的分布比较均匀。对于一次Map的查找来说，首先就需要定位到Segment，然后从过Segment定位到HashEntry链表，最后才是通过遍历链表得到需要的元素。 在不讨论并发的前提下先来讨论如何定位到HashEntry的。在ConcurrentHashMap中是通过hash(key.hashCode())和segmentFor(hash)来得到Segment的。其中hash(int)是将key的hashCode进行二次编码，使之能够在segmentMask+1个Segment上均匀分布（默认是16个）。可以看到的是这里和HashMap还是有点不同的，这里采用的算法叫Wang/Jenkins hash，有兴趣的可以自行查阅。总之它的目的就是使元素能够均匀的分布在不同的Segment上，这样才能够支持最多segmentMask+1个并发，这里segmentMask+1是segments的大小。 定位Segment private static int hash(int h) { // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16); } final Segment&lt;K,V&gt; segmentFor(int hash) { return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask]; } 显然在不能够对Segment扩容的情况下，segments的大小就应该是固定的。所以在ConcurrentHashMap中segments/segmentMask/segmentShift都是常量，一旦初始化后就不能被再次修改，其中segmentShift是查找Segment的一个常量偏移量。 有了Segment以后再定位HashEntry就和HashMap中定位HashEntry一样了，先将hash值与Segment中HashEntry的大小减1进行与操作定位到HashEntry链表，然后遍历链表就可以完成相应的操作了。 能够定位元素以后ConcurrentHashMap就已经具有了HashMap的功能了，现在要解决的就是如何并发的问题。要解决并发问题，加锁是必不可免的。再回头看Segment的类图，可以看到Segment除了有一个volatile类型的元素大小count外，Segment还是集成自ReentrantLock的。另外在前面的原子操作和锁机制中介绍过，要想最大限度的支持并发，那么能够利用的思路就是尽量读操作不加锁，写操作不加锁。如果是读操作不加锁，写操作加锁，对于竞争资源来说就需要定义为volatile类型的。volatile类型能够保证happens-before法则，所以volatile能够近似保证正确性的情况下最大程度的降低加锁带来的影响，同时还与写操作的锁不产生冲突。 同时为了防止在遍历HashEntry的时候被破坏，那么对于HashEntry的数据结构来说，除了value之外其他属性就应该是常量，否则不可避免的会得到ConcurrentModificationException。这就是为什么HashEntry数据结构中key,hash,next是常量的原因(final类型）。 有了上面的分析和条件后再来看Segment的get/put/remove就容易多了。 concurrencyLevel：并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的。 再具体到每个 Segment 内部，其实每个 Segment 很像之前介绍的 HashMap，不过它要保证线程安全，所以处理起来要麻烦些。 初始化initialCapacity：初始容量，这个值指的是整个 ConcurrentHashMap 的初始容量，实际操作的时候需要平均分给每个 Segment。 loadFactor：负载因子，之前我们说了，Segment 数组不可以扩容，所以这个负载因子是给每个 Segment 内部使用的。 public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; // 计算并行级别 ssize，因为要保持并行级别是 2 的 n 次方 while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } // 我们这里先不要那么烧脑，用默认值，concurrencyLevel 为 16，sshift 为 4 // 那么计算出 segmentShift 为 28，segmentMask 为 15，后面会用到这两个值 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // initialCapacity 是设置整个 map 初始的大小， // 这里根据 initialCapacity 计算 Segment 数组中每个位置可以分到的大小 // 如 initialCapacity 为 64，那么每个 Segment 或称之为&quot;槽&quot;可以分到 4 个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组， // 并创建数组的第一个元素 segment[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入 segment[0] UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss; } 初始化完成，我们得到了一个 Segment 数组。 我们就当是用 new ConcurrentHashMap() 无参构造函数进行初始化的，那么初始化完成后： Segment 数组长度为 16，不可以扩容 Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，后面的代码会介绍 当前 segmentShift 的值为 32 – 4 = 28，segmentMask 为 16 – 1 = 15，姑且把它们简单翻译为移位数和掩码，这两个值马上就会用到 put 过程分析我们先看 put 的主流程，对于其中的一些关键细节操作，后面会进行详细介绍。 public V put(K key, V value) { Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 根据 hash 值找到 Segment 数组中的位置 j // hash 是 32 位，无符号右移 segmentShift(28) 位，剩下低 4 位， // 然后和 segmentMask(15) 做一次与操作，也就是说 j 是 hash 值的最后 4 位，也就是槽的数组下标 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 刚刚说了，初始化的时候初始化了 segment[0]，但是其他位置还是 null， // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false); } 第一层皮很简单，根据 hash 值很快就能找到相应的 Segment，之后就是 Segment 内部的 put 操作了。Segment 内部是由 数组+链表 组成的，key不能为空 final V put(K key, int hash, V value, boolean onlyIfAbsent) { // 在往该 segment 写入前，需要先获取该 segment 的独占锁 // 先看主流程，后面还会具体介绍这部分内容 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { // 这个是 segment 内部的数组 HashEntry&lt;K,V&gt;[] tab = table; // 再利用 hash 值，求应该放置的数组下标 int index = (tab.length - 1) &amp; hash; // first 是数组该位置处的链表的表头 HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 下面这串 for 循环虽然很长，不过也很好理解，想想该位置没有任何元素和已经存在一个链表这两种情况 for (HashEntry&lt;K,V&gt; e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { // 覆盖旧值 e.value = value; ++modCount; } break; } // 继续顺着链表走 e = e.next; } else { // node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。 // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值，这个 segment 需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 扩容后面也会具体分析 else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， // 其实就是将新的节点设置成原链表的表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { // 解锁 unlock(); } return oldValue; } 整体流程还是比较简单的，由于有独占锁的保护，所以 segment 内部的操作并不复杂。至于这里面的并发问题，我们稍后再进行介绍。 到这里 put 操作就结束了，接下来，我们说一说其中几步关键的操作。 初始化槽: ensureSegment ConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0]，对于其他槽来说，在插入第一个值的时候进行初始化。 这里需要考虑并发，因为很可能会有多个线程同时进来初始化同一个槽 segment[k]，不过只要有一个成功了就可以。 private Segment&lt;K,V&gt; ensureSegment(int k) { final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg; } 总的来说，ensureSegment(int k) 比较简单，对于并发操作使用 CAS 进行控制。 我没搞懂这里为什么要搞一个 while 循环，CAS 失败不就代表有其他线程成功了吗，为什么要再进行判断？ 获取写入锁: scanAndLockForPut 前面我们看到，在往某个 segment 中 put 的时候，首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value)，也就是说先进行一次 tryLock() 快速获取该 segment 的独占锁，如果失败，那么进入到 scanAndLockForPut 这个方法来获取锁。 下面我们来具体分析这个方法中是怎么控制加锁的。 private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) { HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) { HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) { if (e == null) { if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; } // 重试次数如果超过 MAX_SCAN_RETRIES（单核1多核64），那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) { lock(); break; } else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed retries = -1; } } return node; } 这个方法有两个出口，一个是 tryLock() 成功了，循环终止，另一个就是重试次数超过了 MAX_SCAN_RETRIES，进到 lock() 方法，此方法会阻塞等待，直到成功拿到独占锁。 这个方法就是看似复杂，但是其实就是做了一件事，那就是获取该 segment 的独占锁，如果需要的话顺便实例化了一下 node。 扩容: rehash重复一下，segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry\[] 进行扩容，扩容后，容量为原来的 2 倍。 首先，我们要回顾一下触发扩容的地方，put 的时候，如果判断该值的插入会导致该 segment 的元素个数超过阈值，那么先进行扩容，再插值，读者这个时候可以回去 put 方法看一眼。 该方法不需要考虑并发，因为到这里的时候，是持有该 segment 的独占锁的。// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。 private void rehash(HashEntry&lt;K,V&gt; node) { HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) { // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) { HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else { // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头结点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); } } } } // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable; } 这里的扩容比之前的 HashMap 要复杂一些，代码难懂一点。上面有两个挨着的 for 循环，第一个 for 有什么用呢？ 仔细一看发现，如果没有第一个 for 循环，也是可以工作的，但是，这个 for 循环下来，如果 lastRun 的后面还有比较多的节点，那么这次就是值得的。因为我们只需要克隆 lastRun 前面的节点，后面的一串节点跟着 lastRun 走就是了，不需要做任何操作。 我觉得 Doug Lea 的这个想法也是挺有意思的，不过比较坏的情况就是每次 lastRun 都是链表的最后一个元素或者很靠后的元素，那么这次遍历就有点浪费了。不过 Doug Lea 也说了，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆。 get 过程分析相对于 put 来说，get 真的不要太简单。计算 hash 值，找到 segment 数组中的具体位置，或我们前面用的“槽”槽中也是一个数组，根据 hash 找到数组中具体的位置到这里是链表了，顺着链表进行查找即可 public V get(Object key) { Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 1. hash 值 int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // 2. 根据 hash 找到对应的 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) { // 3. 找到segment 内部数组相应位置的链表，遍历 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; } } return null; } 并发问题分析现在我们已经说完了 put 过程和 get 过程，我们可以看到 get 过程中是没有加锁的，那自然我们就需要去考虑并发问题。 添加节点的操作 put 和删除节点的操作 remove 都是要加 segment 上的独占锁的，所以它们之间自然不会有问题，我们需要考虑的问题就是 get 的时候在同一个 segment 中发生了 put 或 remove 操作。 put 操作的线程安全性 初始化槽，这个我们之前就说过了，使用了 CAS 来初始化 Segment 中的数组。 添加节点到链表的操作是插入到表头的，所以，如果这个时候 get 操作在链表遍历的过程已经到了中间，是不会影响的。当然，另一个并发问题就是 get 操作在 put 之后，需要保证刚刚插入表头的节点被读取，这个依赖于 setEntryAt 方法中使用的 UNSAFE.putOrderedObject。 扩容。扩容是新创建了数组，然后进行迁移数据，最后面将 newTable 设置给属性 table。所以，如果 get 操作此时也在进行，那么也没关系，如果 get 先行，那么就是在旧的 table 上做查询操作；而 put 先行，那么 put 操作的可见性保证就是 table 使用了 volatile 关键字。 remove 操作的线程安全性remove 操作我们没有分析源码，所以这里说的读者感兴趣的话还是需要到源码中去求实一下的。 get 操作需要遍历链表，但是 remove 操作会”破坏”链表。 如果 remove 破坏的节点 get 操作已经过去了，那么这里不存在任何问题。 如果 remove 先破坏了一个节点，分两种情况考虑。1、如果此节点是头结点，那么需要将头结点的 next 设置为数组该位置的元素，table 虽然使用了 volatile 修饰，但是 volatile 并不能提供数组内部操作的可见性保证，所以源码中使用了 UNSAFE 来操作数组，请看方法 setEntryAt。2、如果要删除的节点不是头结点，它会将要删除节点的后继节点接到前驱节点中，这里的并发保证就是 next 属性是 volatile 的。 数据结构要知道hashmap是什么，首先要搞清楚它的数据结构，在java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，hashmap也不例外。Hashmap实际上是一个数组和链表的结合体（在数据结构中，一般称之为“链表散列“），请看下图（横排表示数组，纵排表示数组元素【实际上是一个链表】）。 从图中我们可以看到一个hashmap就是一个数组结构，当新建一个hashmap的时候，就会初始化一个数组。我们来看看java代码： 12345/** * The table, resized as necessary. Length MUST Always be a power of two. * FIXME 这里需要注意这句话，至于原因后面会讲到 */ transient Entry[] table; 1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; final int hash; Entry&lt;K,V&gt; next; .......... &#125; 上面的Entry就是数组中的元素，它持有一个指向下一个元素的引用，这就构成了链表。当我们往hashmap中put元素的时候，先根据key的hash值得到这个元素在数组中的位置（即下标），然后就可以把这个元素放到对应的位置中了。如果这个元素所在的位子上已经存放有其他元素了，那么在同一个位子上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。从hashmap中get元素时，首先计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。从这里我们可以想象得到，如果每个位置上的链表只有一个元素，那么hashmap的get效率将是最高的，但是理想总是美好的，现实总是有困难需要我们去克服 hash算法我们可以看到在hashmap中要找到某个元素，需要根据key的hash值来求得对应数组中的位置。如何计算这个位置就是hash算法。前面说过hashmap的数据结构是数组和链表的结合，所以我们当然希望这个hashmap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。 所以我们首先想到的就是把hashcode对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，“模”运算的消耗还是比较大的，能不能找一种更快速，消耗更小的方式那？java中是这样做的， 123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 首先算得key得hashcode值，然后跟数组的长度-1做一次“与”运算（&amp;）。看上去很简单，其实比较有玄机。比如数组的长度是2的4次方，那么hashcode就会和2的4次方-1做“与”运算。很多人都有这个疑问，为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高，我以2的4次方举例，来解释一下为什么数组大小为2的幂时hashmap访问的性能最高。 看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ 所以说，当数组长度为2的n次幂的时候，不同的key算得得index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。说到这里，我们再回头看一下hashmap中默认的数组大小是多少，查看源代码可以得知是16，为什么是16，而不是15，也不是20呢，看到上面annegu的解释之后我们就清楚了吧，显然是因为16是2的整数次幂的原因，在小数据量的情况下16比15和20更能减少key之间的碰撞，而加快查询的效率。 所以，在存储大容量数据的时候，最好预先指定hashmap的size为2的整数次幂次方。就算不指定的话，也会以大于且最接近指定值大小的2次幂来初始化的，代码如下(HashMap的构造方法中)： // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; key的hashcode与equals方法改写在第一部分hashmap的数据结构中，annegu就写了get方法的过程：首先计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。所以，hashcode与equals方法对于找到对应元素是两个关键方法。 Hashmap的key可以是任何类型的对象，例如User这种对象，为了保证两个具有相同属性的user的hashcode相同，我们就需要改写hashcode方法，比方把hashcode值的计算与User对象的id关联起来，那么只要user对象拥有相同id，那么他们的hashcode也能保持一致了，这样就可以找到在hashmap数组中的位置了。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写了hashcode方法是不够的，equals方法也是需要改写滴~当然啦，按正常思维逻辑，equals方法一般都会根据实际的业务内容来定义，例如根据user对象的id来判断两个user是否相等。在改写equals方法的时候，需要满足以下三点：(1) 自反性：就是说a.equals(a)必须为true。(2) 对称性：就是说a.equals(b)=true的话，b.equals(a)也必须为true。(3) 传递性：就是说a.equals(b)=true，并且b.equals(c)=true的话，a.equals(c)也必须为true。通过改写key对象的equals和hashcode方法，我们可以将任意的业务对象作为map的key(前提是你确实有这样的需要)。 本文主要描述了HashMap的结构，和hashmap中hash函数的实现，以及该实现的特性，同时描述了hashmap中resize带来性能消耗的根本原因，以及将普通的域模型对象作为key的基本要求。尤其是hash函数的实现，可以说是整个HashMap的精髓所在，只有真正理解了这个hash函数，才可以说对HashMap有了一定的理解。 Hashtable和ConcurrentHashMap Hashtable对get,put,remove都使用了同步操作，它的同步级别是正对Hashtable来进行同步的，也就是说如果有线程正在遍历集合，其他的线程就暂时不能使用该集合了，这样无疑就很容易对性能和吞吐量造成影响，从而形成单点。而ConcurrentHashMap则不同，它只对put,remove操作使用了同步操作，get操作并不影响。public V get(Object key)不涉及到锁，也就是说获得对象时没有使用锁；put、remove方法要使用锁，但并不一定有锁争用，原因在于ConcurrentHashMap将缓存的变量分到多个Segment，每个Segment上有一个锁，只要多个线程访问的不是一个Segment就没有锁争用，就没有堵塞，各线程用各自的锁，ConcurrentHashMap缺省情况下生成16个Segment，也就是允许16个线程并发的更新而尽量没有锁争用；当前ConcurrentHashMap这样的做法对一些线程要求很严格的程序来说，还是有所欠缺的，对应这样的程序来说，如果不考虑性能和吞吐量问题的话，个人觉得使用Hashtable还是比较合适的； Hashtable在使用iterator遍历的时候，如果其他线程，包括本线程对Hashtable进行了put，remove等更新操作的话，就会抛出ConcurrentModificationException异常，但如果使用ConcurrentHashMap的话，就不用考虑这方面的问题了，Iterator对象的使用，不一定是和其它更新线程同步，获得的对象可能是更新前的对象，ConcurrentHashMap允许一边更新、一边遍历，也就是说在Iterator对象遍历的时候，ConcurrentHashMap也可以进行remove,put操作，且遍历的数据会随着remove,put操作产出变化，所以希望遍历到当前全部数据的话，要么以ConcurrentHashMap变量为锁进行同步(synchronized该变量)，要么使用CopiedIterator包装iterator，使其拷贝当前集合的全部数据，但是这样生成的iterator不可以进行remove操作。 1.HashMap或者ArrayList边遍历边删除数据会报java.util.ConcurrentModificationException异常 Map&lt;Long, String&gt; mReqPacket = new HashMap&lt;Long, String&gt;(); for (long i = 0; i &lt; 15; i++) { mReqPacket.put(i, i + &quot;&quot;); } for (Entry&lt;Long, String&gt; entry : mReqPacket.entrySet()) { long key = entry.getKey(); String value = entry.getValue(); if (key &lt; 10) { mReqPacket.remove(key); } } for (Entry&lt;Long, String&gt; entry : mReqPacket.entrySet()) { System.out.println(entry.getKey() + &quot; &quot; + entry.getValue()); } 所以要用迭代器删除元素： Map&lt;Long, String&gt; mReqPacket = new HashMap&lt;Long, String&gt;(); for (long i = 0; i &lt; 15; i++) { mReqPacket.put(i, i + &quot;&quot;); } for (Iterator&lt;Entry&lt;Long, String&gt;&gt; iterator = mReqPacket.entrySet().iterator(); iterator.hasNext();) { Entry&lt;Long, String&gt; entry = iterator.next(); long key = entry.getKey(); if (key &lt; 10) { iterator.remove(); } } for (Entry&lt;Long, String&gt; entry : mReqPacket.entrySet()) { System.out.println(entry.getKey() + &quot; &quot; + entry.getValue()); } 2.对ConcurrentHashMap边遍历边删除或者增加操作不会产生异常(可以不用迭代方式删除元素)，因为其内部已经做了维护，遍历的时候都能获得最新的值。即便是多个线程一起删除、添加元素也没问题。 Map&lt;Long, String&gt; conMap = new ConcurrentHashMap&lt;Long, String&gt;(); for (long i = 0; i &lt; 15; i++) { conMap.put(i, i + &quot;&quot;); } for (Entry&lt;Long, String&gt; entry : conMap.entrySet()) { long key = entry.getKey(); if (key &lt; 10) { conMap.remove(key); } } for (Entry&lt;Long, String&gt; entry : conMap.entrySet()) { System.out.println(entry.getKey() + &quot; &quot; + entry.getValue()); } 3.一个线程对ConcurrentHashMap增加数据，另外一个线程在遍历时就能获得。 static Map&lt;Long, String&gt; conMap = new ConcurrentHashMap&lt;Long, String&gt;(); public static void main(String[] args) throws InterruptedException { for (long i = 0; i &lt; 5; i++) { conMap.put(i, i + &quot;&quot;); } Thread thread = new Thread(new Runnable() { public void run() { conMap.put(100l, &quot;100&quot;); System.out.println(&quot;ADD:&quot; + 100); try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } }); Thread thread2 = new Thread(new Runnable() { public void run() { for (Iterator&lt;Entry&lt;Long, String&gt;&gt; iterator = conMap.entrySet().iterator(); iterator.hasNext();) { Entry&lt;Long, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + &quot; - &quot; + entry.getValue()); try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } } }); thread.start(); thread2.start(); Thread.sleep(3000); System.out.println(&quot;--------&quot;); for (Entry&lt;Long, String&gt; entry : conMap.entrySet()) { System.out.println(entry.getKey() + &quot; &quot; + entry.getValue()); } } 输出： ADD:100 0 - 0 100 - 100 2 - 2 1 - 1 3 - 3 4 - 4 -------- 0 0 100 100 2 2 1 1 3 3 4 4 参考链接]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volatile用法]]></title>
    <url>%2F2017%2F07%2F28%2Fvolatile%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[引言volatile这个关键字可能很多朋友都听说过，或许也都用过。在Java 5之前，它是一个备受争议的关键字，因为在程序中使用它往往会导致出人意料的结果。在Java 5之后，volatile关键字才得以重获生机。 volatile关键字虽然从字面上理解起来比较简单，但是要用好不是一件容易的事情。由于volatile关键字是与Java的内存模型有关的，因此在讲述volatile关键之前，我们先来了解一下与内存模型相关的概念和知识，然后分析了volatile关键字的实现原理，最后给出了几个使用volatile关键字的场景。 在Java中除了long和double类型的基本类型变量的赋值和读取操作外都是原子操作，也就是说，对于变量值的简单读取操作没有必要进行同步。 count++不是原子操作，是3个原子操作组合1.读取主存中的count值，赋值给一个局部成员变量tmp2.tmp+13.将tmp赋值给count long和double的赋值操作Java中的原子操作包括：1）除long和double之外的基本类型的赋值和读取操作2）所有引用reference的赋值操作3）java.concurrent.Atomic.* 包中所有类的一切操作。 long和double占用的字节数都是8，也就是64bits。在32位操作系统上对64位的数据的读写要分两步完成，每一步取32位数据。这样对double和long的赋值操作就会有问题：如果有两个线程同时写一个变量内存，一个进程写低32位，而另一个写高32位，这样将导致获取的64位数据是失效的数据。因此需要使用volatile关键字来防止此类现象。volatile本身不保证获取和设置操作的原子性，仅仅保持修改的可见性。但是java的内存模型保证声明为volatile的long和double变量的get和set操作是原子的。 在当前的Java内存模型下，线程可以把变量保存在本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，只需要像在本程序中的这样，把该变量声明为volatile（不稳定的）即可，这就指示JVM，这个变量是不稳定的，每次使用它都到主存中进行读取。一般说来，多任务环境下各任务间共享的标志都应该加volatile修饰。 Volatile修饰的成员变量在每次被线程访问时，都强迫从共享内存中重读该成员变量的值。而且，当成员变量发生变化时，强迫线程将变化值回写到共享内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。 Java语言规范中指出：为了获得最佳速度，允许线程保存共享成员变量的私有拷贝，而且只当线程进入或者离开同步代码块时才与共享成员变量的原始值对比。 这样当多个线程同时与某个对象交互时，就必须要注意到要让线程及时的得到共享成员变量的变化。 而volatile关键字就是提示VM：对于这个成员变量不能保存它的私有拷贝，而应直接与共享成员变量交互。 使用建议：在两个或者更多的线程访问的成员变量上使用volatile。当要访问的变量已在synchronized代码块中，或者为常量时，不必使用。 由于使用volatile屏蔽掉了VM中必要的代码优化，所以在效率上比较低，因此一定在必要时才使用此关键字。 示例public class UnatomicLong implements Runnable { private static long test = 0; private final long val; public UnatomicLong(long val) { this.val = val; } @Override public void run() { while (!Thread.interrupted()) { test = val; //两个线程都试图将自己的私有变量val赋值给类私有静态变量test } } public static void main(String[] args) { Thread t1 = new Thread(new UnatomicLong(-1)); Thread t2 = new Thread(new UnatomicLong(0)); System.out.println(Long.toBinaryString(-1)); System.out.println(pad(Long.toBinaryString(0), 64)); t1.start(); t2.start(); long val; while ((val = test) == -1 || val == 0) { //如果静态成员test的值是-1或0，说明两个线程操作没有交叉 } System.out.println(pad(Long.toBinaryString(val), 64)); System.out.println(val); t1.interrupt(); t2.interrupt(); } // prepend 0s to the string to make it the target length private static String pad(String s, int targetLength) { int n = targetLength - s.length(); for (int x = 0; x &lt; n; x++) { s = &quot;0&quot; + s; } return s; } } 运行发现程序在while循环时进入了死循环，这是因为使用的JVM是64bits。在64位JVM中double和long的赋值操作是原子操作。在eclipse中修改jre为一个32bit的JVM地址，则会有如下运行结果：111111111111111111111111111111111111111111111111111111111111111100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011111111111111111111111111111111//很明显test的值被破坏了4294967295 内存模型大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法(硬件层面)： 1) 通过在总线加LOCK#锁的方式 2) 通过缓存一致性协议 在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 并发编程在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题。 原子性原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 一个很经典的例子就是银行账户转账问题： 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。 同样地反映到并发编程中会出现什么结果呢？ 举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？ i = 9; 假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。 可见性可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 举个简单的例子，看下面这段代码： //线程1执行的代码 int i = 0; i = 10; //线程2执行的代码 j = i; 假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。 此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10. 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。 有序性有序性：即程序执行的顺序按照代码的先后顺序执行。举个简单的例子，看下面这段代码： int i = 0; boolean flag = false; i = 1; //语句1 flag = true; //语句2 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： int a = 10; //语句1 int r = 2; //语句2 a = a + 3; //语句3 r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是：语句2 语句1 语句3 语句4 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 内存模型在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面我们来看一下Java内存模型，研究一下Java内存模型为我们提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 举个简单的例子：在java中，执行下面这个语句： i = 10; 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。 那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？ 原子性 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i： 请分析以下哪些操作是原子性操作： x = 10; //语句1，原子操作 y = x; //语句2，非原子操作 x++; //语句3，非原子操作 x = x + 1; //语句4，非原子操作 咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 所以上面4个语句只有语句1的操作具备原子性。 只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。如long和double但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。 从上面可以看出Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 可见性 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 有序性 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始这8条原则摘自《深入理解Java虚拟机》。 这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。 下面我们来解释一下前4条规则： 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果处于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。 第四条规则实际上就是体现happens-before原则具备传递性。 深入剖析在前面讲述了很多东西，其实都是为讲述volatile关键字作铺垫，那么接下来我们就进入主题。 volatile关键字的两层语义 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 先看一段代码，假如线程1先执行，线程2后执行： //线程1 boolean stop = false; while(!stop){ doSomething(); } //线程2 stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： **第一：使用volatile关键字会强制将修改的值立即写入主存； **第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。 volatile保证原子性吗？ 从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？ 下面看一个例子： public class Test { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 (但是这里有个坑，如果在单核cpu下执行这段代码，运行结果同样是每次都是10000，所以本地测试时小心哈，不过一般情况下也不会，我的开发环境在虚拟机下，所以设置的处理器数量为1，改大了后正常) 可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 把上面的代码改成以下任何一种都可以达到效果： 采用synchronized： public class Test { public int inc = 0; public synchronized void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用Lock： public class Test { public int inc = 0; Lock lock = new ReentrantLock(); public void increase() { lock.lock(); try { inc++; } finally{ lock.unlock(); } } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用AtomicInteger： public class Test { public AtomicInteger inc = new AtomicInteger(); public void increase() { inc.getAndIncrement(); } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 volatile能保证有序性吗？ 在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： //x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会将语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 那么我们回到前面举的一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么就可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。 volatile的原理和实现机制 前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。 下面这段话摘自《深入理解Java虚拟机》： “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2）它会强制将对缓存的修改操作立即写入主存； 3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 使用场景synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件： 1）对变量的写操作不依赖于当前值 2）该变量没有包含在具有其他变量的不变式中 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。 下面列举几个Java中使用volatile的几个场景。 状态标记量 volatile boolean flag = false; while(!flag){ doSomething(); } public void setFlag() { flag = true; } double check class Singleton{ private volatile static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if(instance==null) { synchronized (Singleton.class) { if(instance==null) instance = new Singleton(); } } return instance; } } 原文链接]]></content>
      <categories>
        <category>java</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java常量池]]></title>
    <url>%2F2017%2F07%2F28%2Fjava%E5%B8%B8%E9%87%8F%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[引言接上一篇文章继续探索string类牵带出的常量池 jvm程序计数器：1.在IDE上编译的Java代码运行时都会被转译成字节码。程序计数器的就是给编译好的字节码添加行号，这样这些字节码就以程序计数器的编号来作为调度时候的标识了。 2.在程序运行时，诸如循环，跳转，异常处理这些功能都必须依赖于字节码来完。 我的理解：字节码是二进制文件，所以识别起来很难，代表一个功能的字节码数量巨大。如果在编译的时候就将其在程序计数器上进行编号，则后期调用的时候就可以按照程序员在IDE上用高级语言编译时候的顺序进行分条执行了。栈：栈不灵活，但是很严格，是安全的，易于管理。因为只要上面的引用没有销毁，下面引用就一定还在，在大部分程序中，都是先定义的变量、引用先进栈，后定义的后进栈，同时，区块内部的变量、引用在进入区块时压栈，区块结束时出栈，理解了这种机制，我们就可以很方便地理解各种编程语言的作用域的概念了，同时这也是栈的优点——错误的引用逻辑在编译时就可以被发现，主要存放引用和基本数据类型。包括：&ensp;&ensp;&ensp;1.本地方法栈：是jvm调用操作系统方法所使用的栈。 &ensp;&ensp;&ensp;2.虚拟机栈：是jvm执行java代码所使用的栈。 方法区：存放了一些常量、静态变量、类信息等，可以理解成class文件在内存中的存放位置。 虚拟机堆：堆很灵活，但是不安全。对于对象，我们要动态地创建、销毁，不能说后创建的对象没有销毁，先前创建的对象就不能销毁，那样的话我们的程序就寸步难行，所以Java中用堆来存储对象。而一旦堆中的对象被销毁，我们继续引用这个对象的话，就会出现著名的 NullPointerException，这就是堆的缺点——错误的引用逻辑只有在运行时才会被发现。主要用来存放 new 出来的对象实例。 Java中的常量池：，实际上分为两种形态：静态常量池和运行时常量池。 &ensp;&ensp;&ensp;1.静态常量池：，即*.class文件中的常量池，class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。 &ensp;&ensp;&ensp;2.运行时常量池：，则是jvm虚拟机在完成类装载操作后，将class文件中的常量池载入到内存中，并保存在方法区中，我们常说的常量池，就是指方法区中的运行时常量池。 接下来我们引用一些网络上流行的常量池例子，然后借以讲解。12345678910111213141516String s1 = &quot;Hello&quot;; String s2 = &quot;Hello&quot;; String s3 = &quot;Hel&quot; + &quot;lo&quot;; String s4 = &quot;Hel&quot; + new String(&quot;lo&quot;); String s5 = new String(&quot;Hello&quot;); String s6 = s5.intern(); String s7 = &quot;H&quot;; String s8 = &quot;ello&quot;; String s9 = s7 + s8; System.out.println(s1 == s2); // true System.out.println(s1 == s3); // true System.out.println(s1 == s4); // false System.out.println(s1 == s9); // false System.out.println(s4 == s5); // false System.out.println(s1 == s6); // true 在上节中提到，在java 中，直接使用==操作符，比较的是两个字符串的引用地址，并不是比较内容，比较内容请用equals()方法。 s1 == s2这个非常好理解，s1、s2在赋值时，均使用的字符串字面量，说白话点，就是直接把字符串写死，在编译期间，这种字面量会直接放入class文件的常量池中，从而实现复用，载入运行时常量池后，s1、s2指向的是同一个内存地址，所以相等。 s1 == s3这个地方有个坑，s3虽然是动态拼接出来的字符串，但是所有参与拼接的部分都是已知的字面量，在编译期间，这种拼接会被优化，编译器直接帮你拼好，因此String s3 = “Hel” + “lo”;在class文件中被优化成String s3 = “Hello”;，所以s1 == s3成立。 s1 == s4当然不相等，s4虽然也是拼接出来的，但new String(“lo”)这部分不是已知字面量，是一个不可预料的部分，编译器不会优化，必须等到运行时才可以确定结果，结合字符串不变定理，鬼知道s4被分配到哪去了，所以地址肯定不同。配上一张简图理清思路： s1 == s9也不相等，道理差不多，虽然s7、s8在赋值的时候使用的字符串字面量，但是拼接成s9的时候，s7、s8作为两个变量，都是不可预料的，编译器毕竟是编译器，不可能当解释器用，所以不做优化，等到运行时，s7、s8拼接成的新字符串，在堆中地址不确定，不可能与方法区常量池中的s1地址相同。 s4 == s5已经不用解释了，绝对不相等，二者都在堆中，但地址不同。 s1 == s6这两个相等完全归功于intern方法，s5在堆中，内容为Hello ，intern方法会尝试将Hello字符串添加到常量池中，并返回其在常量池中的地址，因为常量池中已经有了Hello字符串，所以intern方法直接返回地址；而s1在编译期就已经指向常量池了，因此s1和s6指向同一地址，相等。 至此，我们可以得出三个非常重要的结论： &ensp;&ensp;&ensp;必须要关注编译期的行为，才能更好的理解常量池。 &ensp;&ensp;&ensp;运行时常量池中的常量，基本来源于各个class文件中的常量池。 &ensp;&ensp;&ensp;程序运行时，除非手动向常量池中添加常量(比如调用intern方法)，否则jvm不会自动添加常量到常量池。 以上所讲仅涉及字符串常量池，实际上还有整型常量池、浮点型常量池等等，但都大同小异，只不过数值类型的常量池不可以手动添加常量，程序启动时常量池中的常量就已经确定了，比如整型常量池中的常量范围：-128~127，只有这个范围的数字可以用到常量池。 说了这么多理论，接下来让我们触摸一下真正的常量池。 前文提到过，class文件中存在一个静态常量池，这个常量池是由编译器生成的，用来存储java源文件中的字面量(本文仅仅关注字面量)，假设我们有如下java代码：1String s = &quot;hi&quot;; 为了方便起见，就这么简单，没错！将代码编译成class文件后，用UE打开二进制格式的class文件。如图： 在命令行我们通过javap工具来查看一个class文件的字节码。1javap -v test 如图所示： 简单讲解一下class文件的结构，开头的4个字节是class文件魔数，用来标识这是一个class文件，说白话点就是文件头，既：CA FE BA BE。 紧接着4个字节是java的版本号，这里的版本号是34，因为笔者是用jdk8编译的，版本号的高低和jdk版本的高低相对应，高版本可以兼容低版本，但低版本无法执行高版本。所以，如果哪天读者想知道别人的class文件是用什么jdk版本编译的，就可以看这4个字节，对应关系如下： jdk1.4 对应48 （00 30）,jdk1.5 对应49,（00 31）,jdk1.6 对应50,（00 32）,jdk1.7 对应51,（00 33）,jdk1.8 对应52,（00 34）, 接下来就是常量池入口，入口处用2个字节标识常量池常量数量，本例中数值为00 13，翻译成十进制是19，也就是有18个常量，其中第0个常量是特殊值，所以只有18个常量。 常量池中存放了各种类型的常量，他们都有自己的类型，并且都有自己的存储规范，本文只关注字符串常量 01 00 02 68 69 ，其中01代表的是“utf-8编码的字符串”，00 02代表的是这个字符串的长度是2个字节，68 69这2个字节代表的就是这个字符串的内容，因为是ascii码，每个字节对应一个字符，翻译过来就是hi 接下来再说说运行时常量池，由于运行时常量池在方法区中，我们可以通过jvm参数：-XX:PermSize、-XX:MaxPermSize来设置方法区大小，从而间接限制常量池大小。 假设jvm启动参数为：-XX:PermSize＝2M -XX:MaxPermSize＝2M，然后运行如下代码：123456789//保持引用，防止自动垃圾回收List&lt;String&gt; list = new ArrayList&lt;String&gt;(); int i = 0; while(true)&#123; //通过intern方法向常量池中手动添加常量 list.add(String.valueOf(i++).intern());&#125; 程序立刻会抛出：Exception in thread “main” java.lang.outOfMemoryError: PermGen space异常。PermGen space正是方法区，足以说明常量池在方法区中。 在jdk8中，移除了方法区，转而用Metaspace区域替代，所以我们需要使用新的jvm参数：-XX:MaxMetaspaceSize=2M，依然运行如上代码，抛出：java.lang.OutOfMemoryError: Metaspace异常。同理说明运行时常量池是划分在Metaspace区域中。具体关于Metaspace区域的知识，请读者自行搜索。 本文所有代码均在jdk7、jdk8下测试通过，其他版本jdk可能会略有差异，请读者自行探索。]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>常量池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[string比较之‘’equals‘’和‘’==‘’]]></title>
    <url>%2F2017%2F07%2F28%2Fstring%E6%AF%94%E8%BE%83%E4%B9%8B%E2%80%98%E2%80%99equals%E2%80%98%E2%80%99%E5%92%8C%E2%80%98%E2%80%99%3D%3D%E2%80%98%E2%80%99%2F</url>
    <content type="text"><![CDATA[引言最近我发现了一个事情，那就是在面试笔试中，好多公司都喜欢在String字符串上出问题，涉及到方方面面的知识，包括其中的一些常用方法。在此，我总结下关于String类中的equals方法，以备应对以后的笔试面试。 equals和”==”string是一个final class，两种声明方法 :1.通过new关键字，创建一个新对象，分配一块新的、独立的内存堆String s1 = new String(“Hello”);2.直接赋值，创建一个”Hello”字符串放入字符串常量池里面,s2只是这个字符串的引用.String s2 = “Hello”; 这里s2属于字符串字面量，下一节会详细介绍 在java 中，string重写了equals和hashCode方法，都是以字符串内容复写的，直接使用”==”操作符，比较的是两个字符串的引用是否指向同一个对象，并不是比较内容，”equals”方法比较的是字符串内容，所以如果”==”返回true则”equals”一定为true,反之则不然。下面来看具体例子：1234567891011121314151617181920212223String s1 = new String(&quot;Hello&quot;);String s2 = new String(&quot;Hello&quot;);System.out.println(s1 == s2);// falseSystem.out.println(s1.equals(s2)); //trueString s3 = new String(&quot;Hello&quot;);String s4 = s3;System.out.println(s3 == s4);// trueSystem.out.println(s3.equals(s4));// trueString s5 = &quot;Hello&quot;;String s6 = &quot;Hello&quot;;System.out.println(s5 == s6);// trueSystem.out.println(s5.equals(s6));// trueString s7 = &quot;Hello&quot;;String s8 = new String(&quot;Hello&quot;);System.out.println(s7 == s8);// false，System.out.println(s7.equals(s8));// trueString s9 = s3.intern();System.out.println(s7 == s9);// true，System.out.println(s7.equals(s9));// true intern方法会尝试将Hello字符串添加到常量池中，并返回其在常量池中的地址，因为常量池中已经有了Hello字符串，所以intern方法直接返回地址；而s7在编译期就已经指向常量池了，因此s7和s9指向同一地址，相等。 扩展假设有一个类，它有一个记录消息的方法，这个方法记录用户传来的消息(假设消息内容可能较大，并且重复率较高)，并且把消息按接收顺序记录在一个列表中。我想有些朋友会这样设计：1234567891011121314import java.util.*;public class Messages &#123;ArrayList messages = new ArrayList();public void record(String msg) &#123;messages.add(msg);&#125;public List getMessages() &#123;return messages;&#125;&#125; 这种设计方案好吗？假设我们重复的发送给record()方法同一个消息(消息来自不同的用户，所以可以视每个消息为一个new String(“…”))，并且消息内容较大，那么这个设计将会大大浪费内存空间，因为消息列表中记录的都是新创建的、独立的String对象，虽然它们的内容都相同。那么怎么样可以对其进行优化呢，其实很简单，请看如下优化后的示例：1234567891011121314import java.util.*;public class Messages &#123;ArrayList messages = new ArrayList();public void record(String msg) &#123;messages.add(msg.intern());&#125;public List getMessages() &#123;return messages;&#125;&#125; 正如你所看到的，原先record()方法中的messages.add(msg);代码段变成了messages.add(msg.intern());，仅仅对msg参数调用了intern()方法，这样将对重复的消息进行共享机制，从而降低了内存消耗，提高了性能。 自己写一个类MyString，里边有一个char[ ] value，实现里边的equalsString方法，要求可以比较两个MyString类的对象。相等返回0，大于返回1，小于返回-1，若比较的不是MyString类型的对象，则返回-100。 代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package cn.ywq.test; class MyString &#123; char[] value; public MyString(char[] value) &#123; this.value=value; //通过构造方法将字符传入 &#125; public int equalsString(Object obj) &#123; if(this==obj)&#123; return 0; &#125; //若该对象是MyString类型的 if(obj instanceof MyString)&#123; MyString string =(MyString) obj; int n=this.value.length; if (n&gt;string.value.length) &#123; //先判断长度的关系 return 1; &#125;else if(n&lt;string.value.length)&#123; return -1; &#125;else&#123; //若长度相等 char v1[] = this.value; char v2[] = string.value; int i = 0; while (n-- != 0) &#123; //按照数组的每一位进行比较 if (v1[i] &gt; v2[i])&#123; return 1; &#125;else if(v1[i] &lt; v2[i])&#123; return -1; &#125; i++; &#125; return 0; //若while循环正常结束，则说明相等，返回0 &#125; &#125; return -100; //若传入的不是MyString类型的对象 &#125; &#125; #测试代码：package cn.ywq.test; public class Test &#123; public static void main(String[] args) &#123; char[] value=&#123;&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;&#125;; char[] value2=&#123;&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;&#125;; char[] value3=&#123;&apos;c&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;&#125;; char[] value4=&#123;&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&#125;; MyString myString = new MyString(value); MyString s=new MyString(value4); int i = myString.equalsString(s); System.out.println(i); &#125; &#125;]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>string</tag>
        <tag>==</tag>
        <tag>equals</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring Bean的作用域]]></title>
    <url>%2F2017%2F07%2F26%2Fspring%20Bean%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[引言在面向对象程序设计中作用域一般指对象或变量之间的可见范围。而在Spring容器中是指其创建的Bean对象相对于其他Bean对象的请求可见范围。 IoC容器最顶级的接口BeanFactory中有两个方法isSingleton（单例）和isPrototype）（原型） isSingeleton：返回true，Bean在Ioc容器中以单例存在（Spring默认）isPrototype：返回true，每次获取Bean，Ioc容器都会创建一个新的 Bean。 而在JAVA EE 的WEB容器中，还存在page（页面），request（请求），session（会话）和application（应用）4种作用域。其中page是针对JSP当前页面的作用域，Spring无法支持。 作用域关系如下表 作用域类型 使用范围 作用域描述 singleton 所有Spring应用 默认值，Ioc容器只存在一个Bean实例,Bean以单例方式存在 prototype 所有Spring应用 每当从Ioc容器中调用Bean时，则创建一个新的bean返回 session Spring Web应用 同一个Http Session会话共享一个Bean，不同session使用不同的bean application Spring Web应用 Web工程生命周期 request Spring Web应用 每次Http请求都会创建一个新的Bean globalSession Spring Web应用 在一个全局的HTTP session中，一个bean定义对应一个实例,一般用于Porlet应用环境，实践中基本不使用 singleton： @Scope(ConfigurableListableBeanFactory.SCOPE_SINGLETON) 当Bean的作用域为singleton的时候,Spring容器中只会存在一个共享的Bean实例，所有对Bean的请求只要id与bean的定义相匹配，则只会返回bean的同一实例。单一实例会被存储在单例缓存中，为Spring的缺省作用域。 prototype： @Scope(ConfigurableListableBeanFactory.SCOPE_PROTOTYPE) 每次对该Bean请求的时候，Spring IoC都会创建一个新的作用域。对于有状态的Bean应该使用prototype，对于无状态的Bean则使用singleton session： @Scope(value = WebApplicationContext.SCOPE_SESSION, proxyMode = ScopedProxyMode.INTERFACES) 针对http session起作用，Spring容器会根据该Bean的定义来创建一个全新的Bean的实例。而且该Bean只在当前http session内是有效的。 request： @Scope(value = WebApplicationContext.SCOPE_REQUEST, proxyMode = ScopedProxyMode.INTERFACES) Request作用域针对的是每次的Http请求，Spring容器会根据相关的Bean的 定义来创建一个全新的Bean实例。而且该Bean只在当前request内是有效的。 global session： @Scope(value = WebApplicationContext.SCOPE_GLOBAL_SESSION, proxyMode = ScopedProxyMode.INTERFACES) 类似标准的http session作用域，不过仅仅在基于portlet的web应用当中才有意义。Portlet规范定义了全局的Session的概念。他被所有构成某个portlet外部应用中的各种不同的portlet所共享。在global session作用域中所定义的bean被限定于全局的portlet session的生命周期范围之内。 singleton作用域 当一个bean的作用域设置为singleton, 那么Spring IOC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例。换言之，当把一个bean定义设置为singleton作用域时，Spring IOC容器只会创建该bean定义的唯一实例。这个单一实例会被存储到单例缓存（singleton cache）中，并且所有针对该bean的后续请求和引用都将返回被缓存的对象实例。 这里要注意的是singleton作用域和GOF设计模式中的单例是完全不同的，单例设计模式表示一个ClassLoader中只有一个class存在，而这里的singleton则表示一个容器对应一个bean，也就是说当一个bean被标识为singleton时候，spring的IOC容器中只会存在一个该bean。 prototype prototype作用域部署的bean，每一次请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean()方法）都会产生一个新的bean实例，相当与一个new的操作，对于prototype作用域的bean，有一点非常重要，那就是Spring不能对一个prototype bean的整个生命周期负责，容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法，而对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责。（让Spring容器释放被singleton作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用。） scope=”prototype”没写的问题,项目中对一个表的增删该操作是用一个action，这个actionadd,update,delete,save这些方法， 添加和修改是共用一个页面，当页面得到id时代表进行的修改操作，反之是添加操作。因为在配置spring的bean是忘了写scope=”prototype” 所以每次添加时都显示最后一次访问过的记录,scope=”prototype” 会在该类型的对象被请求 时创建一个新的action对象。如果没有配置scope=prototype则添加的时候不会新建一个action，他任然会保留上次访问的过记录的信息 webwork的Action不是线程安全的，要求在多线程环境下必须是一个线程对应一个独立的实例，不能使用singleton。所以，我们在Spring配置Webwork Action Bean时，需要加上属性scope=”prototype”或singleton=”false”。 singleton模式指的是对某个对象的完全共享，包括代码空间和数据空间，说白了，如果一个类是singleton的，假如这个类有成员变量，那么这个成员变量的值是各个线程共享的（有点类似于static的样子了），当线程A往给变量赋了一个值以后，线程B就能读出这个值。 因此，对于前台Action，肯定不能使用singleton的模式，必须是一个线程请求对应一个独立的实例。推而广之，只要是带数据成员变量的类，为了防止多个线程混用数据，就不能使用singleton。对于我们用到的Service、Dao，之所以用了singleton，就是因为他们没有用到数据成员变量，如果谁的Service需要数据成员变量，请设置singleton=false。 有状态的bean都使用Prototype作用域，而对无状态的bean则应该使用singleton作用域。 单例模式 单例模式的意思就是只有一个实例。单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例。这个类称为单例类。当多用户同时请求一个服务时，容器会给每一个请求分配一个线程，这时多个线程会并发执行该请求多对应的业务逻辑（成员方法），此时就要注意了，如果该处理逻辑中有对该单列状态的修改（体现为该单列的成员属性），则必须考虑线程同步问题 同步机制的比较:ThreadLocal和线程同步机制相比有什么优势呢？ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 由于ThreadLocal中可以持有任何类型的对象，低版本JDK所提供的get()返回的是Object对象，需要强制类型转换。但JDK 5.0通过泛型很好的解决了这个问题，在一定程度地简化ThreadLocal的使用 概括起来说，对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 线程安全 Spring使用ThreadLocal解决线程安全问题 我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全状态采用ThreadLocal进行处理，让它们也成为线程安全的状态，所以有状态的Bean就可以在多线程中共享了。 首先对于Spring的IOC来说，对象是由Spring来帮我们管理，也就是在Spring启动的时候，在Spring容器中，由Spring给我们创建的，Spring会帮我们维护，一般都是单例的，也就是一个对象。 spring生成对象默认是单例的。通过scope属性可以更改为多例。 Spring生成对象默认是单例的下面我们来一个网上的例子验证一下：1234567891011121314151617181920212223242526272829&lt;bean id=&quot;singleton&quot; class=&quot;java.util.Date&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;prototype&quot; class=&quot;java.util.Date&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; package test; import java.util.Date; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import com.opensymphony.xwork2.ActionContext; public class TestScope &#123; public static void main(String[] args) &#123; ApplicationContext context=new ClassPathXmlApplicationContext(&quot;applicationContext-web.xml&quot;); Date s1=(Date)context.getBean(&quot;singleton&quot;); Date p1=(Date)context.getBean(&quot;prototype&quot;); Date s2=(Date)context.getBean(&quot;singleton&quot;); Date p2=(Date)context.getBean(&quot;prototype&quot;); System.out.println(&quot;单例：&quot;+(s1==s2)); System.out.println(&quot;非单例：&quot;+(p1==p2)); &#125; &#125; 输出结果:单例：true非单例：false 注：复合数据类型的==对比的是内存中存放的地址，所以此处我们采用==去对比因为object中的equals初始行为是比较内存中的地址，但在一些类库中被覆盖掉了如（String，Integer，Date等） SpringMVC和Struts2中并发访问对于使用过SpringMVC和Struts2的人来说，大家都知道SpringMVC是基于方法的拦截，而Struts2是基于类的拦截。 对于Struts2来说，因为每次处理一个请求，struts就会实例化一个对象；这样就不会有线程安全的问题了; 而Spring的controller默认是Singleton的，这意味着每一个request过来，系统都会用原有的instance去处理，这样导致两个结果：一是我们不用每次创建Controller，二是减少了对象创建和垃圾收集的时间;由于只有一个Controller的instance，当多个线程调用它的时候，它里面的instance变量就不是线程安全的了，会发生窜数据的问题。 当然大多数情况下，我们根本不需要考虑线程安全的问题，比如dao,service等，除非在bean中声明了实例变量。因此，我们在使用spring mvc 的contrller时，应避免在controller中定义实例变量。如：123456public class Controller extends AbstractCommandController &#123; protected Company company; //controller默认单列，定义实例变量会有线程安全问题 protected ModelAndView handle(HttpServletRequest request,HttpServletResponse response,Object command,BindException errors) throws Exception &#123; company = ................; &#125; &#125; 解决方案：有几种解决方法：1、在Controller中使用ThreadLocal变量2、在spring配置文件Controller中声明 scope=”prototype”，每次都创建新的controller所以在使用spring开发web时要注意，默认Controller、Dao、Service都是单例的。 例如：12345678910@Controller @RequestMapping(&quot;/fui&quot;) public class FuiController extends SpringController &#123; //这么定义的话就是单例 @Controller @Scope(&quot;prototype&quot;) @RequestMapping(&quot;/fui&quot;) public class FuiController extends SpringController &#123; //每次都创建 springMVC与struts2的区别 机制：spring mvc的入口是servlet，而struts2是filter，这样就导致了二者的机制不同。 性能：spring会稍微比struts快。spring mvc是基于方法的设计，而sturts是基于类，每次发一次请求都会实例一个action，每个action都会被注入属性，而spring基于方法，粒度更细，但要小心把握像在servlet控制数据一样。spring3 mvc是方法级别的拦截，拦截到方法后根据参数上的注解，把request数据注入进去，在spring3 mvc中，一个方法对应一个request上下文。而struts2框架是类级别的拦截，每次来了请求就创建一个Action，然后调用setter、getter方法把request中的数据注入；struts2实际上是通过setter、getter方法与request打交道的；struts2中，一个Action对象对应一个request上下文。 参数传递：struts是在接受参数的时候，可以用属性来接受参数，这就说明参数是让多个方法共享的。 设计思想上：struts更加符合oop的编程思想， spring就比较谨慎，在servlet上扩展。 intercepter的实现机制：struts有以自己的interceptor机制，spring mvc用的是独立的AOP方式。这样导致struts的配置文件量还是比spring mvc大，虽然struts的配置能继承，所以我觉得论使用上来讲，spring mvc使用更加简洁，开发效率Spring MVC确实比struts2高。spring mvc是方法级别的拦截，一个方法对应一个request上下文，而方法同时又跟一个url对应，所以说从架构本身上spring3 mvc就容易实现restful url。 struts2是类级别的拦截，一个类对应一个request上下文；实现restful url要费劲，因为struts2 action的一个方法可以对应一个url；而其类属性却被所有方法共享，这也就无法用注解或其他方式标识其所属方法了。spring3 mvc的方法之间基本上独立的，独享request response数据，请求数据通过参数获取，处理结果通过ModelMap交回给框架方法之间不共享变量，而struts2搞的就比较乱，虽然方法之间也是独立的，但其所有Action变量是共享的，这不会影响程序运行，却给我们编码，读程序时带来麻烦。 另外，spring3 mvc的验证也是一个亮点，支持JSR303，处理ajax的请求更是方便，只需一个注解@ResponseBody ，然后直接返回响应文本即可。 这样也说明了SpringMVC还有Servlet都是方法的线程安全，所以在类方法声明的私有或者公有变量不是线程安全的，而struts2的确实是线程安全的。 Struts2+Spring来管理注入的时候呢？Struts2是多实例的，对于每个请求都会生成1个实例，spring默认是单实例的对于无Spring插件Struts2-spring-plugin-XXX.jar的整合方式，需要在spring的action Bean中加业务逻辑控制器类配scope=”prototype”。1&lt;bean id=&quot;user&quot; class=&quot;modle.User&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; 对于有Spring插件Struts2-spring-plugin-XXX.jar的整合方式：反编译StrutsSpringObjectFactory以及相关的代码才发现，如果在struts action的配置文件中&lt;action name=&quot;..&quot; class=&quot;..&quot;/&gt;中class如果写的是完整的包名和类名的话就是struts创建action对象，也就是多实例的； 总结：如果是spring配置文件中的bean的名字的话就是spring创建，那么单实例还是多实例就由spring的action Bean中的业务逻辑控制器类是否配置为scope=”prototype”，有就是多实例的，没有就是单实例的，顺序是先从spring中找，找不到再从struts配置文件中找。 ThreadLocal是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。 或者说:一个类或者程序所提供的接口对于线程来说是原子操作或者多个线程之间的切换不会导致该接口的执行结果存在二义性,也就是说我们不用考虑同步的问题。 线程安全问题都是由全局变量及静态变量引起的。 若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则就可能影响线程安全。 1）常量始终是线程安全的，因为只存在读操作。2）每次调用方法前都新建一个实例是线程安全的，因为不会访问共享的资源。3）局部变量是线程安全的。因为每执行一个方法，都会在独立的空间创建局部变量，它不是共享的资源。局部变量包括方法的参数变量和方法内变量。 有状态就是有数据存储功能。有状态对象(Stateful Bean)，就是有实例变量的对象，可以保存数据，是非线程安全的。在不同方法调用间不保留任何状态。 有状态的Bean，多线程环境下不安全，那么适合用Prototype原型模式。Prototype: 每次对bean的请求都会创建一个新的bean实例。 无状态就是一次操作，不能保存数据。无状态对象(Stateless Bean)，就是没有实例变量的对象,不能保存数据，是不变类，是线程安全的。 无状态的Bean适合用不变模式，技术就是单例模式，这样可以共享实例，提高性能。 Struts2默认的实现是Prototype模式。也就是每个请求都新生成一个Action实例，所以不存在线程安全问题。需要注意的是，如果由Spring管理action的生命周期， scope要配成prototype作用域。 线程安全案例SimpleDateFormat(下面简称sdf)类内部有一个Calendar对象引用,它用来储存和这个sdf相关的日期信息,例如sdf.parse(dateStr), sdf.format(date) 诸如此类的方法参数传入的日期相关String, Date等等, 都是交友Calendar引用来储存的.这样就会导致一个问题,如果你的sdf是个static的, 那么多个thread 之间就会共享这个sdf, 同时也是共享这个Calendar引用, 并且, 观察 sdf.parse() 方法,你会发现有如下的调用: Date parse() { calendar.clear(); // 清理calendar ... // 执行一些操作, 设置 calendar 的日期什么的 calendar.getTime(); // 获取calendar的时间 } 这里会导致的问题就是, 如果 线程A 调用了 sdf.parse(), 并且进行了 calendar.clear()后还未执行calendar.getTime()的时候,线程B又调用了sdf.parse(), 这时候线程B也执行了sdf.clear()方法, 这样就导致线程A的的calendar数据被清空了(实际上A,B的同时被清空了). 又或者当 A 执行了calendar.clear() 后被挂起, 这时候B 开始调用sdf.parse()并顺利i结束, 这样 A 的 calendar内存储的的date 变成了后来B设置的calendar的date这个问题背后隐藏着一个更为重要的问题—无状态：无状态方法的好处之一，就是它在各种环境下，都可以安全的调用。衡量一个方法是否是有状态的，就看它是否改动了其它的东西，比如全局变量，比如实例的字段。format方法在运行过程中改动了SimpleDateFormat的calendar字段，所以，它是有状态的。 这也同时提醒我们在开发和设计系统的时候注意下一下三点: 1.自己写公用类的时候，要对多线程调用情况下的后果在注释里进行明确说明2.对线程环境下，对每一个共享的可变变量都要注意其线程安全性3.我们的类和方法在做设计的时候，要尽量设计成无状态的 解决办法 需要的时候创建新实例： 说明：在需要用到SimpleDateFormat 的地方新建一个实例，不管什么时候，将有线程安全问题的对象由共享变为局部私有都能避免多线程问题，不过也加重了创建对象的负担。在一般情况下，这样其实对性能影响比不是很明显的。 使用同步：同步SimpleDateFormat对象 public class DateSyncUtil { private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static String formatDate(Date date)throws ParseException{ synchronized(sdf){ return sdf.format(date); } } public static Date parse(String strDate) throws ParseException{ synchronized(sdf){ return sdf.parse(strDate); } } } 说明：当线程较多时，当一个线程调用该方法时，其他想要调用此方法的线程就要block，多线程并发量大的时候会对性能有一定的影响。 使用ThreadLocal： public class ConcurrentDateUtil { private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); } }; public static Date parse(String dateStr) throws ParseException { return threadLocal.get().parse(dateStr); } public static String format(Date date) { return threadLocal.get().format(date); } } 或 public class ThreadLocalDateUtil { private static final String date_format = &quot;yyyy-MM-dd HH:mm:ss&quot;; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;(); public static DateFormat getDateFormat() { DateFormat df = threadLocal.get(); if(df==null){ df = new SimpleDateFormat(date_format); threadLocal.set(df); } return df; } public static String formatDate(Date date) throws ParseException { return getDateFormat().format(date); } public static Date parse(String strDate) throws ParseException { return getDateFormat().parse(strDate); } } 说明：使用ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 抛弃JDK，使用其他类库中的时间格式化类： 1.使用Apache commons 里的FastDateFormat，宣称是既快又线程安全的SimpleDateFormat, 可惜它只能对日期进行format, 不能对日期串进行解析。 2.使用Joda-Time类库来处理时间相关问题 做一个简单的压力测试，方法一最慢，方法三最快，但是就算是最慢的方法一性能也不差，一般系统方法一和方法二就可以满足，所以说在这个点很难成为你系统的瓶颈所在。从简单的角度来说，建议使用方法一或者方法。 如果在必要的时候，追求那么一点性能提升的话，可以考虑用方法三，用ThreadLocal做缓存。Joda-Time类库对时间处理方式比较完美，建议使用。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Bean的作用域</tag>
        <tag>spring线程安全</tag>
        <tag>spring单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring单例模式与线程安全]]></title>
    <url>%2F2017%2F07%2F26%2FSpring%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[引言Spring框架里的bean，或者说组件，获取实例的时候都是默认的单例模式，这是在多线程开发的时候要尤其注意的地方。 单例模式单例模式的意思就是只有一个实例。单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例。这个类称为单例类。当多用户同时请求一个服务时，容器会给每一个请求分配一个线程，这是多个线程会并发执行该请求多对应的业务逻辑（成员方法），此时就要注意了，如果该处理逻辑中有对该单列状态的修改（体现为该单列的成员属性），则必须考虑线程同步问题同步机制的比较 ThreadLocal和线程同步机制相比有什么优势呢？ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 由于ThreadLocal中可以持有任何类型的对象，低版本JDK所提供的get()返回的是Object对象，需要强制类型转换。但JDK 5.0通过泛型很好的解决了这个问题，在一定程度地简化ThreadLocal的使用 概括起来说，对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 线程安全 Spring使用ThreadLocal解决线程安全问题 我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全状态采用ThreadLocal进行处理，让它们也成为线程安全的状态，因为有状态的Bean就可以在多线程中共享了。 一般的Web应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程ThreadLocal是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。 或者说:一个类或者程序所提供的接口对于线程来说是原子操作或者多个线程之间的切换不会导致该接口的执行结果存在二义性,也就是说我们不用考虑同步的问题。 线程安全问题都是由全局变量及静态变量引起的。若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则就可能影响线程安全。1） 常量始终是线程安全的，因为只存在读操作。2）每次调用方法前都新建一个实例是线程安全的，因为不会访问共享的资源。3）局部变量是线程安全的。因为每执行一个方法，都会在独立的空间创建局部变量，它不是共享的资源。局部变量包括方法的参数变量和方法内变量。有状态就是有数据存储功能。有状态对象(Stateful Bean)，就是有实例变量的对象 ，可以保存数据，是非线程安全的。在不同方法调用间不保留任何状态。无状态就是一次操作，不能保存数据。无状态对象(Stateless Bean)，就是没有实例变量的对象 .不能保存数据，是不变类，是线程安全的。有状态对象:无状态的Bean适合用不变模式，技术就是单例模式，这样可以共享实例，提高性能。有状态的Bean，多线程环境下不安全，那么适合用Prototype原型模式。Prototype: 每次对bean的请求都会创建一个新的bean实例。Struts2默认的实现是Prototype模式。也就是每个请求都新生成一个Action实例，所以不存在线程安全问题。需要注意的是，如果由Spring管理action的生命周期， scope要配成prototype作用域。 线程安全案例SimpleDateFormat(下面简称sdf)类内部有一个Calendar对象引用,它用来储存和这个sdf相关的日期信息,例如sdf.parse(dateStr), sdf.format(date) 诸如此类的方法参数传入的日期相关String, Date等等, 都是交友Calendar引用来储存的.这样就会导致一个问题,如果你的sdf是个static的, 那么多个thread 之间就会共享这个sdf, 同时也是共享这个Calendar引用, 并且, 观察 sdf.parse() 方法,你会发现有如下的调用: Date parse() { calendar.clear(); // 清理calendar ... // 执行一些操作, 设置 calendar 的日期什么的 calendar.getTime(); // 获取calendar的时间 } 这里会导致的问题就是, 如果 线程A 调用了 sdf.parse(), 并且进行了 calendar.clear()后还未执行calendar.getTime()的时候,线程B又调用了sdf.parse(), 这时候线程B也执行了sdf.clear()方法, 这样就导致线程A的的calendar数据被清空了(实际上A,B的同时被清空了). 又或者当 A 执行了calendar.clear() 后被挂起, 这时候B 开始调用sdf.parse()并顺利i结束, 这样 A 的 calendar内存储的的date 变成了后来B设置的calendar的date这个问题背后隐藏着一个更为重要的问题—无状态：无状态方法的好处之一，就是它在各种环境下，都可以安全的调用。衡量一个方法是否是有状态的，就看它是否改动了其它的东西，比如全局变量，比如实例的字段。format方法在运行过程中改动了SimpleDateFormat的calendar字段，所以，它是有状态的。 这也同时提醒我们在开发和设计系统的时候注意下一下三点: 1.自己写公用类的时候，要对多线程调用情况下的后果在注释里进行明确说明2.对线程环境下，对每一个共享的可变变量都要注意其线程安全性3.我们的类和方法在做设计的时候，要尽量设计成无状态的 解决办法 需要的时候创建新实例： 说明：在需要用到SimpleDateFormat 的地方新建一个实例，不管什么时候，将有线程安全问题的对象由共享变为局部私有都能避免多线程问题，不过也加重了创建对象的负担。在一般情况下，这样其实对性能影响比不是很明显的。 使用同步：同步SimpleDateFormat对象 public class DateSyncUtil { private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static String formatDate(Date date)throws ParseException{ synchronized(sdf){ return sdf.format(date); } } public static Date parse(String strDate) throws ParseException{ synchronized(sdf){ return sdf.parse(strDate); } } } 说明：当线程较多时，当一个线程调用该方法时，其他想要调用此方法的线程就要block，多线程并发量大的时候会对性能有一定的影响。 使用ThreadLocal： public class ConcurrentDateUtil { private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); } }; public static Date parse(String dateStr) throws ParseException { return threadLocal.get().parse(dateStr); } public static String format(Date date) { return threadLocal.get().format(date); } } 或 public class ThreadLocalDateUtil { private static final String date_format = &quot;yyyy-MM-dd HH:mm:ss&quot;; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;(); public static DateFormat getDateFormat() { DateFormat df = threadLocal.get(); if(df==null){ df = new SimpleDateFormat(date_format); threadLocal.set(df); } return df; } public static String formatDate(Date date) throws ParseException { return getDateFormat().format(date); } public static Date parse(String strDate) throws ParseException { return getDateFormat().parse(strDate); } } 说明：使用ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 抛弃JDK，使用其他类库中的时间格式化类： 1.使用Apache commons 里的FastDateFormat，宣称是既快又线程安全的SimpleDateFormat, 可惜它只能对日期进行format, 不能对日期串进行解析。 2.使用Joda-Time类库来处理时间相关问题 做一个简单的压力测试，方法一最慢，方法三最快，但是就算是最慢的方法一性能也不差，一般系统方法一和方法二就可以满足，所以说在这个点很难成为你系统的瓶颈所在。从简单的角度来说，建议使用方法一或者方法。 如果在必要的时候，追求那么一点性能提升的话，可以考虑用方法三，用ThreadLocal做缓存。Joda-Time类库对时间处理方式比较完美，建议使用。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring scope prototype与singleton]]></title>
    <url>%2F2017%2F07%2F26%2Fspring-scope-prototype%E4%B8%8Esingleton%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[singleton作用域 当一个bean的作用域设置为singleton, 那么Spring IOC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例。换言之，当把一个bean定义设置为singleton作用域时，Spring IOC容器只会创建该bean定义的唯一实例。这个单一实例会被存储到单例缓存（singleton cache）中，并且所有针对该bean的后续请求和引用都将返回被缓存的对象实例。 这里要注意的是singleton作用域和GOF设计模式中的单例是完全不同的，单例设计模式表示一个ClassLoader中只有一个class存在，而这里的singleton则表示一个容器对应一个bean，也就是说当一个bean被标识为singleton时候，spring的IOC容器中只会存在一个该bean。 prototype prototype作用域部署的bean，每一次请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean()方法）都会产生一个新的bean实例，相当与一个new的操作，对于prototype作用域的bean，有一点非常重要，那就是Spring不能对一个prototype bean的整个生命周期负责，容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法，而对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责。（让Spring容器释放被singleton作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用。） scope=”prototype”没写的问题,项目中对一个表的增删该操作是用一个action，这个actionadd,update,delete,save这些方法， 添加和修改是共用一个页面，当页面得到id时代表进行的修改操作，反之是添加操作。因为在配置spring的bean是忘了写scope=”prototype” 所以每次添加时都显示最后一次访问过的记录,scope=”prototype” 会在该类型的对象被请求 时创建一个新的action对象。如果没有配置scope=prototype则添加的时候不会新建一个action，他任然会保留上次访问的过记录的信息 webwork的Action不是线程安全的，要求在多线程环境下必须是一个线程对应一个独立的实例，不能使用singleton。所以，我们在Spring配置Webwork Action Bean时，需要加上属性scope=”prototype”或singleton=”false”。 singleton模式指的是对某个对象的完全共享，包括代码空间和数据空间，说白了，如果一个类是singleton的，假如这个类有成员变量，那么这个成员变量的值是各个线程共享的（有点类似于static的样子了），当线程A往给变量赋了一个值以后，线程B就能读出这个值。 因此，对于前台Action，肯定不能使用singleton的模式，必须是一个线程请求对应一个独立的实例。推而广之，只要是带数据成员变量的类，为了防止多个线程混用数据，就不能使用singleton。对于我们用到的Service、Dao，之所以用了singleton，就是因为他们没有用到数据成员变量，如果谁的Service需要数据成员变量，请设置singleton=false。 有状态的bean都使用Prototype作用域，而对无状态的bean则应该使用singleton作用域。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>scope</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm内存模型]]></title>
    <url>%2F2017%2F07%2F20%2Fjvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[引言JVM定义了若干个程序执行期间使用的数据区域。这个区域里的一些数据在JVM启动的时候创建，在JVM退出的时候销毁。而其他的数据依赖于每一个线程，在线程创建时创建，在线程退出时销毁。 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。如果线程正在执行的是一个Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 虚拟机栈线程私有，它的生命周期与线程相同。虚拟机栈描述的是Java 方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。动画是由一帧一帧图片连续切换结果的结果而产生的，其实虚拟机的运行和动画也类似，每个在虚拟机中运行的程序也是由许多的帧的切换产生的结果，只是这些帧里面存放的是方法的局部变量，操作数栈，动态链接，方法返回地址和一些额外的附加信息组成。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 对于执行引擎来说，活动线程中，只有栈顶的栈帧是有效的，称为当前栈帧，这个栈帧所关联的方法称为当前方法。执行引擎所运行的所有字节码指令都只针对当前栈帧进行操作。局部变量表局部变量表是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。在Java程序被编译成Class文件时，就在方法的Code属性的max_locals数据项中确定了该方法所需要分配的最大局部变量表的容量。局部变量表的容量以变量槽（Slot）为最小单位，32位虚拟机中一个Slot可以存放一个32位以内的数据类型（boolean、byte、char、short、int、float、reference和returnAddress八种）。reference类型虚拟机规范没有明确说明它的长度，但一般来说，虚拟机实现至少都应当能从此引用中直接或者间接地查找到对象在Java堆中的起始地址索引和方法区中的对象类型数据。returnAddress类型是为字节码指令jsr、jsr_w和ret服务的，它指向了一条字节码指令的地址。虚拟机是使用局部变量表完成参数值到参数变量列表的传递过程的，如果是实例方法（非static），那么局部变量表的第0位索引的Slot默认是用于传递方法所属对象实例的引用，在方法中通过this访问。 Slot是可以重用的，当Slot中的变量超出了作用域，那么下一次分配Slot的时候，将会覆盖原来的数据。Slot对对象的引用会影响GC（要是被引用，将不会被回收）。 系统不会为局部变量赋予初始值（实例变量和类变量都会被赋予初始值）。也就是说不存在类变量那样的准备阶段。 操作数栈和局部变量区一样，操作数栈也是被组织成一个以字长为单位的数组。但是和前者不同的是，它不是通过索引来访问，而是通过标准的栈操作——压栈和出栈—来访问的。比如，如果某个指令把一个值压入到操作数栈中，稍后另一个指令就可以弹出这个值来使用。虚拟机在操作数栈中存储数据的方式和在局部变量区中是一样的：如int、long、float、double、reference和returnType的存储。对于byte、short以及char类型的值在压入到操作数栈之前，也会被转换为int。虚拟机把操作数栈作为它的工作区——大多数指令都要从这里弹出数据，执行运算，然后把结果压回操作数栈。比如，iadd指令就要从操作数栈中弹出两个整数，执行加法运算，其结果又压回到操作数栈中，看看下面的示例，它演示了虚拟机是如何把两个int类型的局部变量相加，再把结果保存到第三个局部变量的：123456begin iload_0 // push the int in local variable 0 ontothe stack iload_1 //push the int in local variable 1 onto the stack iadd // pop two ints, add them, push result istore_2 // pop int, store into local variable 2 end 在这个字节码序列里，前两个指令iload_0和iload_1将存储在局部变量中索引为0和1的整数压入操作数栈中，其后iadd指令从操作数栈中弹出那两个整数相加，再将结果压入操作数栈。第四条指令istore_2则从操作数栈中弹出结果，并把它存储到局部变量区索引为2的位置。下图详细表述了这个过程中局部变量和操作数栈的状态变化，图中没有使用的局部变量区和操作数栈区域以空白表示。 动态连接虚拟机运行的时候,运行时常量池会保存大量的符号引用，这些符号引用可以看成是每个方法的间接引用。如果代表栈帧A的方法想调用代表栈帧B的方法，那么这个虚拟机的方法调用指令就会以B方法的符号引用作为参数，但是因为符号引用并不是直接指向代表B方法的内存位置，所以在调用之前还必须要将符号引用转换为直接引用，然后通过直接引用才可以访问到真正的方法。如果符号引用是在类加载阶段或者第一次使用的时候转化为直接应用，那么这种转换成为静态解析，如果是在运行期间转换为直接引用，那么这种转换就成为动态连接。 返回地址方法的返回分为两种情况，一种是正常退出，退出后会根据方法的定义来决定是否要传返回值给上层的调用者，一种是异常导致的方法结束，这种情况是不会传返回值给上层的调用方法。不过无论是那种方式的方法结束，在退出当前方法时都会跳转到当前方法被调用的位置，如果方法是正常退出的，则调用者的PC计数器的值就可以作为返回地址,，果是因为异常退出的，则是需要通过异常处理表来确定。方法的的一次调用就对应着栈帧在虚拟机栈中的一次入栈出栈操作，因此方法退出时可能做的事情包括：恢复上层方法的局部变量表以及操作数栈，如果有返回值的话，就把返回值压入到调用者栈帧的操作数栈中，还会把PC计数器的值调整为方法调用入口的下一条指令。异常在Java 虚拟机规范中，对虚拟机栈规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError 异常；如果虚拟机栈可以动态扩展（当前大部分的Java 虚拟机都可动态扩展，只不过Java 虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError 异常。 本地方法栈本地方法栈（Native MethodStacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native 方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 堆堆是Java 虚拟机所管理的内存中最大的一块。Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。但是随着JIT 编译器的发展与逃逸分析技术的逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC 堆”。堆的大小可以通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，默认为操作系统物理内存的1/64但小于1G，-Xmx为JVM可申请的最大内存，默认为物理内存的1/4但小于1G，默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过-XX:MinHeapFreeRation=来指定这个比列；当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过XX:MaxHeapFreeRation=来指定这个比列，对于运行系统，为避免在运行时频繁调整Heap的大小，通常-Xms与-Xmx的值设成一样。 如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java 堆中还可以细分为：新生代和老年代；新生代：程序新创建的对象都是从新生代分配内存，新生代由Eden Space和两块相同大小的Survivor Space(通常又称S0和S1或From和To)构成，可通过-Xmn参数来指定新生代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及SurvivorSpace的大小。老年代：用于存放经过多次新生代GC仍然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：1、大对象，可通过启动参数设置-XX:PretenureSizeThreshold=1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。2、大的数组对象，且数组中无引用外部对象。老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。 方法区方法区在一个jvm实例的内部，类型信息被存储在一个称为方法区的内存逻辑区中。类型信息是由类加载器在类加载时从类文件中提取出来的。类(静态)变量也存储在方法区中。简单说方法区用来存储类型的元数据信息，一个.class文件是类被java虚拟机使用之前的表现形式，一旦这个类要被使用，java虚拟机就会对其进行装载、连接（验证、准备、解析）和初始化。而装载（后的结果就是由.class文件转变为方法区中的一段特定的数据结构。这个数据结构会存储如下信息： 类型信息 这个类型的全限定名 这个类型的直接超类的全限定名 这个类型是类类型还是接口类型 这个类型的访问修饰符 任何直接超接口的全限定名的有序列表 字段信息 字段名 字段类型 字段的修饰符 方法信息 方法名 方法返回类型 方法参数的数量和类型（按照顺序） 方法的修饰符 其他信息 除了常量以外的所有类（静态）变量 一个指向ClassLoader的指针 一个指向Class对象的指针 常量池（常量数据以及对其他类型的符号引用） JVM为每个已加载的类型都维护一个常量池。常量池就是这个类型用到的常量的一个有序集合，包括实际的常量(string,integer,和floating point常量)和对类型，域和方法的符号引用。池中的数据项象数组项一样，是通过索引访问的。 每个类的这些元数据，无论是在构建这个类的实例还是调用这个类某个对象的方法，都会访问方法区的这些元数据。构建一个对象时，JVM会在堆中给对象分配空间，这些空间用来存储当前对象实例属性以及其父类的实例属性（而这些属性信息都是从方法区获得），注意，这里并不是仅仅为当前对象的实例属性分配空间，还需要给父类的实例属性分配，到此其实我们就可以回答第一个问题了，即实例化父类的某个子类时，JVM也会同时构建父类的一个对象。从另外一个角度也可以印证这个问题：调用当前类的构造方法时，首先会调用其父类的构造方法直到Object，而构造方法的调用意味着实例的创建，所以子类实例化时，父类肯定也会被实例化。类变量被类的所有实例共享，即使没有类实例时你也可以访问它。这些变量只与类相关，所以在方法区中，它们成为类数据在逻辑上的一部分。在JVM使用一个类之前，它必须在方法区中为每个non-final类变量分配空间。 方法区主要有以下几个特点：1、方法区是线程安全的。由于所有的线程都共享方法区，所以，方法区里的数据访问必须被设计成线程安全的。例如，假如同时有两个线程都企图访问方法区中的同一个类，而这个类还没有被装入JVM，那么只允许一个线程去装载它，而其它线程必须等待2、方法区的大小不必是固定的，JVM可根据应用需要动态调整。同时，方法区也不一定是连续的，方法区可以在一个堆(甚至是JVM自己的堆)中自由分配。3、方法区也可被垃圾收集，当某个类不在被使用(不可触及)时，JVM将卸载这个类，进行垃圾收集 可以通过-XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。对于习惯在HotSpot 虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（PermanentGeneration），本质上两者并不等价，仅仅是因为HotSpot 虚拟机的设计团队选择把GC 分代收集扩展至方法区，或者说使用永久代来实现方法区而已。对于其他虚拟机（如BEA JRockit、IBM J9 等）来说是不存在永久代的概念的。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 总结 名称 特征 作用 配置参数 异常 程序计数器 占用内存小，线程私有，生命周期与线程相同 大致为字节码行号指示器 - - 虚拟机栈 线程私有，生命周期与线程相同，使用连续的内存空间 Java 方法执行的内存模型，存储局部变量表、操作栈、动态链接、方法出口等信息 -Xss StackOverflowError OutOfMemoryError java堆 线程共享，生命周期与虚拟机相同，可以不使用连续的内存地址 保存对象实例，所有对象实例（包括数组）都要在堆上分配 -Xms-Xsx-Xmn OutOfMemoryError 方法区 线程共享，生命周期与虚拟机相同，可以不使用连续的内存地址 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据 -XX:PermSize:16M-XX:MaxPermSize64M OutOfMemoryError 运行时常量池 方法区的一部分，具有动态性 存放字面量及符号引用 - - 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError 异常出现，所以我们放到这里一起讲解。在JDK 1.4 中新加入了NIO（NewInput/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O 方式，它可以使用Native 函数库直接分配堆外内存，然后通过一个存储在Java 堆里面的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java 堆和Native 堆中来回复制数据。 堆与栈的对比经常有人把Java 内存区分为堆内存（Heap）和栈内存（Stack），这种分法比较粗糙，Java内存区域的划分实际上远比这复杂。这种划分方式的流行只能说明大多数程序员最关注的、与对象内存分配关系最密切的内存区域是这两块。堆很灵活，但是不安全。对于对象，我们要动态地创建、销毁，不能说后创建的对象没有销毁，先前创建的对象就不能销毁，那样的话我们的程序就寸步难行，所以Java中用堆来存储对象。而一旦堆中的对象被销毁，我们继续引用这个对象的话，就会出现著名的 NullPointerException，这就是堆的缺点——错误的引用逻辑只有在运行时才会被发现。栈不灵活，但是很严格，是安全的，易于管理。因为只要上面的引用没有销毁，下面引用就一定还在，在大部分程序中，都是先定义的变量、引用先进栈，后定义的后进栈，同时，区块内部的变量、引用在进入区块时压栈，区块结束时出栈，理解了这种机制，我们就可以很方便地理解各种编程语言的作用域的概念了，同时这也是栈的优点——错误的引用逻辑在编译时就可以被发现。 栈—主要存放引用和基本数据类型。 堆—用来存放 new 出来的对象实例。 内存溢出和内存泄漏内存溢出 out of memory，是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer，但给它存了long才能存下的数，那就是内存溢出。内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。memory leak会最终会导致out ofmemory。 Java 堆内存的OutOfMemoryError异常是实际应用中最常见的内存溢出异常情况。出现Java 堆内存溢出时，异常堆栈信息“java.lang.OutOfMemoryError”会跟着进一步提示“Java heapspace”。要解决这个区域的异常，一般的手段是首先通过内存映像分析工具（如Eclipse Memory Analyzer）对dump 出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏（Memory Leak）还是内存溢出（Memory Overflow）。如果是内存泄漏，可进一步通过工具查看泄漏对象到GC Roots 的引用链。于是就能找到泄漏对象是通过怎样的路径与GC Roots 相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息，以及GC Roots 引用链的信息，就可以比较准确地定位出泄漏代码的位置。如果不存在泄漏，换句话说就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx 与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 内存分配过程1、JVM 会试图为相关Java对象在Eden Space中初始化一块内存区域。2、当Eden空间足够时，内存申请结束；否则到下一步。3、JVM 试图释放在Eden中所有不活跃的对象（这属于1或更高级的垃圾回收）。释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区。4、Survivor区被用来作为Eden及Old的中间交换区域，当Old区空间足够时，Survivor区的对象会被移到Old区，否则会被保留在Survivor区。5、当Old区空间不够时，JVM 会在Old区进行完全的垃圾收集（0级）。6、完全垃圾收集后，若Survivor及Old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现“outofmemory”错误。 对象访问对象访问在Java 语言中无处不在，是最普通的程序行为，但即使是最简单的访问，也会却涉及Java 栈、Java 堆、方法区这三个最重要内存区域之间的关联关系，如下面的这句代码： Object obj = newObject(); 假设这句代码出现在方法体中，那“Object obj”这部分的语义将会反映到Java 栈的本地变量表中，作为一个reference 类型数据出现。而“new Object()”这部分的语义将会反映到Java 堆中，形成一块存储了Object 类型所有实例数据值（Instance Data，对象中各个实例字段的数据）的结构化内存，根据具体类型以及虚拟机实现的对象内存布局（Object Memory Layout）的不同，这块内存的长度是不固定的。另外，在Java 堆中还必须包含能查找到此对象类型数据（如对象类型、父类、实现的接口、方法等）的地址信息，这些类型数据则存储在方法区中。由于reference 类型在Java 虚拟机规范里面只规定了一个指向对象的引用，并没有定义这个引用应该通过哪种方式去定位，以及访问到Java 堆中的对象的具体位置，因此不同虚拟机实现的对象访问方式会有所不同，主流的访问方式有两种：使用句柄和直接指针。如果使用句柄访问方式，Java 堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>方法区</tag>
        <tag>堆</tag>
        <tag>本地方法栈</tag>
        <tag>虚拟机栈</tag>
        <tag>程序计数器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程-线程间协作]]></title>
    <url>%2F2017%2F07%2F18%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E9%97%B4%E5%8D%8F%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Java并发编程：线程间协作的两种方式：wait、notify、notifyAll和Condition 在前面介绍了很多关于同步和锁的问题，然而在现实中，需要线程之间的协作。比如说最经典的生产者-消费者模型：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此，一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作。 wait()、notify()和notifyAll()wait()、notify()和notifyAll()是Object类中的方法： /** * Wakes up a single thread that is waiting on this object&#39;s * monitor. If any threads are waiting on this object, one of them * is chosen to be awakened. The choice is arbitrary and occurs at * the discretion of the implementation. A thread waits on an object&#39;s * monitor by calling one of the {@code wait} methods. * &lt;p&gt; * The awakened thread will not be able to proceed until the current * thread relinquishes the lock on this object. The awakened thread will * compete in the usual manner with any other threads that might be * actively competing to synchronize on this object; for example, the * awakened thread enjoys no reliable privilege or disadvantage in being * the next thread to lock this object. * &lt;p&gt; * This method should only be called by a thread that is the owner * of this object&#39;s monitor. A thread becomes the owner of the * object&#39;s monitor in one of three ways: * &lt;ul&gt; * &lt;li&gt;By executing a synchronized instance method of that object. * &lt;li&gt;By executing the body of a {@code synchronized} statement * that synchronizes on the object. * &lt;li&gt;For objects of type {@code Class,} by executing a * synchronized static method of that class. * &lt;/ul&gt; * &lt;p&gt; * Only one thread at a time can own an object&#39;s monitor. * * @throws IllegalMonitorStateException if the current thread is not * the owner of this object&#39;s monitor. * @see java.lang.Object#notifyAll() * @see java.lang.Object#wait() */ public final native void notify(); /** * Wakes up all threads that are waiting on this object&#39;s monitor. A * thread waits on an object&#39;s monitor by calling one of the * {@code wait} methods. * &lt;p&gt; * The awakened threads will not be able to proceed until the current * thread relinquishes the lock on this object. The awakened threads * will compete in the usual manner with any other threads that might * be actively competing to synchronize on this object; for example, * the awakened threads enjoy no reliable privilege or disadvantage in * being the next thread to lock this object. * &lt;p&gt; * This method should only be called by a thread that is the owner * of this object&#39;s monitor. See the {@code notify} method for a * description of the ways in which a thread can become the owner of * a monitor. * * @throws IllegalMonitorStateException if the current thread is not * the owner of this object&#39;s monitor. * @see java.lang.Object#notify() * @see java.lang.Object#wait() */ public final native void notifyAll(); /** * Causes the current thread to wait until either another thread invokes the * {@link java.lang.Object#notify()} method or the * {@link java.lang.Object#notifyAll()} method for this object, or a * specified amount of time has elapsed. * &lt;p&gt; * The current thread must own this object&#39;s monitor. * &lt;p&gt; * This method causes the current thread (call it &lt;var&gt;T&lt;/var&gt;) to * place itself in the wait set for this object and then to relinquish * any and all synchronization claims on this object. Thread &lt;var&gt;T&lt;/var&gt; * becomes disabled for thread scheduling purposes and lies dormant * until one of four things happens: * &lt;ul&gt; * &lt;li&gt;Some other thread invokes the {@code notify} method for this * object and thread &lt;var&gt;T&lt;/var&gt; happens to be arbitrarily chosen as * the thread to be awakened. * &lt;li&gt;Some other thread invokes the {@code notifyAll} method for this * object. * &lt;li&gt;Some other thread {@linkplain Thread#interrupt() interrupts} * thread &lt;var&gt;T&lt;/var&gt;. * &lt;li&gt;The specified amount of real time has elapsed, more or less. If * {@code timeout} is zero, however, then real time is not taken into * consideration and the thread simply waits until notified. * &lt;/ul&gt; * The thread &lt;var&gt;T&lt;/var&gt; is then removed from the wait set for this * object and re-enabled for thread scheduling. It then competes in the * usual manner with other threads for the right to synchronize on the * object; once it has gained control of the object, all its * synchronization claims on the object are restored to the status quo * ante - that is, to the situation as of the time that the {@code wait} * method was invoked. Thread &lt;var&gt;T&lt;/var&gt; then returns from the * invocation of the {@code wait} method. Thus, on return from the * {@code wait} method, the synchronization state of the object and of * thread {@code T} is exactly as it was when the {@code wait} method * was invoked. * &lt;p&gt; * A thread can also wake up without being notified, interrupted, or * timing out, a so-called &lt;i&gt;spurious wakeup&lt;/i&gt;. While this will rarely * occur in practice, applications must guard against it by testing for * the condition that should have caused the thread to be awakened, and * continuing to wait if the condition is not satisfied. In other words, * waits should always occur in loops, like this one: * &lt;pre&gt; * synchronized (obj) { * while (&amp;lt;condition does not hold&amp;gt;) * obj.wait(timeout); * ... // Perform action appropriate to condition * } * &lt;/pre&gt; * (For more information on this topic, see Section 3.2.3 in Doug Lea&#39;s * &quot;Concurrent Programming in Java (Second Edition)&quot; (Addison-Wesley, * 2000), or Item 50 in Joshua Bloch&#39;s &quot;Effective Java Programming * Language Guide&quot; (Addison-Wesley, 2001). * * &lt;p&gt;If the current thread is {@linkplain java.lang.Thread#interrupt() * interrupted} by any thread before or while it is waiting, then an * {@code InterruptedException} is thrown. This exception is not * thrown until the lock status of this object has been restored as * described above. * * &lt;p&gt; * Note that the {@code wait} method, as it places the current thread * into the wait set for this object, unlocks only this object; any * other objects on which the current thread may be synchronized remain * locked while the thread waits. * &lt;p&gt; * This method should only be called by a thread that is the owner * of this object&#39;s monitor. See the {@code notify} method for a * description of the ways in which a thread can become the owner of * a monitor. * * @param timeout the maximum time to wait in milliseconds. * @throws IllegalArgumentException if the value of timeout is * negative. * @throws IllegalMonitorStateException if the current thread is not * the owner of the object&#39;s monitor. * @throws InterruptedException if any thread interrupted the * current thread before or while the current thread * was waiting for a notification. The &lt;i&gt;interrupted * status&lt;/i&gt; of the current thread is cleared when * this exception is thrown. * @see java.lang.Object#notify() * @see java.lang.Object#notifyAll() */ public final native void wait(long timeout) throws InterruptedException; 从这三个方法的annotation可以知道以下几点信息： wait()、notify()和notifyAll()方法是本地方法，并且为final方法，无法被重写。 调用某个对象的wait()方法能让当前线程阻塞，并且当前线程必须拥有此对象的monitor（即锁） 调用某个对象的notify()方法能够唤醒一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程； 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程； 有朋友可能会有疑问：为何这三个不是Thread类声明中的方法，而是Object类中声明的方法（当然由于Thread类继承了Object类，所以Thread也可以调用这三个方法）？其实这个问题很简单，由于每个对象都拥有monitor（即锁），所以让当前线程等待某个对象的锁，当然应该通过这个对象来操作了。而不是用当前线程来操作，因为当前线程可能会等待多个线程的锁，如果通过线程来操作，就非常复杂了。 上面已经提到，如果调用某个对象的wait()方法，当前线程必须拥有这个对象的monitor（即锁），因此调用wait()方法必须在同步块或者同步方法中进行（synchronized块或者synchronized方法）。 调用某个对象的wait()方法，相当于让当前线程交出此对象的monitor，然后进入等待状态，等待后续再次获得此对象的锁（Thread类中的sleep方法使当前线程暂停执行一段时间，从而让其他线程有机会继续执行，但它并不释放对象锁）； notify()方法能够唤醒一个正在等待该对象的monitor的线程，当有多个线程都在等待该对象的monitor的话，则只能随机唤醒其中一个线程，具体唤醒哪个线程则不得而知。 同样地，调用某个对象的notify()方法，当前线程也必须拥有这个对象的monitor，因此调用notify()方法必须在同步块或者同步方法中进行（synchronized块或者synchronized方法）。 nofityAll()方法能够唤醒所有正在等待该对象的monitor的线程，这一点与notify()方法是不同的。 这里要注意一点：notify()和notifyAll()方法只是唤醒等待该对象的monitor的线程，并不决定哪个线程能够获取到monitor。 举个简单的例子：假如有三个线程Thread1、Thread2和Thread3都在等待对象objectA的monitor，此时Thread4拥有对象objectA的monitor，当在Thread4中调用objectA.notify()方法之后，Thread1、Thread2和Thread3只有一个能被唤醒。注意，被唤醒不等于立刻就获取了objectA的monitor。假若在Thread4中调用objectA.notifyAll()方法，则Thread1、Thread2和Thread3三个线程都会被唤醒，至于哪个线程接下来能够获取到objectA的monitor就具体依赖于操作系统的调度了。 上面尤其要注意一点，一个线程被唤醒不代表立即获取了对象的monitor，只有等调用完notify()或者notifyAll()并退出synchronized块，释放对象锁后，其余线程才可获得锁执行。 public class Test { private static final Object object = new Object(); public static void main(String[] args) { Thread1 thread1 = new Thread1(); Thread2 thread2 = new Thread2(); thread1.start(); try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } thread2.start(); } static class Thread1 extends Thread { @Override public void run() { synchronized (object) { try { object.wait(); } catch (InterruptedException ignored) { } System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;获取到了锁&quot;); } } } static class Thread2 extends Thread { @Override public void run() { synchronized (object) { object.notify(); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;调用了object.notify()&quot;); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;释放了锁&quot;); } } } } 无论运行多少次，运行结果必定是： 线程Thread-1调用了object.notify() 线程Thread-1释放了锁 线程Thread-0获取到了锁 Condition Condition是在java 1.5中才出现的，它用来替代传统的Object的wait()、notify()、notifyAll()实现线程间的协作，相比之下，使用Condition的await()、signal()、signalAll()这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用Condition，在阻塞队列那一篇博文中就讲述到了，阻塞队列实际上是使用了Condition来模拟线程间协作。 Condition是个接口，基本的方法就是await()和signal()方法；Condition依赖于Lock接口，生成一个Condition的基本代码是lock.newCondition() 调用Condition的await()和signal()方法，都必须在lock保护之内，就是说必须在lock.lock()和lock.unlock之间才可以使用 Conditon中的await()对应Object的wait()； Condition中的signal()对应Object的notify()； Condition中的signalAll()对应Object的notifyAll()。 生产者-消费者模型的实现使用Object的wait()和notify()实现：private int queueSize = 10; private final PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(queueSize); public static void main(String[] args) { Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); } class Consumer extends Thread{ @Override public void run() { consume(); } private void consume() { while (true) synchronized (queue) { while (queue.size() == 0) { try { System.out.println(&quot;队列空，等待数据&quot;); queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); queue.notify(); throw new RuntimeException(&quot;&quot;); } } queue.poll(); //每次移走队首元素 queue.notify(); System.out.println(&quot;从队列取走一个元素，队列剩余&quot; + queue.size() + &quot;个元素&quot;); } } } class Producer extends Thread{ @Override public void run() { produce(); } private void produce() { while(true){ synchronized (queue) { while(queue.size() == queueSize) try { System.out.println(&quot;队列满，等待有空余空间&quot;); queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); queue.notify(); throw new RuntimeException(&quot;&quot;); } queue.offer(1); //每次插入一个元素 queue.notify(); System.out.println(&quot;向队列取中插入一个元素，队列剩余空间：&quot;+(queueSize-queue.size())); } } } } 使用Condition实现 private int queueSize = 10; private PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(queueSize); private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public static void main(String[] args) { Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); } class Consumer extends Thread{ @Override public void run() { consume(); } private void consume() { while(true){ lock.lock(); try { while(queue.size() == 0){ try { System.out.println(&quot;队列空，等待数据&quot;); condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } } queue.poll(); //每次移走队首元素 condition.signal(); System.out.println(&quot;从队列取走一个元素，队列剩余&quot;+queue.size()+&quot;个元素&quot;); } finally{ lock.unlock(); } } } } class Producer extends Thread{ @Override public void run() { produce(); } private void produce() { while(true){ lock.lock(); try { while(queue.size() == queueSize){ try { System.out.println(&quot;队列满，等待有空余空间&quot;); condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } } queue.offer(1); //每次插入一个元素 condition.signal(); System.out.println(&quot;向队列取中插入一个元素，队列剩余空间：&quot;+(queueSize-queue.size())); } finally{ lock.unlock(); } } } }]]></content>
      <categories>
        <category>java</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>wait</tag>
        <tag>notify</tag>
        <tag>notifyAll</tag>
        <tag>await</tag>
        <tag>signal</tag>
        <tag>signalAll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx安装与使用]]></title>
    <url>%2F2017%2F07%2F17%2Fnginx%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[nginx安装第一步，在/etc/yum.repos.d/目录下创建一个源配置文件nginx.repo： 12cd /etc/yum.repos.d/ vim nginx.repo 填写如下内容： 12345[nginx]name=nginx repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=0 enabled=1 保存，则会产生一个/etc/yum.repos.d/nginx.repo文件。 下面直接执行如下指令即可自动安装好Nginx： 1yum install nginx 安装完成，下面直接就可以启动Nginx了： 1/etc/init.d/nginx start 我是用：服务启动并设置开机启动 12345service nginx startchkconfig nginx on --level 35#指定配置文件启动/usr/sbin/nginx -c /etc/nginx/nginx.conf 启动成功如下 123456789[op@bogon init.d]$ ps aux | grep nginxroot 19210 0.0 0.0 46348 1192 ? Ss 14:57 0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 19211 0.0 0.0 46740 1968 ? S 14:57 0:00 nginx: worker process nginx 19212 0.0 0.0 46740 1936 ? S 14:57 0:00 nginx: worker process nginx 19214 0.0 0.0 46740 1968 ? S 14:57 0:00 nginx: worker process nginx 19215 0.0 0.0 46740 1968 ? S 14:57 0:00 nginx: worker process nginx 19216 0.0 0.0 46740 1936 ? S 14:57 0:00 nginx: worker process nginx 19217 0.0 0.0 46740 1936 ? S 14:57 0:00 nginx: worker process op 19219 0.0 0.0 103328 872 pts/0 S+ 14:58 0:00 grep nginx 常用命令 123456789101112131415sudo nginx #启动nginxnginx -s reload|reopen|stop|quit #重新加载配置|重启|停止|退出 nginxnginx -t #测试配置是否有语法错误nginx [-?hvVtq] [-s signal] [-c filename] [-p prefix] [-g directives]-?,-h : 打开帮助信息-v : 显示版本信息并退出-V : 显示版本和配置选项信息，然后退出-t : 检测配置文件是否有语法错误，然后退出-q : 在检测配置文件期间屏蔽非错误信息-s signal : 给一个 nginx 主进程发送信号：stop（停止）, quit（退出）, reopen（重启）, reload（重新加载配置文件）-p prefix : 设置前缀路径（默认是：/usr/local/Cellar/nginx/1.2.6/）-c filename : 设置配置文件（默认是：/usr/local/etc/nginx/nginx.conf）-g directives : 设置配置文件外的全局指令 nginx 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115user nginx;#启动进程,通常设置成和cpu的数量相等worker_processes 4;#全局错误日志及PID文件#error_log /data/nginx/logs/error.log warn;#error_log /data/nginx/logs/error.log notice;#error_log /data/nginx/logs/error.log info;#pid /data/nginx/pid/nginx.pid;#工作模式及连接数上限events &#123; #epoll是多路复用IO(I/O Multiplexing)中的一种方式, #仅用于linux2.6以上内核,可以大大提高nginx的性能 use epoll; #单个后台worker process进程的最大并发链接数 worker_connections 1024; # 并发总数是 worker_processes 和 worker_connections 的乘积 # 即 max_clients = worker_processes * worker_connections # 在设置了反向代理的情况下，max_clients = worker_processes * worker_connections / 4 为什么 # 为什么上面反向代理要除以4，应该说是一个经验值 # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000 # worker_connections 值的设置跟物理内存大小有关 # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数 # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右 # 我们来看看360M内存的VPS可以打开的文件句柄数是多少： # $ cat /proc/sys/fs/file-max # 输出 34336 # 32000 &lt; 34336，即并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内 # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置 # 使得并发总数小于操作系统可以打开的最大文件数目 # 其实质也就是根据主机的物理CPU和内存进行配置 # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。 # ulimit -SHn 65535&#125;http &#123; #设定mime类型,类型由mime.type文件定义 include mime.types; default_type application/octet-stream; #设定日志格式 log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件， #对于普通应用，必须设为 on, #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off， #以平衡磁盘与网络I/O处理速度，降低系统的uptime. sendfile on; #tcp_nopush on; #连接超时时间 #keepalive_timeout 0; keepalive_timeout 65; tcp_nodelay on; #开启gzip压缩 gzip on; gzip_disable &quot;MSIE [1-6].&quot;; #设定请求缓冲 client_header_buffer_size 128k; large_client_header_buffers 4 128k; #设定虚拟主机配置 server &#123; #侦听80端口 listen 80; #定义使用 www.nginx.cn访问 server_name www.nginx.cn; #定义服务器的默认网站根目录位置 root html; #设定本虚拟主机的访问日志 access_log logs/nginx.access.log main; #默认请求 location / &#123; #定义首页索引文件的名称 index index.php index.html index.htm; &#125; # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; #过期30天，静态文件不怎么更新，过期可以设大一点， #如果频繁更新，则可以设置得小一点。 expires 30d; &#125; #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置. location ~ .php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; #禁止访问 .htxxx 文件 location ~ /.ht &#123; deny all; &#125; &#125;&#125; 访问nginx 502错误,而通过服务器地址:原始端口号(8080)可以访问查看/var/log/nginx/error.log显示为 failed (13: Permission denied)命令行临时关闭selinux确认是否由此设置引起的 12$ getenforce #查看selinux状态$ setenforce 0 然后测试是否可以访问如果可以则证明是selinux的问题，于是先关掉selinux nginx拒绝或允许指定IP,是使用模块HTTP访问控制模块（HTTP Access）. 控制规则按照声明的顺序进行检查，首条匹配IP的访问规则将被启用。 123456location / &#123; deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/16; deny all;&#125; 上面的例子中仅允许192.168.1.0/24和10.1.1.0/16网络段访问这个location字段，但192.168.1.1是个例外。注意规则的匹配顺序，如果你使用过apache你可能会认为你可以随意控制规则的顺序并且他们能够正常的工作，但实际上不行。 下面的这个例子将拒绝掉所有的连接： 12345678location / &#123; #这里将永远输出403错误。 deny all; #这些指令不会被启用，因为到达的连接在第一条已经被拒绝 deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/1&#125; 反向代理nginx负载均衡基于iphash的session黏贴nginx可以根据客户端IP进行负载均衡，在upstream里设置ip_hash，就可以针对同一个C类地址段中的客户端选择同一个后端服务器，除非那个后端服务器宕了才会换一个。nginx的upstream目前支持的5种方式的分配 轮询（默认）每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。1234upstream backserver &#123;server 192.168.0.14;server 192.168.0.15;&#125; weight指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。1234upstream backserver &#123;server 192.168.0.14 weight=10;server 192.168.0.15 weight=10;&#125; ip_hash每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。12345upstream backserver &#123;ip_hash;server 192.168.0.14:88;server 192.168.0.15:80;&#125; fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。12345upstream backserver &#123;server server1;server server2;fair;&#125; url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。123456upstream backserver &#123;server squid1:3128;server squid2:3128;hash $request_uri;hash_method crc32;&#125; 在需要使用负载均衡的server中增加12345678proxy_pass http://backserver/;upstream backserver&#123;ip_hash;server 127.0.0.1:9090 down; (down 表示单前的server暂时不参与负载)server 127.0.0.1:8080 weight=2; (weight 默认为1.weight越大，负载的权重就越大)server 127.0.0.1:6060;server 127.0.0.1:7070 backup; (其它所有的非backup机器down或者忙的时候，请求backup机器)&#125; max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误 fail_timeout: max_fails次失败后，暂停的时间 一般做负载均衡，都需要后端多台web服务器之间实现session共享，否则用户登录可能就有问题了。今天看nginx文档时候，发现nginx可以根据客户端IP进行负载均衡，在upstream里设置ip_hash，就可以针对同一个C类地址段中的客户端选择同一个后端服务器，除非那个后端服务器宕了才会换一个。原文如下：The key for the hash is the class-C network address of the client. This method guarantees that the client request will always be forwarded to the same server. But if this server is considered inoperative, then the request of this client will be transferred to another server. This gives a high probability clients will always connect to the same server.也就是说我可以在两台服务器上跑两个论坛，但共享一个后台数据库，而不用去关心session共享的问题，前面启用ip_hash，正常情况下，客户端上网获得IP，登录浏览发帖等都会被转到固定的后端服务器上，这样就不会出问题了。应该说来访IP分布越广，负载均衡就越平均。嘿嘿，如果都是同一个C来的用户，那就没用了。虚拟机上搭了个环境测试了一下：装了三个nginx，分别在80，81，82端口上。80端口上的nginx做负载均衡前端，配置到后面两个nginx：12345upstream test&#123; ip_hash; server 127.0.0.1:81 ; server 127.0.0.1:82 ;&#125; 在81端口的nginx上写个简单的html，内容为1；在82端口的nginx上写个内容为2的html，两个文件同名。在有ip_hash的时候，刷新页面http://192.168.1.33/index.html，始终显示为1，没有ip_hash的时候，则为轮流的1和2]]></content>
      <categories>
        <category>代理</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr安装与使用]]></title>
    <url>%2F2017%2F07%2F16%2Fsolr%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[solr+tomcat单节点Solr6部署在Tomcat8环境下 solr下载 在之前版本的Solr安装包中，存在solr.war文件，但是Solr6已经没有这个war包了，它已经被解压到了.\solr-6.0.0\server\solr-webapp文件夹下，将该文件夹复制到.\apache-tomcat-8.5.5\webapps下，并将其重命名为solr。 日志处理：将Solr安装包中.\solr-6.0.0\server\lib\ext内的5个jar包复制到.\apache-tomcat-8.5.5\webapps\solr\WEB-INF\lib下。将.\solr-6.0.0\server\resources下的log4j.properties文件复制到.\apache-tomcat-8.5.5\webapps\solr\WEB-INF\classes中，这里的classes目录需要自己新建。log4j.properties文件中有一行log4j.appender.file.File=${solr.log}/solr.log指定log文件的存放路径，可以指定到特定的目录。配置solr_home：在磁盘任意位置新建目录，取名solr_home,把.\solr-6.0.0\server\solr下的整个solr文件夹复制到solr_home，编辑.\apache-tomcat-8.5.5\webapps\solr\WEB-INF下的web.xml文件。将web.xml文件注释去掉，&lt;env-entry-value&gt;中填刚才新建的solr_home路径 12345&lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/home/jet/workspace/github/bigdata/solr/solr_home&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt;&lt;/env-entry&gt; 这个solr_home里面的内容是复制.\solr-6.0.0\server\solr\下的内容，然后在该目录下新建文件夹core0，把 .\solr-6.0.0\server\solr\configsets\basic_configs\下的所有文件复制进来。 运行的话，直接双击startup.sh 访问http://localhost:8080/solr/index.html，可以看到solr管理界面，但是我们的sore还是空的，需要手动创建。在管理界面，点击No cores available选项，在弹出的窗口刚才新建的文件夹的名字。Add core这样，最简单的Solr就搭建完成了。 【注意事项】solr6以上必须要求jdk1.8以上版本，不然访问会报404,而且tomcat日志不会报任何错误信息且访问路径是localhost:8080/solr/index.html solrcloud6配置hosts192.168.40.200 jet.solr.com192.168.40.2 hadoop02.hadoop.com192.168.40.3 hadoop03.hadoop.com 关闭防火墙(可以只执行第二步：推荐)1) 永久关闭防火墙: vi /etc/selinux/config，将其中的SELinux设置：SELINUX=disabled 2) 关闭防火墙 : systemctl disable firewalld.service #禁用 3) 关闭packagekit: vi /etc/yum/pluginconf.d/langpacks.conf，将enabled设为 0 配置tomcat（各个solr节点一样）启动与关闭 启动：到bin目录下./startup.sh 验证：http://192.168.40.200:8080 关闭：到bin目录下./shutdown.sh 配置zookeeper（各个zookeeper节点上面）(1) 解压 1tar -zxvf zookeeper-3.4.10.tar.gz (2)创建data和logs目录 创建目录并赋于写权限 指定zookeeper的数据存放目录/hadoop/zookeeper/data和日志目录/hadoop/zookeeper/logs (3)拷贝zookeeper配制文件zoo_sample.cfg，文件在conf目录下 拷贝zookeeper配制文件zoo_sample.cfg并重命名zoo.cfg (4) 修改zoo.cfg 123456789101112131415161718192021222324252627282930313233### Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing,# software distributed under the License is distributed on an# &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY# KIND, either express or implied. See the License for the# specific language governing permissions and limitations# under the License.###clientPort=2181initLimit=10autopurge.purgeInterval=24syncLimit=5tickTime=3000dataDir=/hadoop/zookeeper/data #上面创建的数据目录dataLogDir=/hadoop/zookeeper/logs #上面创建的日志目录autopurge.snapRetainCount=30server.1=hadoop02.hadoop.com:2888:3888server.2=hadoop03.hadoop.com:2888:3888 (5)进入data文件夹建立对应的myid文件 1vi /hadoop/zookeeper/data/myid (6)拷贝zookeeper 文件夹到其他机器,进行相同配置 配置solr（各个solr节点一样）和上面单节点一样 配置solr服务(1) 在solr下，上传相关信息至zookeeper 在/home/jet/workspace/github/bigdata/solr/solr-6.0.0/server/scripts/cloud-scripts目录下执行 1./zkcli.sh -zkhost 192.168.40.2:2181,192.168.40.3:2181 -cmd upconfig -confdir /home/jet/workspace/github/bigdata/solr/solr_home/core0/conf -confname solrconfig.xml (2) 每一台solr和zookeeper关联 修改每一台solr的tomcat 的 bin目录下catalina.sh文件中加入DzkHost指定zookeeper服务器地址： 在JAVA_OPTS添加zookeepre地址 1JAVA_OPTS=&quot;-DzkHost=192.168.40.2:2181,192.168.40.3:2181&quot; (3) 启动每一台solr服务器进入对应tomcat bin目录启动 1sh startup.sh (4) 查看tomcat日志 1tail -fn 100 catalina.out 如下信息 110035261 INFO (zkCallback-4-thread-34-processing-n:192.168.40.200:8080_solr-EventThread) [ ] o.a.s.c.c.ConnectionManager Watcher org.apache.solr.common.cloud.ConnectionManager@6edf2d5f name:ZooKeeperConnection Watcher:192.168.40.2:2181,192.168.40.3:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None (5) 修改solr启动端口（各个节点的配置一样） 在solrhome下，有一个solr.xml文件，修改其中的内容为： 123$&#123;host:192.168.40.200&#125;$&#123;jetty.port:8080&#125; 然后访问地址 1http://192.168.40.200:8080/solr/index.html#/ 如下 solr全文检索原理solr全文检索实现原理]]></content>
      <categories>
        <category>bigdata</category>
        <category>搜索引擎</category>
      </categories>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab安装及使用]]></title>
    <url>%2F2017%2F07%2F16%2Fcrontab%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[引言经常有需求要在服务器定时执行各种脚本或者服务（shell脚本，python脚本等等），那么自然就会用到crontab 安装crontab:1[root@CentOS ~]# yum install vixie-cron 如下即安装成功 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@bogon mysql]# yum install vixie-cronLoaded plugins: fastestmirror, securitySetting up Install ProcessLoading mirror speeds from cached hostfile * base: mirror.bit.edu.cn * extras: mirror.bit.edu.cn * updates: mirror.bit.edu.cnResolving Dependencies--&gt; Running transaction check---&gt; Package cronie.x86_64 0:1.4.4-16.el6_8.2 will be installed--&gt; Processing Dependency: dailyjobs for package: cronie-1.4.4-16.el6_8.2.x86_64--&gt; Processing Dependency: /usr/sbin/sendmail for package: cronie-1.4.4-16.el6_8.2.x86_64--&gt; Running transaction check---&gt; Package cronie-anacron.x86_64 0:1.4.4-16.el6_8.2 will be installed--&gt; Processing Dependency: crontabs for package: cronie-anacron-1.4.4-16.el6_8.2.x86_64---&gt; Package sendmail.x86_64 0:8.14.4-9.el6_8.1 will be installed--&gt; Processing Dependency: procmail for package: sendmail-8.14.4-9.el6_8.1.x86_64--&gt; Running transaction check---&gt; Package crontabs.noarch 0:1.10-33.el6 will be installed---&gt; Package procmail.x86_64 0:3.22-25.1.el6_5.1 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved============================================================================================================================================================================ Package Arch Version Repository Size============================================================================================================================================================================Installing: cronie x86_64 1.4.4-16.el6_8.2 base 75 kInstalling for dependencies: cronie-anacron x86_64 1.4.4-16.el6_8.2 base 31 k crontabs noarch 1.10-33.el6 base 10 k procmail x86_64 3.22-25.1.el6_5.1 base 162 k sendmail x86_64 8.14.4-9.el6_8.1 base 717 kTransaction Summary============================================================================================================================================================================Install 5 Package(s)Total download size: 995 kInstalled size: 2.1 MIs this ok [y/N]: yDownloading Packages:(1/5): cronie-1.4.4-16.el6_8.2.x86_64.rpm | 75 kB 00:00 (2/5): cronie-anacron-1.4.4-16.el6_8.2.x86_64.rpm | 31 kB 00:00 (3/5): crontabs-1.10-33.el6.noarch.rpm | 10 kB 00:00 (4/5): procmail-3.22-25.1.el6_5.1.x86_64.rpm | 162 kB 00:00 (5/5): sendmail-8.14.4-9.el6_8.1.x86_64.rpm | 717 kB 00:00 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 788 kB/s | 995 kB 00:01 Running rpm_check_debugRunning Transaction TestTransaction Test SucceededRunning Transaction Installing : procmail-3.22-25.1.el6_5.1.x86_64 1/5 Installing : sendmail-8.14.4-9.el6_8.1.x86_64 2/5 Installing : cronie-1.4.4-16.el6_8.2.x86_64 3/5 Installing : crontabs-1.10-33.el6.noarch 4/5 Installing : cronie-anacron-1.4.4-16.el6_8.2.x86_64 5/5 Verifying : crontabs-1.10-33.el6.noarch 1/5 Verifying : procmail-3.22-25.1.el6_5.1.x86_64 2/5 Verifying : cronie-anacron-1.4.4-16.el6_8.2.x86_64 3/5 Verifying : sendmail-8.14.4-9.el6_8.1.x86_64 4/5 Verifying : cronie-1.4.4-16.el6_8.2.x86_64 5/5 Installed: cronie.x86_64 0:1.4.4-16.el6_8.2 Dependency Installed: cronie-anacron.x86_64 0:1.4.4-16.el6_8.2 crontabs.noarch 0:1.10-33.el6 procmail.x86_64 0:3.22-25.1.el6_5.1 sendmail.x86_64 0:8.14.4-9.el6_8.1 Complete! 说明：vixie-cron软件包是cron的主程序；crontabs软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。 cron 是linux的内置服务，但它不自动起来，可以用以下的方法启动、关闭这个服务： 12345service crond start //启动服务service crond stop //关闭服务service crond restart //重启服务service crond reload //重新载入配置service crond status //查看服务状态 执行命令：ntsysv 查看开机启动的服务，tab键选择菜单 加入开机自动启动: 1chkconfig --level 35 crond on 使用crontab1.crontab命令 功能说明：设置计时器。 语 法：crontab [-u &lt;用户名称&gt;][配置文件] 或 crontab [-u &lt;用户名称&gt;][-elr] 补充说明：cron是一个常驻服务，它提供计时器的功能，让用户在特定的时间得以执行预设的指令或程序。只要用户会编辑计时器的配置文件，就可以使用计时器的功能。其配置文件格式如下：Minute Hour Day Month DayOFWeek Command 参 数： -e 编辑该用户的计时器设置。 -l 列出该用户的计时器设置。 -r 删除该用户的计时器设置。 -u&lt;用户名称&gt; 指定要设定计时器的用户名称。 2.crontab 格式 基本格式 : * * * * * command 分 时 日 月 周 命令 第1列表示分钟0～59 每分钟用*或者 */1表示第2列表示小时0～23（0表示0点）第3列表示日期1～31第4列表示月份1～12第5列标识号星期0～6（0表示星期天）第6列要运行的命令 示例： 每晚凌晨0点执行sql_backup.sh脚本，并将日志输出追加到backlog_date.log文件中，date是按天的时间2&gt;&amp;1表示将错误信息重定向至标准输出 10 0 * * * sh /home/backups/mysql/sql_backup.sh &gt;&gt; /home/logs/mysql_backup/backlog_`date +&quot;\%Y\%m\%d&quot;`.log 2&gt;&amp;1 每晚的21:30重启apache。130 21 * * * /usr/local/etc/rc.d/lighttpd restart 每月1、10、22日的4 : 45重启apache。 145 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart 每周六、周日的1 : 10重启apache。 110 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart 每天18 : 00至23 : 00之间每隔30分钟重启apache。 10,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart 每星期六的23 : 00 pm重启apache。 10 23 * * 6 /usr/local/etc/rc.d/lighttpd restart 每一小时重启apache 1* */1 * * * /usr/local/etc/rc.d/lighttpd restart 晚上11点到早上7点之间，每隔一小时重启apache 1* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart 每月的4号与每周一到周三的11点重启apache 10 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart 一月一号的4点重启apache 10 4 1 jan * /usr/local/etc/rc.d/lighttpd restart 每分钟同步time.windows.com服务器时间 10-59/1 * * * * /usr/sbin/ntpdate time.windows.com 在以上任何值中，星号（*）可以用来代表所有有效的值。譬如，月份值中的星号意味着在满足其它制约条件后每月都执行该命令。 整数间的短线（-）指定一个整数范围。譬如，1-4 意味着整数 1、2、3、4。 用逗号（,）隔开的一系列值指定一个列表。譬如，3, 4, 6, 8 标明这四个指定的整数。 正斜线（/）可以用来指定间隔频率。在范围后加上 /&lt;integer&gt; 意味着在范围内可以跳过 integer。譬如，0-59/2 可以用来在分钟字段定义每两分钟。间隔频率值还可以和星号一起使用。例如，*/3 的值可以用在月份字段中表示每三个月运行一次任务。 开头为井号（#）的行是注释，不会被处理。 crontab命令主要有3个参数： -e ：编辑用户的crontab。 -l ：列出用户的crontab的内容。 -r ：删除用户的crontab的内容。 执行crontab -e，将自动打开编辑器，你可以编辑自己的crontab文件，语法和 /etc/crontab 文件一样，不同的只是，不必再指出执行的用户，编辑后保存即可。 crontab -l用来查看自己的crontab文件内能，crontab -r删除自己的crontab。]]></content>
      <categories>
        <category>cron</category>
        <category>定时</category>
      </categories>
      <tags>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台-ambari]]></title>
    <url>%2F2017%2F07%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0-ambari%2F</url>
    <content type="text"><![CDATA[前言Ambari 跟 Hadoop 等开源软件一样，也是 Apache Software Foundation 中的一个项目，并且是顶级项目。 Ambari 就是创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等 详见），而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。Ambari 现在所支持的平台组件也越来越多，例如流行的 Spark，Storm 等计算框架，以及资源调度平台 YARN 等，我们都能轻松地通过 Ambari 来进行部署。Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。 目前Ambari最新的发布版本是 2.5.1，下面的配置操作都是在centos7下进行的 安装ambari-server执行yum list ambari-server查看是否有ambari源，如果没有则在/etc/yum.repo.d/目录下，创建ambari源文件ambari.repo，并填写以下内容： 123456789#VERSION_NUMBER=2.5.1[Updates-ambari-2.5.1.0]name=ambari-2.5.1.0 - Updatesbaseurl=http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.5.1.0gpgcheck=1gpgkey=http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 其中的版本号可以改成你想要安装的且官网中存在的版本号,执行yum update更新源 再执行yum list ambari-server查看结果如下： 12345678 [jet@ambari yum.repos.d]$ yum list ambari-serverLoaded plugins: fastestmirror, refresh-packagekit, securityLoading mirror speeds from cached hostfile * base: mirror.bit.edu.cn * extras: mirrors.163.com * updates: mirrors.neusoft.edu.cnAvailable Packagesambari-server.x86_64 2.5.1.0-159 Updates-ambari-2.5.1.0 执行命令sudo yum install ambari-server开始安装 设置并启动ambari-server设置ambari-server，根据自己需要修改默认配置，注意如果选择postgresql数据库，不要选择4，要选择1，不然后面start的时候会报错，拒绝连接，没找到原因 1ambari-server setup 启动ambari-server： 1ambari-server start 由于我之前安装过2.2.2版本，这次运行时出错了，log文件里面错误如下： 1org.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version, serverVersion=2.5.1.0, schemaVersion=2.2.2.0 运行下面的命令进行升级后重新启动，成功。 1ambari-server upgrade 集群部署通过http://localhost:8080访问ambari web管理界面默认用户名密码是:admin/admin 如下图所示： 多台计算无密码登录-ssh强烈建议使用root账户配置 1.安装ssh. yum install openssh-server 安装完成后会在~目录（当前用户主目录，即这里的/root）下产生一个隐藏文件夹.ssh（ls -al 可以查看隐藏文件）。如果没有这个文件，自己新建即可（mkdir .ssh） 123456789101112131415161718[root@ambari ~]# ls -altotal 60dr-xr-x---. 6 root root 269 Aug 22 17:26 .dr-xr-xr-x. 18 root root 236 Apr 14 12:51 ..-rw-------. 1 root root 1441 Apr 14 12:17 anaconda-ks.cfg-rw-------. 1 root root 13050 Aug 22 17:20 .bash_history-rw-r--r--. 1 root root 18 Dec 29 2013 .bash_logout-rw-r--r--. 1 root root 176 Dec 29 2013 .bash_profile-rw-r--r--. 1 root root 176 Dec 29 2013 .bashrcdrwxr-xr-x. 3 root root 18 Apr 14 12:51 .cachedrwxr-xr-x. 3 root root 18 Apr 14 12:51 .config-rw-r--r--. 1 root root 100 Dec 29 2013 .cshrc-rw-r--r--. 1 root root 404 Aug 22 17:26 id_rsa.pubdrwxr-xr-x. 2 root root 40 Apr 20 14:27 .oracle_jre_usage-rw-------. 1 root root 247 Apr 17 18:38 .psql_history-rw-r--r--. 1 root root 6899 Aug 22 16:39 remove.sh-rw-------. 1 root root 1024 Apr 20 14:40 .rnddrwx------. 2 root root 80 Aug 22 17:27 .ssh 2.进入.ssh目录下面，在每台机器上执行：ssh-keygen -t rsa 之后一路回车，产生密钥； 1234[root@ambari .ssh]# lltotal 16-rw-------. 1 root root 1679 Aug 22 17:25 id_rsa-rw-r--r--. 1 root root 404 Aug 22 17:25 id_rsa.pub 3。完成第二步后会产生两个文件： id-rsa #私钥 id-rsa.pub #公钥 4.在第一台机器的目录.ssh下执行命令，cat id_rsa.pub &gt;&gt; authorized_keys；生成authorized_keys文件。 123[root@ambari .ssh]# lltotal 16-rw-r-----. 1 root root 404 Aug 22 17:26 authorized_keys 5.然后将第一台机器的.ssh目录下面的authorized_keys文件拷贝到第二台计算机的/root/.ssh目录下， 1scp authorized_keys root@hadoop02.hadoop.com:/root/.ssh/ 6.再转到第二台机器的.ssh目录下，会发现刚刚传输过来的文件authorized_keys，然后执行命令cat id-rsa.pub &gt;&gt; authorized_keys，将第二台计算机的公钥也加进来 7.将第二台计算机新生成的authorized_keys传输到第三台计算机，将第三台计算机的公钥id-rsa.pub添加到从第二台计算机传过来的authorized_keys里面。 8.依次类推，直至集群中的最后一台计算机。 9.在集群的最后一台计算机执行完添加后，生成的authorized_keys文件就包含集群中所有计算机的公钥，如果以后还有机器加进到集群中来，可以直接添加到文件authorized_keys。最后，将最后生成的authorized_keys复制到集群中的每一台计算机的.ssh目录下，覆盖掉之前的authorized_keys即可。 10.完成第九步后，就可以在集群中任意一台计算机上，免密码ssh登录到其他计算了。否则错误：Agent admitted failure to sign using the key解决：ssh-add ~/.ssh/id_rsa 如果需无密码登录到目标主机，只需要将自己的公钥id_rsa.pub添加到对应的目标主机的 .ssh/authorized_keys文件中即可，注意所属账户的.ssh,如以下是免密码登录到jet用户 1cat id_rsa.pub | ssh jet@10.111.24.144 &apos;cat - &gt;&gt; ~/.ssh/authorized_keys&apos; 一般以上几步就ok了，但我的仍要输入密码，用root用户登陆查看系统的日志文件：$ tail /var/log/secure -n 20 12345678…………Aug 22 10:26:43 MasterServer sshd[2734]: Authentication refused: bad ownership or modes for file /home/hadooper/.ssh/authorized_keysAug 22 10:26:48 MasterServer sshd[2734]: Accepted password for hadooper from ::1 port 37456 ssh2Aug 22 10:26:48 MasterServer sshd[2734]: pam_unix(sshd:session): session opened for user hadooper by (uid=0)Aug 22 10:36:30 MasterServer sshd[2809]: Accepted password for hadooper from 192.168.1.241 port 36257 ssh2Aug 22 10:36:30 MasterServer sshd[2809]: pam_unix(sshd:session): session opened for user hadooper by (uid=0)Aug 22 10:38:28 MasterServer sshd[2857]: Authentication refused: bad ownership or modes for directory /home/hadooper/.ssh………… 提示/home/root/.ssh和 /home/root/.ssh/authorized_keys权限不对，修改如下： 12chmod 700 /root/.sshchmod 640 /root/.ssh/authorized_keys 测试ssh root@hadoop02.hadoop.com第一次登录可能需要yes确认，之后就可以直接登录了。 下面是ambari界面安装添加集群过程，只介绍重要的可能遇到问题的部分 Install Options 【注意】1.Target Hosts 图中的hosts为各个客户机的hostname，一定要用域名的格式，配置方式如下两步： 集群中每台机器各自命名，最好添加ip号作为后缀，编辑/etc/sysconfig/network ambari-server192.168.40.112NETWORKING=yesHOSTNAME=ambari.hadoop.com ambari-agent192.168.40.212NETWORKING=yesHOSTNAME=hadoop02.hadoop.com ambari-agent192.168.40.312NETWORKING=yesHOSTNAME=hadoop03.hadoop.com 集群中每台机器都需要配置一样的，编辑文件/etc/hosts 192.168.40.1 192.168.40.2 192.168.40.3 ambari.hadoop.com12192.168.40.2 hadoop02.hadoop.com192.168.40.3 hadoop03.hadoop.com 2.Host Registration Information 注意这里是之前配置的免密码登录的用户的私钥 我前面是用root用户配置的，所以应该用/root/.ssh/id_rsa ssh user是root，ssh默认端口就是22 Hosts Check 一般就是图中的两个warning： 1.关闭防火墙 关闭防火墙 sudo service iptables stop 临时关闭命令关闭selinux 123456getenforcesetenforce 0[说明]setenforce 1 #设置SELinux 成为enforcing模式setenforce 0 #设置SELinux 成为permissive模式 或者直接修改配置文件关闭/etc/selinux/config(需要重启服务器) 1SELINUX=disabled 2.启动ntpd服务 12yum -y install ntpchkconfig ntpd on centos7服务模式变化 123/bin/systemctl start ntpd/bin/systemctl start chronyd/bin/systemctl stop firewalld 完成这两个操作以后，warning消失，如图 Choose Services选择需要安装的服务 Assign Masters分配主服务器 Assign slaves and Clients分配从服务器和客户端 控制面板 访问不了注意关闭防火墙 常见错误汇总错误信息123456789102017-08-22 21:09:18,495 - Skipping installation of existing package hdp-select2017-08-22 21:09:18,755 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf2017-08-22 21:09:18,757 - checked_call[&apos;hostid&apos;] &#123;&#125;2017-08-22 21:09:18,761 - checked_call returned (0, &apos;a8c00228&apos;)2017-08-22 21:09:18,763 - Package[&apos;ambari-metrics-monitor&apos;] &#123;&apos;retry_on_repo_unavailability&apos;: False, &apos;retry_count&apos;: 5&#125;2017-08-22 21:09:18,829 - Skipping installation of existing package ambari-metrics-monitor2017-08-22 21:09:18,830 - Package[&apos;ambari-metrics-hadoop-sink&apos;] &#123;&apos;retry_on_repo_unavailability&apos;: False, &apos;retry_count&apos;: 5&#125;2017-08-22 21:09:18,839 - Skipping installation of existing package ambari-metrics-hadoop-sink2017-08-22 21:09:18,840 - Package[&apos;ambari-metrics-grafana&apos;] &#123;&apos;retry_on_repo_unavailability&apos;: False, &apos;retry_count&apos;: 5&#125;2017-08-22 21:09:18,849 - Installing package ambari-metrics-grafana (&apos;/usr/bin/yum -d 0 -e 0 -y install ambari-metrics-grafana&apos;) 手动安装即可参考 1234567yum install ambari-metrics-monitor hdp-select -yyum install slider_2_6_1_0_129 hdp-select -yyum install spark_2_6_1_0_129 hdp-select -yyum install mysql-connector-java hdp-select -yyum install tez_2_6_1_0_129 hdp-select -yyum install zookeeper_2_6_1_0_129 hdp-select -yyum install zookeeper_2_6_1_0_129-server hdp-select -y 此版本新增了自动启动服务功能，再也不用手动去挨个点击启动了 在admin-services auto start去enable对应的服务即可，太方便了]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql配置详解]]></title>
    <url>%2F2017%2F07%2F10%2Fmysql%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[mysql配置文件my.cnf详解 mysqld程序：基本配置 basedir = path #使用给定目录作为根目录(安装目录)。 character-sets-dir = path #给出存放着字符集的目录。 datadir = path #从给定目录读取数据库文件。 pid-file = filename #为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统); Init-V脚本需要使用这个文件里的进程ID结束mysqld进程。 socket = filename #为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(仅适用于UNIX/Linux系统; 默认设置一般是/var/lib/mysql/mysql.sock文件)。在Windows环境下，如果MySQL客户与服务器是通过命名管道进行通信 的，–sock选项给出的将是该命名管道的名字(默认设置是MySQL)。 lower_case_table_name = 1/0 #新目录和数据表的名字是否只允许使用小写字母; 这个选项在Windows环境下的默认设置是1(只允许使用小写字母)。 mysqld程序：语言设置 character-sets-server = name #新数据库或数据表的默认字符集。为了与MySQL的早期版本保持兼容，这个字符集也可以用–default-character-set选项给出; 但这个选项已经显得有点过时了。 collation-server = name #新数据库或数据表的默认排序方式。 language = name #用指定的语言显示出错信息。 mysqld程序：通信、网络、信息安全 enable-named-pipes #允许Windows 2000/XP环境下的客户和服务器使用命名管道(named pipe)进行通信。这个命名管道的默认名字是MySQL，但可以用–socket选项来改变。 local-infile [=0] #允许/禁止使用LOAD DATA LOCAL语句来处理本地文件。 myisam-recover [=opt1, opt2, ...] 在启动时自动修复所有受损的MyISAM数据表。这个选项的可取值有4种:DEFAULT、BACKUP、QUICK和FORCE; 它们与myisamchk程序的同名选项作用相同。 old-passwords #使用MySQL 3.23和4.0版本中的老算法来加密mysql数据库里的密码(默认使用MySQL 4.1版本开始引入的新加密算法)。 port = n #为MySQL程序指定一个TCP/IP通信端口(通常是3306端口)。 safe-user-create #只有在mysql.user数据库表上拥有INSERT权限的用户才能使用GRANT命令; 这是一种双保险机制(此用户还必须具备GRANT权限才能执行GRANT命令)。 shared-memory #允许使用内存(shared memory)进行通信(仅适用于Windows)。 shared-memory-base-name = name #给共享内存块起一个名字(默认的名字是MySQL)。 skip-grant-tables #不使用mysql数据库里的信息来进行访问控制(警告:这将允许用户任何用户去修改任何数据库)。 skip-host-cache #不使用高速缓存区来存放主机名和IP地址的对应关系。 skip-name-resovle #不把IP地址解析为主机名; 与访问控制(mysql.user数据表)有关的检查全部通过IP地址行进。 skip-networking #只允许通过一个套接字文件(Unix/Linux系统)或通过命名管道(Windows系统)进行本地连接，不允许ICP/IP连接; 这提高了安全性，但阻断了来自网络的外部连接和所有的Java客户程序(Java客户即使在本地连接里也使用TCP/IP)。 user = name #mysqld程序在启动后将在给定UNIX/Linux账户下执行; mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序。 mysqld程序：内存管理、优化、查询缓存区 bulk_insert_buffer_size = n #为一次插入多条新记录的INSERT命令分配的缓存区长度(默认设置是8M)。 key_buffer_size = n #用来存放索引区块的RMA值(默认设置是8M)。 join_buffer_size = n #在参加JOIN操作的数据列没有索引时为JOIN操作分配的缓存区长度(默认设置是128K)。 max_heap_table_size = n #HEAP数据表的最大长度(默认设置是16M); 超过这个长度的HEAP数据表将被存入一个临时文件而不是驻留在内存里。 max_connections = n #MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。 query_cache_limit = n #允许临时存放在查询缓存区里的查询结果的最大长度(默认设置是1M)。 query_cache_size = n #查询缓存区的最大长度(默认设置是0，不开辟查询缓存区)。 query_cache_type = 0/1/2 #查询缓存区的工作模式:0, 禁用查询缓存区; 1，启用查询缓存区(默认设置); 2，”按需分配”模式，只响应SELECT SQL_CACHE命令。 read_buffer_size = n #为从数据表顺序读取数据的读操作保留的缓存区的长度(默认设置是128KB); 这个选项的设置值在必要时可以用SQL命令SET SESSION read_buffer_size = n命令加以改变。 read_rnd_buffer_size = n #类似于read_buffer_size选项，但针对的是按某种特定顺序(比如使用了ORDER BY子句的查询)输出的查询结果(默认设置是256K)。 sore_buffer = n #为排序操作分配的缓存区的长度(默认设置是2M); 如果这个缓存区太小，则必须创建一个临时文件来进行排序。 table_cache = n #同时打开的数据表的数量(默认设置是64)。 tmp_table_size = n #临时HEAP数据表的最大长度(默认设置是32M); 超过这个长度的临时数据表将被转换为MyISAM数据表并存入一个临时文件。 mysqld程序：日志 log [= file] #把所有的连接以及所有的SQL命令记入日志(通用查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname.log文件作为这种日志文件(hostname是服务器的主机名)。 log-slow-queries [= file] #把执行用时超过long_query_time变量值的查询命令记入日志(慢查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname-slow.log文件作为这种日志文件(hostname是服务器主机 名)。 long_query_time = n #慢查询的执行用时上限(默认设置是10s)。 long_queries_not_using_indexs #把慢查询以及执行时没有使用索引的查询命令全都记入日志(其余同–log-slow-queries选项)。 log-bin [= filename] #把对数据进行修改的所有SQL命令(也就是INSERT、UPDATE和DELETE命令)以二进制格式记入日志(二进制变更日志，binary update log)。这种日志的文件名是filename.n或默认的hostname.n，其中n是一个6位数字的整数(日志文件按顺序编号)。 log-bin-index = filename #二进制日志功能的索引文件名。在默认情况下，这个索引文件与二进制日志文件的名字相同，但后缀名是.index而不是.nnnnnn。 max_binlog_size = n #二进制日志文件的最大长度(默认设置是1GB)。在前一个二进制日志文件里的信息量超过这个最大长度之前，MySQL服务器会自动提供一个新的二进制日志文件接续上。 binlog-do-db = dbname #只把给定数 据库里的变化情况记入二进制日志文件，其他数据库里的变化情况不记载。如果需要记载多个数据库里的变化情况，就必须在配置文件使用多个本选项来设置，每个数据库一行。 binlog-ignore-db = dbname #不把给定数据库里的变化情况记入二进制日志文件。 sync_binlog = n #每经过n次日志写操作就把日志文件写入硬盘一次(对日志信息进行一次同步)。n=1是最安全的做法，但效率最低。默认设置是n=0，意思是由操作系统来负责二进制日志文件的同步工作。 log-update [= file] #记载出错情况的日志文件名(出错日志)。这种日志功能无法禁用。如果没有给出file参数，MySQL会使用hostname.err作为种日志文件的名字。 mysqld程序：镜像(主控镜像服务器) server-id = n #给服务器分配一个独一无二的ID编号; n的取值范围是1~2的32次方启用二进制日志功能。 log-bin = name #启用二进制日志功能。这种日志的文件名是filename.n或默认的hostname.n，其中的n是一个6位数字的整数(日志文件顺序编号)。 binlog-do/ignore-db = dbname #只把给定数据库里的变化情况记入二进制日志文件/不把给定的数据库里的变化记入二进制日志文件。 mysqld程序：镜像(从属镜像服务器) server-id = n #给服务器分配一个唯一的ID编号 log-slave-updates #启用从属服务器上的日志功能，使这台计算机可以用来构成一个镜像链(A-&gt;B-&gt;C)。 master-host = hostname #主控服务器的主机名或IP地址。如果从属服务器上存在mater.info文件(镜像关系定义文件)，它将忽略此选项。 master-user = replicusername #从属服务器用来连接主控服务器的用户名。如果从属服务器上存在mater.info文件，它将忽略此选项。 master-password = passwd #从属服务器用来连接主控服务器的密码。如果从属服务器上存在mater.info文件，它将忽略此选项。 master-port = n #从属服务器用来连接主控服务器的TCP/IP端口(默认设置是3306端口)。 master-connect-retry = n #如果与主控服务器的连接没有成功，则等待n秒(s)后再进行管理方式(默认设置是60s)。如果从属服务器存在mater.info文件，它将忽略此选项。 master-ssl-xxx = xxx #对主、从服务器之间的SSL通信进行配置。 read-only = 0/1 #0: 允许从属服务器独立地执行SQL命令(默认设置); 1: 从属服务器只能执行来自主控服务器的SQL命令。 read-log-purge = 0/1 #1: 把处理完的SQL命令立刻从中继日志文件里删除(默认设置); 0: 不把处理完的SQL命令立刻从中继日志文件里删除。 replicate-do-table = dbname.tablename 与–replicate-do-table选项的含义和用法相同，但数据库和数据库表名字里允许出现通配符”%” (例如: test%.%–对名字以”test”开头的所有数据库里的所以数据库表进行镜像处理)。 replicate-do-db = name #只对这个数据库进行镜像处理。 replicate-ignore-table = dbname.tablename #不对这个数据表进行镜像处理。 replicate-wild-ignore-table = dbn.tablen #不对这些数据表进行镜像处理。 replicate-ignore-db = dbname #不对这个数据库进行镜像处理。 replicate-rewrite-db = db1name &gt; db2name #把主控数据库上的db1name数据库镜像处理为从属服务器上的db2name数据库。 report-host = hostname #从属服务器的主机名; 这项信息只与SHOW SLAVE HOSTS命令有关–主控服务器可以用这条命令生成一份从属服务器的名单。 slave-compressed-protocol = 1 #主、从服务器使用压缩格式进行通信–如果它们都支持这么做的话。 slave-skip-errors = n1, n2, …或all #即使发生出错代码为n1、n2等的错误，镜像处理工作也继续进行(即不管发生什么错误，镜像处理工作也继续进行)。如果配置得当，从属服务器不应该在执行 SQL命令时发生错误(在主控服务器上执行出错的SQL命令不会被发送到从属服务器上做镜像处理); 如果不使用slave-skip-errors选项，从属服务器上的镜像工作就可能因为发生错误而中断，中断后需要有人工参与才能继续进行。 mysqld–InnoDB：基本设置、表空间文件 skip-innodb #不加载InnoDB数据表驱动程序–如果用不着InnoDB数据表，可以用这个选项节省一些内存。 innodb-file-per-table #为每一个新数据表创建一个表空间文件而不是把数据表都集中保存在中央表空间里(后者是默认设置)。该选项始见于MySQL 4.1。 innodb-open-file = n #InnoDB数据表驱动程序最多可以同时打开的文件数(默认设置是300)。如果使用了innodb-file-per-table选项并且需要同时打开很多数据表的话，这个数字很可能需要加大。 innodb_data_home_dir = p #InnoDB主目录，所有与InnoDB数据表有关的目录或文件路径都相对于这个路径。在默认的情况下，这个主目录就是MySQL的数据目录。 innodb_data_file_path = ts #用来容纳InnoDB为数据表的表空间: 可能涉及一个以上的文件; 每一个表空间文件的最大长度都必须以字节(B)、兆字节(MB)或千兆字节(GB)为单位给出; 表空间文件的名字必须以分号隔开; 最后一个表空间文件还可以带一个autoextend属性和一个最大长度(max:n)。例如，ibdata1:1G; ibdata2:1G:autoextend:max:2G的意思是: 表空间文件ibdata1的最大长度是1GB，ibdata2的最大长度也是1G，但允许它扩充到2GB。除文件名外，还可以用硬盘分区的设置名来定义表 空间，此时必须给表空间的最大初始长度值加上newraw关键字做后缀，给表空间的最大扩充长度值加上raw关键字做后缀(例如/dev/hdb1: 20Gnewraw或/dev/hdb1:20Graw); MySQL 4.0及更高版本的默认设置是ibdata1:10M:autoextend。 innodb_autoextend_increment = n #带有autoextend属性的表空间文件每次加大多少兆字节(默认设置是8MB)。这个属性不涉及具体的数据表文件，那些文件的增大速度相对是比较小的。 innodb_lock_wait_timeout = n #如果某个事务在等待n秒(s)后还没有获得所需要的资源，就使用ROLLBACK命令放弃这个事务。这项设置对于发现和处理未能被InnoDB数据表驱动 程序识别出来的死锁条件有着重要的意义。这个选项的默认设置是50s。 innodb_fast_shutdown 0/1 #是否以最快的速度关闭InnoDB，默认设置是1，意思是不把缓存在INSERT缓存区的数据写入数据表，那些数据将在MySQL服务器下次启动时再写入 (这么做没有什么风险，因为INSERT缓存区是表空间的一个组成部分，数据不会丢失)。把这个选项设置为0反面危险，因为在计算机关闭时，InnoDB 驱动程序很可能没有足够的时间完成它的数据同步工作，操作系统也许会在它完成数据同步工作之前强行结束InnoDB，而这会导致数据不完整。 mysqld程序：InnoDB–日志 innodb_log_group_home_dir = p #用来存放InnoDB日志文件的目录路径(如ib_logfile0、ib_logfile1等)。在默认的情况下，InnoDB驱动程序将使用 MySQL数据目录作为自己保存日志文件的位置。 innodb_log_files_in_group = n #使用多少个日志文件(默认设置是2)。InnoDB数据表驱动程序将以轮转方式依次填写这些文件; 当所有的日志文件都写满以后，之后的日志信息将写入第一个日志文件的最大长度(默认设置是5MB)。这个长度必须以MB(兆字节)或GB(千兆字节)为单 位进行设置。 innodb_flush_log_at_trx_commit = 0/1/2 #这个选项决定着什么时候把日志信息写入日志文件以及什么时候把这些文件物理地写(术语称为”同步”)到硬盘上。设置值0的意思是每隔一秒写一次日志并进行 同步，这可以减少硬盘写操作次数，但可能造成数据丢失; 设置值1(设置设置)的意思是在每执行完一条COMMIT命令就写一次日志并进行同步，这可以防止数据丢失，但硬盘写操作可能会很频繁; 设置值2是一般折衷的办法，即每执行完一条COMMIT命令写一次日志，每隔一秒进行一次同步。 innodb_flush_method = x #InnoDB日志文件的同步办法(仅适用于UNIX/Linux系统)。这个选项的可取值有两种: fdatasync，用fsync()函数进行同步; O_DSYNC，用O_SYNC()函数进行同步。 innodb_log_archive = 1 #启用InnoDB驱动程序的archive(档案)日志功能，把日志信息写入ib_arch_log_n文件。启用这种日志功能在InnoDB与 MySQL一起使用时没有多大意义(启用MySQL服务器的二进制日志功能就足够用了)。 mysqld程序–InnoDB：缓存区的设置和优化 innodb_log_buffer_pool_size = n #为InnoDB数据表及其索引而保留的RAM内存量(默认设置是8MB)。这个参数对速度有着相当大的影响，如果计算机上只运行有 MySQL/InnoDB数据库服务器，就应该把全部内存的80%用于这个用途。 innodb_log_buffer_size = n #事务日志文件写操作缓存区的最大长度(默认设置是1MB)。 innodb_additional_men_pool_size = n #为用于内部管理的各种数据结构分配的缓存区最大长度(默认设置是1MB)。 innodb_file_io_threads = n #I/O操作(硬盘写操作)的最大线程个数(默认设置是4)。 innodb_thread_concurrency = n #InnoDB驱动程序能够同时使用的最大线程个数(默认设置是8)。 mysqld程序：其它选项 bind-address = ipaddr #MySQL服务器的IP地址。如果MySQL服务器所在的计算机有多个IP地址，这个选项将非常重要。 default-storage-engine = type #新数据表的默认数据表类型(默认设置是MyISAM)。这项设置还可以通过–default-table-type选项来设置。 default-timezone = name #为MySQL服务器设置一个地理时区(如果它与本地计算机的地理时区不一样)。 ft_min_word_len = n #全文索引的最小单词长度工。这个选项的默认设置是4，意思是在创建全文索引时不考虑那些由3个或更少的字符构建单词。 Max-allowed-packet = n #客户与服务器之间交换的数据包的最大长度，这个数字至少应该大于客户程序将要处理的最大BLOB块的长度。这个选项的默认设置是1MB。 Sql-mode = model1, mode2, … #MySQL将运行在哪一种SQL模式下。这个选项的作用是让MySQL与其他的数据库系统保持最大程度的兼容。这个选项的可取值包括ansi、db2、 oracle、no_zero_date、pipes_as_concat。 mysql配置文件my.cnf详解2 配置MySQL服务器是一个丰富而复杂的工作。在本文中，我只能肤浅的说一下各种选项。可以使用的mysql配置文件共有５个。·/etc/my.cnf是默认的MySQL配置文件。应该对这个文件配置修改。它是为学习目的而设计的。·my-small.cnf是为了小型数据库而设计的。不应该把这个模型用于含有一些常用项目的数据库。·my-medium.cnf是为中等规模的数据库而设计的。如果你正在企业中使用RHEL,可能会比这个操作系统的最小RAM需求(256MB)明显多得多的物理内存。由此可见，如果有那么多RAM内存可以使用，自然可以在同一台机器上运行其它服务。·my-large.cnf是为专用于一个SQL数据库的计算机而设计的。由于它可以为该数据库使用多达512MB的内存，所以在这种类型的系统上将需要至少1GB的RAM,以便它能够同时处理操作系统与数据库应用程序。·my-huge.cnf是为企业中的数据库而设计的。这样的数据库要求专用服务器和1GB或1GB以上的RAM。这些选择高度依赖于内存的数量、计算机的运算速度、数据库的细节大小、访问数据库的用户数量以及在数据库中装入并访问数据的用户数量。随着数据库和用户的不断增加，数据库的性能可能会发生变化。我将逐个的说明这些配置文件。如果用户决定使用my-.cnf文件之一，将首先需要把这个文件复制到/etc/my.cnf文件上。由于这些原因，用户应该仔细观察数据库系统的性能。如果发现问题，可能需要增加更多的RAM，或者把数据库迁移到一个含有附加资源(比如多个CPU)的系统上。提示：数据库变得非常大。把一个SQL数据库目录配置在一个专用分区上可能更有道理。虽然一个不断增长的数据库可能会占满整个分区，但它至少不会吞掉RHEL运行所必需的磁盘空间。/etc/my.cnf文件默认是/etc/my.cnf文件。它包含6条命令，并且这6条命令被组织在3个配置段中。这些配置段与Samba配置文件中的配置段相似，并且含有功能组名称和相关的命令。本文将逐行的说明这个文件的默认版本。如果用户进行了任何修改，将需要确保MySQL启动脚本(即/etc/rc.d /init.d/mysqld)中的命令一致。[mysqld]在这个配置段之内，将会看到与MySQL守护进程相关的命令。datadir=/var/lib/mysqlMySQL服务器把数据库存储在由datadir变量所定义的目录中。Socket=/var/lib/mysql/mysql.sockMySQL套接字把数据库程序局部的或通过网络连接到MySQL客户。提示：MySQL被配置成使用InnoDB存储器引擎。如果用户在自己的系统上还没有一个InnoDB数据库，将需要给[mysqld]配置段添加skip-innodb语句。[mysql.server]在这个配置段之内，将会看到MySQL服务器守护进程有关的命令。这个配置段的较早期版本被命名为[mysql_server]。如果使用 MySQL4.X或MySQL4.X以上版本，将必须把这个配置段标题改成[mysql_server]。当启动MySQL服务时，它使用这个配置段中的选项。user=mysql与MySQL服务相关联的标准用户名是mysql。它应该是/etc/passwd文件的一部分；如果在这个文件中没有发现它，用户可能还没有安装Red Hat Enterprise Linux mysql-server RPM程序包。basedir=/var/lib这表示MySQL数据库的顶级目录。它充当MySQL系统上的一个根目录；这个数据库中的其它目录都是相对于这个目录。[safe_mysqld]这个配置段包含MySQL启动脚本所引用的命令。如果使用MySQL4.X或4.X以上版本，必须把这个配置段改成[mysqld_safe]。err-log=/var/log/mysqld.log这是MySQL所关联的错误被发送到的这个文件。如果使用MySQL4.X或4.X以上版本，必须使用log-error指令替换这条命令。pid-file=/var/run/mysqld/mysqld.pid最后，pid-file指令定义MySQL服务器在运作期间的进程标识符(PID)。如果MySQL服务器当前没有运行，这个文件应该不存在。提示：用户可以配置与用户特定相关的MySQL配置文件；为此，只需给指定用户主目录中的.my.cnf隐含文件添加所选的配置命令即可。my-samll-cnf在本文中，将说明my-small-cnf配置文本中的所有命令。当回顾其它MySQL样本配置文件时，将参考本文所解释的各条命令和指令的含义。先从下面这个配置段开始分析该文件中的有效命令和指令：[client]这个配置把指令传递给与MySQL服务器相关的客户。port＝3306MySQL所相关的标准TCP/IP端口是3306。如果需要修改这个端口号(可以增强安全)，必须确保用于MySQL客户与服务器的所有相应配置文件中均修改这个号。socket=/var/lib/mysql/mysql.sock正像默认的/etc/my.cnf文件中所定义的那样，这是控制MySQL客户与服务器间通信的标准套接字文件。[mysqld]当启动MySQL服务器时，它由[mysqld]配置段中所定义的命令来控制。port=3306socket=/var/lib/mysql/mysql.sock当然，与同一个MySQL数据库相关的客户与服务器需要使用相同的TCP/IP端口和套接字。skip-locking多个客户可能会访问同一个数据库，因此这防止外部客户锁定MySQL服务器。这个skip-locking命令是MySQL4.X或4.X以上版本中的skip-external-locking命令。一般来说，如果正在使用MySQL4.X或4.X上以版本，这个set-variable指令没有必要带有这个列表中的这些命令。set-variable=key_buffer=16k这个缓冲区确实很小；如果一个数据库在一个文本文件中包含不止几百行数据，它将会超载这个缓冲区的容量。这个数据库可能不会超载一个文本文件地址簿的容量。如果这不只是一个供个人使用的数据库，这个限额很快就会被达到。假使那样的话，可能需要考虑与其它配置文件之一相关的那些限额。set-variable=max_allowed_packet=1M当然，与一个数据库相关的信息会增加到超出实际数据。在默认的情况下，如果该信息在一个服务器上超过1MB以上，MySQL将会产生一条错误信息。set-variable=thread_stack=64k这条指令限定用于每个数据库线程的栈大小。默认设置足以满足大多数应用。set-variable=table_cache=4用户可以限定一个数据库中打开表的数量；越小的限额(默认值是64)适合越小规模的数据库。set-variable=sort_buffer=64k在处理一个数据库时，用户可能需要内存中附加的缓冲区空间。set-variable=net_buffer_length=2k正如net_buffer_length指令所定义的，MySQL服务器还给传入的请求保留了空间。server-id=1一般来说，如果有一个MySQL主服务器，应该把它的server-id设置成１；应该把MySQL从属服务器的server-id设置成２；[mysqldump]用户可以在不同类型的SQL数据库之间传输数据，这由[mysqldump]配置段中的命令来控制。quickquick选项支持较大数据库的转储。set-variable=max_allowed_packet=16M当然，用来传输数据库表到其它数据库的max_allowed_packet大于客户与服务器之间的简单通信所使用的信息包。[mysql]no-auto-rehash这个配置段设置启动MySQL服务的条件；在这种情况下，no-auto-rehash确保这个服务启动得比较快。[isamchk][myisamchk]像SQL这样的关系数据库用所谓的Indexed Sequential Access Method(索引顺序存取方法，简称ISAM)来处理。这两个配置段中的命令是相同的；这些命令与检查并修复数据库表的同名命令有关。set-variable=key_buffer=8Mset-variable=sort_buffer=8M在前面谈及MySQL服务器时，用户己经见过这些变量。它们在这里都比较大，以便支持数据库的较快速检查与修复。[mysqlhotcopy]interactive-timeout正如[mysqlhotcopy]配置段所指定的，在一个数据库复制操作期间，连接会挂起。在默认情况下，interactive-timeout变量把一个数据传输的最大时间量设置为28800秒(8个小时)。my-medium.cnf文件与中等数据库相关的MySQL配置文件含有和my-small-cnf配置文件中一样的有效配置段。在[mysqld]配置段中，下面这些命令支持较大规模的服务器数据库：set-variable=key_buffer=16Mset-variable=table_cache=64set-variable=sort_buffer=512Kset-variable=net_buffer_length=8Klog-bin一般来说，这个配置段中的命令支持服务器上的较大高速缓存与缓冲区长度。应该看到两条新命令。set-variable=myisam_sort_buffer_size=8Mlog-binmyisam_sort_buffer_size命令允许MySQL索引数据库，第二条命令支持二进制日志记录方法。[isamchk][myisamchk]当然，这两个配置段中的缓冲区比用于数据库传输的缓冲区大，这个文件包含下面这些命令；它们发送消息到服务器和接收来自服务器的消息。set-variable=read_buffer=2Mset-variable=write_buffer=2Mmy-large.cnf文件与较大型数据库相关的MySQL配置文件含有和my-samll-cnf配置文件中一样的有效配置段。在本文中，将比较my-large-cnf与my-medium-cnf样本文件中的各条命令。在[mysqld]配置段中，下面这些命令支持较大型的服务器数据库：set-variable=key_buffer=256Mset-variable=table_cache=256set-variable=sort_buffer=1Mset-variable=myisam_sort_buffer_size=64Mset-variable=net_buffer_length=8K这个配置段中有３条附加的命令。record_buffer命令保存对一个数据库中不同表的扫描结果。thread_cache命令对多请求有用；空闲线程被高速缓存起来，进而允许新的搜索操作采用己有的线程。只要这防止搜索操作启动新的服务器进程，这就能减轻系统上的负荷。set-variable=record_buffer=1Mset-variable=thread_cache=8set-variable=thread_concurrency=8thread_concurrency变量限定同时运行的线程数量。my-large.cnf样本文件建议用户应该把这个数量限定于本计算机上CPU数量的两倍；这个特定设置相当于４个CPU。*my-huge.cnf文件my-huge.cnf文件含有和my-large.cnf配置文件中一样的命令。当然，分配给大多数指令的值比较大并适合较大型的数据库。 mysql数据库引擎]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>my.cnf配置详解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库之mysql(二)]]></title>
    <url>%2F2017%2F07%2F10%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8Bmysql-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[mysql配置文件my.cnf详解1 mysqld程序：基本配置 basedir = path #使用给定目录作为根目录(安装目录)。 character-sets-dir = path #给出存放着字符集的目录。 datadir = path #从给定目录读取数据库文件。 pid-file = filename #为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统); Init-V脚本需要使用这个文件里的进程ID结束mysqld进程。 socket = filename #为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(仅适用于UNIX/Linux系统; 默认设置一般是/var/lib/mysql/mysql.sock文件)。在Windows环境下，如果MySQL客户与服务器是通过命名管道进行通信 的，–sock选项给出的将是该命名管道的名字(默认设置是MySQL)。 lower_case_table_name = 1/0 #新目录和数据表的名字是否只允许使用小写字母; 这个选项在Windows环境下的默认设置是1(只允许使用小写字母)。 mysqld程序：语言设置 character-sets-server = name #新数据库或数据表的默认字符集。为了与MySQL的早期版本保持兼容，这个字符集也可以用–default-character-set选项给出; 但这个选项已经显得有点过时了。 collation-server = name #新数据库或数据表的默认排序方式。 language = name #用指定的语言显示出错信息。 mysqld程序：通信、网络、信息安全 enable-named-pipes #允许Windows 2000/XP环境下的客户和服务器使用命名管道(named pipe)进行通信。这个命名管道的默认名字是MySQL，但可以用–socket选项来改变。 local-infile [=0] #允许/禁止使用LOAD DATA LOCAL语句来处理本地文件。 myisam-recover [=opt1, opt2, ...] 在启动时自动修复所有受损的MyISAM数据表。这个选项的可取值有4种:DEFAULT、BACKUP、QUICK和FORCE; 它们与myisamchk程序的同名选项作用相同。 old-passwords #使用MySQL 3.23和4.0版本中的老算法来加密mysql数据库里的密码(默认使用MySQL 4.1版本开始引入的新加密算法)。 port = n #为MySQL程序指定一个TCP/IP通信端口(通常是3306端口)。 safe-user-create #只有在mysql.user数据库表上拥有INSERT权限的用户才能使用GRANT命令; 这是一种双保险机制(此用户还必须具备GRANT权限才能执行GRANT命令)。 shared-memory #允许使用内存(shared memory)进行通信(仅适用于Windows)。 shared-memory-base-name = name #给共享内存块起一个名字(默认的名字是MySQL)。 skip-grant-tables #不使用mysql数据库里的信息来进行访问控制(警告:这将允许用户任何用户去修改任何数据库)。 skip-host-cache #不使用高速缓存区来存放主机名和IP地址的对应关系。 skip-name-resovle #不把IP地址解析为主机名; 与访问控制(mysql.user数据表)有关的检查全部通过IP地址行进。 skip-networking #只允许通过一个套接字文件(Unix/Linux系统)或通过命名管道(Windows系统)进行本地连接，不允许ICP/IP连接; 这提高了安全性，但阻断了来自网络的外部连接和所有的Java客户程序(Java客户即使在本地连接里也使用TCP/IP)。 user = name #mysqld程序在启动后将在给定UNIX/Linux账户下执行; mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序。 mysqld程序：内存管理、优化、查询缓存区 bulk_insert_buffer_size = n #为一次插入多条新记录的INSERT命令分配的缓存区长度(默认设置是8M)。 key_buffer_size = n #用来存放索引区块的RMA值(默认设置是8M)。 join_buffer_size = n #在参加JOIN操作的数据列没有索引时为JOIN操作分配的缓存区长度(默认设置是128K)。 max_heap_table_size = n #HEAP数据表的最大长度(默认设置是16M); 超过这个长度的HEAP数据表将被存入一个临时文件而不是驻留在内存里。 max_connections = n #MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。 query_cache_limit = n #允许临时存放在查询缓存区里的查询结果的最大长度(默认设置是1M)。 query_cache_size = n #查询缓存区的最大长度(默认设置是0，不开辟查询缓存区)。 query_cache_type = 0/1/2 #查询缓存区的工作模式:0, 禁用查询缓存区; 1，启用查询缓存区(默认设置); 2，”按需分配”模式，只响应SELECT SQL_CACHE命令。 read_buffer_size = n #为从数据表顺序读取数据的读操作保留的缓存区的长度(默认设置是128KB); 这个选项的设置值在必要时可以用SQL命令SET SESSION read_buffer_size = n命令加以改变。 read_rnd_buffer_size = n #类似于read_buffer_size选项，但针对的是按某种特定顺序(比如使用了ORDER BY子句的查询)输出的查询结果(默认设置是256K)。 sore_buffer = n #为排序操作分配的缓存区的长度(默认设置是2M); 如果这个缓存区太小，则必须创建一个临时文件来进行排序。 table_cache = n #同时打开的数据表的数量(默认设置是64)。 tmp_table_size = n #临时HEAP数据表的最大长度(默认设置是32M); 超过这个长度的临时数据表将被转换为MyISAM数据表并存入一个临时文件。 mysqld程序：日志 log [= file] #把所有的连接以及所有的SQL命令记入日志(通用查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname.log文件作为这种日志文件(hostname是服务器的主机名)。 log-slow-queries [= file] #把执行用时超过long_query_time变量值的查询命令记入日志(慢查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname-slow.log文件作为这种日志文件(hostname是服务器主机 名)。 long_query_time = n #慢查询的执行用时上限(默认设置是10s)。 long_queries_not_using_indexs #把慢查询以及执行时没有使用索引的查询命令全都记入日志(其余同–log-slow-queries选项)。 log-bin [= filename] #把对数据进行修改的所有SQL命令(也就是INSERT、UPDATE和DELETE命令)以二进制格式记入日志(二进制变更日志，binary update log)。这种日志的文件名是filename.n或默认的hostname.n，其中n是一个6位数字的整数(日志文件按顺序编号)。 log-bin-index = filename #二进制日志功能的索引文件名。在默认情况下，这个索引文件与二进制日志文件的名字相同，但后缀名是.index而不是.nnnnnn。 max_binlog_size = n #二进制日志文件的最大长度(默认设置是1GB)。在前一个二进制日志文件里的信息量超过这个最大长度之前，MySQL服务器会自动提供一个新的二进制日志文件接续上。 binlog-do-db = dbname #只把给定数 据库里的变化情况记入二进制日志文件，其他数据库里的变化情况不记载。如果需要记载多个数据库里的变化情况，就必须在配置文件使用多个本选项来设置，每个数据库一行。 binlog-ignore-db = dbname #不把给定数据库里的变化情况记入二进制日志文件。 sync_binlog = n #每经过n次日志写操作就把日志文件写入硬盘一次(对日志信息进行一次同步)。n=1是最安全的做法，但效率最低。默认设置是n=0，意思是由操作系统来负责二进制日志文件的同步工作。 log-update [= file] #记载出错情况的日志文件名(出错日志)。这种日志功能无法禁用。如果没有给出file参数，MySQL会使用hostname.err作为种日志文件的名字。 mysqld程序：镜像(主控镜像服务器) server-id = n #给服务器分配一个独一无二的ID编号; n的取值范围是1~2的32次方启用二进制日志功能。 log-bin = name #启用二进制日志功能。这种日志的文件名是filename.n或默认的hostname.n，其中的n是一个6位数字的整数(日志文件顺序编号)。 binlog-do/ignore-db = dbname #只把给定数据库里的变化情况记入二进制日志文件/不把给定的数据库里的变化记入二进制日志文件。 mysqld程序：镜像(从属镜像服务器) server-id = n #给服务器分配一个唯一的ID编号 log-slave-updates #启用从属服务器上的日志功能，使这台计算机可以用来构成一个镜像链(A-&gt;B-&gt;C)。 master-host = hostname #主控服务器的主机名或IP地址。如果从属服务器上存在mater.info文件(镜像关系定义文件)，它将忽略此选项。 master-user = replicusername #从属服务器用来连接主控服务器的用户名。如果从属服务器上存在mater.info文件，它将忽略此选项。 master-password = passwd #从属服务器用来连接主控服务器的密码。如果从属服务器上存在mater.info文件，它将忽略此选项。 master-port = n #从属服务器用来连接主控服务器的TCP/IP端口(默认设置是3306端口)。 master-connect-retry = n #如果与主控服务器的连接没有成功，则等待n秒(s)后再进行管理方式(默认设置是60s)。如果从属服务器存在mater.info文件，它将忽略此选项。 master-ssl-xxx = xxx #对主、从服务器之间的SSL通信进行配置。 read-only = 0/1 #0: 允许从属服务器独立地执行SQL命令(默认设置); 1: 从属服务器只能执行来自主控服务器的SQL命令。 read-log-purge = 0/1 #1: 把处理完的SQL命令立刻从中继日志文件里删除(默认设置); 0: 不把处理完的SQL命令立刻从中继日志文件里删除。 replicate-do-table = dbname.tablename 与–replicate-do-table选项的含义和用法相同，但数据库和数据库表名字里允许出现通配符”%” (例如: test%.%–对名字以”test”开头的所有数据库里的所以数据库表进行镜像处理)。 replicate-do-db = name #只对这个数据库进行镜像处理。 replicate-ignore-table = dbname.tablename #不对这个数据表进行镜像处理。 replicate-wild-ignore-table = dbn.tablen #不对这些数据表进行镜像处理。 replicate-ignore-db = dbname #不对这个数据库进行镜像处理。 replicate-rewrite-db = db1name &gt; db2name #把主控数据库上的db1name数据库镜像处理为从属服务器上的db2name数据库。 report-host = hostname #从属服务器的主机名; 这项信息只与SHOW SLAVE HOSTS命令有关–主控服务器可以用这条命令生成一份从属服务器的名单。 slave-compressed-protocol = 1 #主、从服务器使用压缩格式进行通信–如果它们都支持这么做的话。 slave-skip-errors = n1, n2, …或all #即使发生出错代码为n1、n2等的错误，镜像处理工作也继续进行(即不管发生什么错误，镜像处理工作也继续进行)。如果配置得当，从属服务器不应该在执行 SQL命令时发生错误(在主控服务器上执行出错的SQL命令不会被发送到从属服务器上做镜像处理); 如果不使用slave-skip-errors选项，从属服务器上的镜像工作就可能因为发生错误而中断，中断后需要有人工参与才能继续进行。 mysqld–InnoDB：基本设置、表空间文件 skip-innodb #不加载InnoDB数据表驱动程序–如果用不着InnoDB数据表，可以用这个选项节省一些内存。 innodb-file-per-table #为每一个新数据表创建一个表空间文件而不是把数据表都集中保存在中央表空间里(后者是默认设置)。该选项始见于MySQL 4.1。 innodb-open-file = n #InnoDB数据表驱动程序最多可以同时打开的文件数(默认设置是300)。如果使用了innodb-file-per-table选项并且需要同时打开很多数据表的话，这个数字很可能需要加大。 innodb_data_home_dir = p #InnoDB主目录，所有与InnoDB数据表有关的目录或文件路径都相对于这个路径。在默认的情况下，这个主目录就是MySQL的数据目录。 innodb_data_file_path = ts #用来容纳InnoDB为数据表的表空间: 可能涉及一个以上的文件; 每一个表空间文件的最大长度都必须以字节(B)、兆字节(MB)或千兆字节(GB)为单位给出; 表空间文件的名字必须以分号隔开; 最后一个表空间文件还可以带一个autoextend属性和一个最大长度(max:n)。例如，ibdata1:1G; ibdata2:1G:autoextend:max:2G的意思是: 表空间文件ibdata1的最大长度是1GB，ibdata2的最大长度也是1G，但允许它扩充到2GB。除文件名外，还可以用硬盘分区的设置名来定义表 空间，此时必须给表空间的最大初始长度值加上newraw关键字做后缀，给表空间的最大扩充长度值加上raw关键字做后缀(例如/dev/hdb1: 20Gnewraw或/dev/hdb1:20Graw); MySQL 4.0及更高版本的默认设置是ibdata1:10M:autoextend。 innodb_autoextend_increment = n #带有autoextend属性的表空间文件每次加大多少兆字节(默认设置是8MB)。这个属性不涉及具体的数据表文件，那些文件的增大速度相对是比较小的。 innodb_lock_wait_timeout = n #如果某个事务在等待n秒(s)后还没有获得所需要的资源，就使用ROLLBACK命令放弃这个事务。这项设置对于发现和处理未能被InnoDB数据表驱动 程序识别出来的死锁条件有着重要的意义。这个选项的默认设置是50s。 innodb_fast_shutdown 0/1 #是否以最快的速度关闭InnoDB，默认设置是1，意思是不把缓存在INSERT缓存区的数据写入数据表，那些数据将在MySQL服务器下次启动时再写入 (这么做没有什么风险，因为INSERT缓存区是表空间的一个组成部分，数据不会丢失)。把这个选项设置为0反面危险，因为在计算机关闭时，InnoDB 驱动程序很可能没有足够的时间完成它的数据同步工作，操作系统也许会在它完成数据同步工作之前强行结束InnoDB，而这会导致数据不完整。 mysqld程序：InnoDB–日志 innodb_log_group_home_dir = p #用来存放InnoDB日志文件的目录路径(如ib_logfile0、ib_logfile1等)。在默认的情况下，InnoDB驱动程序将使用 MySQL数据目录作为自己保存日志文件的位置。 innodb_log_files_in_group = n #使用多少个日志文件(默认设置是2)。InnoDB数据表驱动程序将以轮转方式依次填写这些文件; 当所有的日志文件都写满以后，之后的日志信息将写入第一个日志文件的最大长度(默认设置是5MB)。这个长度必须以MB(兆字节)或GB(千兆字节)为单 位进行设置。 innodb_flush_log_at_trx_commit = 0/1/2 #这个选项决定着什么时候把日志信息写入日志文件以及什么时候把这些文件物理地写(术语称为”同步”)到硬盘上。设置值0的意思是每隔一秒写一次日志并进行 同步，这可以减少硬盘写操作次数，但可能造成数据丢失; 设置值1(设置设置)的意思是在每执行完一条COMMIT命令就写一次日志并进行同步，这可以防止数据丢失，但硬盘写操作可能会很频繁; 设置值2是一般折衷的办法，即每执行完一条COMMIT命令写一次日志，每隔一秒进行一次同步。 innodb_flush_method = x #InnoDB日志文件的同步办法(仅适用于UNIX/Linux系统)。这个选项的可取值有两种: fdatasync，用fsync()函数进行同步; O_DSYNC，用O_SYNC()函数进行同步。 innodb_log_archive = 1 #启用InnoDB驱动程序的archive(档案)日志功能，把日志信息写入ib_arch_log_n文件。启用这种日志功能在InnoDB与 MySQL一起使用时没有多大意义(启用MySQL服务器的二进制日志功能就足够用了)。 mysqld程序–InnoDB：缓存区的设置和优化 innodb_log_buffer_pool_size = n #为InnoDB数据表及其索引而保留的RAM内存量(默认设置是8MB)。这个参数对速度有着相当大的影响，如果计算机上只运行有 MySQL/InnoDB数据库服务器，就应该把全部内存的80%用于这个用途。 innodb_log_buffer_size = n #事务日志文件写操作缓存区的最大长度(默认设置是1MB)。 innodb_additional_men_pool_size = n #为用于内部管理的各种数据结构分配的缓存区最大长度(默认设置是1MB)。 innodb_file_io_threads = n #I/O操作(硬盘写操作)的最大线程个数(默认设置是4)。 innodb_thread_concurrency = n #InnoDB驱动程序能够同时使用的最大线程个数(默认设置是8)。 mysqld程序：其它选项 bind-address = ipaddr #MySQL服务器的IP地址。如果MySQL服务器所在的计算机有多个IP地址，这个选项将非常重要。 default-storage-engine = type #新数据表的默认数据表类型(默认设置是MyISAM)。这项设置还可以通过–default-table-type选项来设置。 default-timezone = name #为MySQL服务器设置一个地理时区(如果它与本地计算机的地理时区不一样)。 ft_min_word_len = n #全文索引的最小单词长度工。这个选项的默认设置是4，意思是在创建全文索引时不考虑那些由3个或更少的字符构建单词。 Max-allowed-packet = n #客户与服务器之间交换的数据包的最大长度，这个数字至少应该大于客户程序将要处理的最大BLOB块的长度。这个选项的默认设置是1MB。 Sql-mode = model1, mode2, … #MySQL将运行在哪一种SQL模式下。这个选项的作用是让MySQL与其他的数据库系统保持最大程度的兼容。这个选项的可取值包括ansi、db2、 oracle、no_zero_date、pipes_as_concat。 mysql配置文件my.cnf详解2 配置MySQL服务器是一个丰富而复杂的工作。在本文中，我只能肤浅的说一下各种选项。可以使用的mysql配置文件共有５个。·/etc/my.cnf是默认的MySQL配置文件。应该对这个文件配置修改。它是为学习目的而设计的。·my-small.cnf是为了小型数据库而设计的。不应该把这个模型用于含有一些常用项目的数据库。·my-medium.cnf是为中等规模的数据库而设计的。如果你正在企业中使用RHEL,可能会比这个操作系统的最小RAM需求(256MB)明显多得多的物理内存。由此可见，如果有那么多RAM内存可以使用，自然可以在同一台机器上运行其它服务。·my-large.cnf是为专用于一个SQL数据库的计算机而设计的。由于它可以为该数据库使用多达512MB的内存，所以在这种类型的系统上将需要至少1GB的RAM,以便它能够同时处理操作系统与数据库应用程序。·my-huge.cnf是为企业中的数据库而设计的。这样的数据库要求专用服务器和1GB或1GB以上的RAM。这些选择高度依赖于内存的数量、计算机的运算速度、数据库的细节大小、访问数据库的用户数量以及在数据库中装入并访问数据的用户数量。随着数据库和用户的不断增加，数据库的性能可能会发生变化。我将逐个的说明这些配置文件。如果用户决定使用my-.cnf文件之一，将首先需要把这个文件复制到/etc/my.cnf文件上。由于这些原因，用户应该仔细观察数据库系统的性能。如果发现问题，可能需要增加更多的RAM，或者把数据库迁移到一个含有附加资源(比如多个CPU)的系统上。提示：数据库变得非常大。把一个SQL数据库目录配置在一个专用分区上可能更有道理。虽然一个不断增长的数据库可能会占满整个分区，但它至少不会吞掉RHEL运行所必需的磁盘空间。/etc/my.cnf文件默认是/etc/my.cnf文件。它包含6条命令，并且这6条命令被组织在3个配置段中。这些配置段与Samba配置文件中的配置段相似，并且含有功能组名称和相关的命令。本文将逐行的说明这个文件的默认版本。如果用户进行了任何修改，将需要确保MySQL启动脚本(即/etc/rc.d /init.d/mysqld)中的命令一致。[mysqld]在这个配置段之内，将会看到与MySQL守护进程相关的命令。datadir=/var/lib/mysqlMySQL服务器把数据库存储在由datadir变量所定义的目录中。Socket=/var/lib/mysql/mysql.sockMySQL套接字把数据库程序局部的或通过网络连接到MySQL客户。提示：MySQL被配置成使用InnoDB存储器引擎。如果用户在自己的系统上还没有一个InnoDB数据库，将需要给[mysqld]配置段添加skip-innodb语句。[mysql.server]在这个配置段之内，将会看到MySQL服务器守护进程有关的命令。这个配置段的较早期版本被命名为[mysql_server]。如果使用 MySQL4.X或MySQL4.X以上版本，将必须把这个配置段标题改成[mysql_server]。当启动MySQL服务时，它使用这个配置段中的选项。user=mysql与MySQL服务相关联的标准用户名是mysql。它应该是/etc/passwd文件的一部分；如果在这个文件中没有发现它，用户可能还没有安装Red Hat Enterprise Linux mysql-server RPM程序包。basedir=/var/lib这表示MySQL数据库的顶级目录。它充当MySQL系统上的一个根目录；这个数据库中的其它目录都是相对于这个目录。[safe_mysqld]这个配置段包含MySQL启动脚本所引用的命令。如果使用MySQL4.X或4.X以上版本，必须把这个配置段改成[mysqld_safe]。err-log=/var/log/mysqld.log这是MySQL所关联的错误被发送到的这个文件。如果使用MySQL4.X或4.X以上版本，必须使用log-error指令替换这条命令。pid-file=/var/run/mysqld/mysqld.pid最后，pid-file指令定义MySQL服务器在运作期间的进程标识符(PID)。如果MySQL服务器当前没有运行，这个文件应该不存在。提示：用户可以配置与用户特定相关的MySQL配置文件；为此，只需给指定用户主目录中的.my.cnf隐含文件添加所选的配置命令即可。my-samll-cnf在本文中，将说明my-small-cnf配置文本中的所有命令。当回顾其它MySQL样本配置文件时，将参考本文所解释的各条命令和指令的含义。先从下面这个配置段开始分析该文件中的有效命令和指令：[client]这个配置把指令传递给与MySQL服务器相关的客户。port＝3306MySQL所相关的标准TCP/IP端口是3306。如果需要修改这个端口号(可以增强安全)，必须确保用于MySQL客户与服务器的所有相应配置文件中均修改这个号。socket=/var/lib/mysql/mysql.sock正像默认的/etc/my.cnf文件中所定义的那样，这是控制MySQL客户与服务器间通信的标准套接字文件。[mysqld]当启动MySQL服务器时，它由[mysqld]配置段中所定义的命令来控制。port=3306socket=/var/lib/mysql/mysql.sock当然，与同一个MySQL数据库相关的客户与服务器需要使用相同的TCP/IP端口和套接字。skip-locking多个客户可能会访问同一个数据库，因此这防止外部客户锁定MySQL服务器。这个skip-locking命令是MySQL4.X或4.X以上版本中的skip-external-locking命令。一般来说，如果正在使用MySQL4.X或4.X上以版本，这个set-variable指令没有必要带有这个列表中的这些命令。set-variable=key_buffer=16k这个缓冲区确实很小；如果一个数据库在一个文本文件中包含不止几百行数据，它将会超载这个缓冲区的容量。这个数据库可能不会超载一个文本文件地址簿的容量。如果这不只是一个供个人使用的数据库，这个限额很快就会被达到。假使那样的话，可能需要考虑与其它配置文件之一相关的那些限额。set-variable=max_allowed_packet=1M当然，与一个数据库相关的信息会增加到超出实际数据。在默认的情况下，如果该信息在一个服务器上超过1MB以上，MySQL将会产生一条错误信息。set-variable=thread_stack=64k这条指令限定用于每个数据库线程的栈大小。默认设置足以满足大多数应用。set-variable=table_cache=4用户可以限定一个数据库中打开表的数量；越小的限额(默认值是64)适合越小规模的数据库。set-variable=sort_buffer=64k在处理一个数据库时，用户可能需要内存中附加的缓冲区空间。set-variable=net_buffer_length=2k正如net_buffer_length指令所定义的，MySQL服务器还给传入的请求保留了空间。server-id=1一般来说，如果有一个MySQL主服务器，应该把它的server-id设置成１；应该把MySQL从属服务器的server-id设置成２；[mysqldump]用户可以在不同类型的SQL数据库之间传输数据，这由[mysqldump]配置段中的命令来控制。quickquick选项支持较大数据库的转储。set-variable=max_allowed_packet=16M当然，用来传输数据库表到其它数据库的max_allowed_packet大于客户与服务器之间的简单通信所使用的信息包。[mysql]no-auto-rehash这个配置段设置启动MySQL服务的条件；在这种情况下，no-auto-rehash确保这个服务启动得比较快。[isamchk][myisamchk]像SQL这样的关系数据库用所谓的Indexed Sequential Access Method(索引顺序存取方法，简称ISAM)来处理。这两个配置段中的命令是相同的；这些命令与检查并修复数据库表的同名命令有关。set-variable=key_buffer=8Mset-variable=sort_buffer=8M在前面谈及MySQL服务器时，用户己经见过这些变量。它们在这里都比较大，以便支持数据库的较快速检查与修复。[mysqlhotcopy]interactive-timeout正如[mysqlhotcopy]配置段所指定的，在一个数据库复制操作期间，连接会挂起。在默认情况下，interactive-timeout变量把一个数据传输的最大时间量设置为28800秒(8个小时)。my-medium.cnf文件与中等数据库相关的MySQL配置文件含有和my-small-cnf配置文件中一样的有效配置段。在[mysqld]配置段中，下面这些命令支持较大规模的服务器数据库：set-variable=key_buffer=16Mset-variable=table_cache=64set-variable=sort_buffer=512Kset-variable=net_buffer_length=8Klog-bin一般来说，这个配置段中的命令支持服务器上的较大高速缓存与缓冲区长度。应该看到两条新命令。set-variable=myisam_sort_buffer_size=8Mlog-binmyisam_sort_buffer_size命令允许MySQL索引数据库，第二条命令支持二进制日志记录方法。[isamchk][myisamchk]当然，这两个配置段中的缓冲区比用于数据库传输的缓冲区大，这个文件包含下面这些命令；它们发送消息到服务器和接收来自服务器的消息。set-variable=read_buffer=2Mset-variable=write_buffer=2Mmy-large.cnf文件与较大型数据库相关的MySQL配置文件含有和my-samll-cnf配置文件中一样的有效配置段。在本文中，将比较my-large-cnf与my-medium-cnf样本文件中的各条命令。在[mysqld]配置段中，下面这些命令支持较大型的服务器数据库：set-variable=key_buffer=256Mset-variable=table_cache=256set-variable=sort_buffer=1Mset-variable=myisam_sort_buffer_size=64Mset-variable=net_buffer_length=8K这个配置段中有３条附加的命令。record_buffer命令保存对一个数据库中不同表的扫描结果。thread_cache命令对多请求有用；空闲线程被高速缓存起来，进而允许新的搜索操作采用己有的线程。只要这防止搜索操作启动新的服务器进程，这就能减轻系统上的负荷。set-variable=record_buffer=1Mset-variable=thread_cache=8set-variable=thread_concurrency=8thread_concurrency变量限定同时运行的线程数量。my-large.cnf样本文件建议用户应该把这个数量限定于本计算机上CPU数量的两倍；这个特定设置相当于４个CPU。*my-huge.cnf文件my-huge.cnf文件含有和my-large.cnf配置文件中一样的命令。当然，分配给大多数指令的值比较大并适合较大型的数据库。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google-cloud搭建vpn]]></title>
    <url>%2F2017%2F07%2F09%2Fgoogle-cloud%E6%90%AD%E5%BB%BAvpn%2F</url>
    <content type="text"><![CDATA[引言由于公司需要，而老板又不舍得花钱，于是使用google cloud免费vps，搭建vpn。google cloud需要翻墙访问，有gmail账户，单币种美金信用卡(好像有些双币种信用卡也可以，具体没试过)免费试用1年，赠送300$，所以申请vps时，用最低配置，每个月接近5$，300$可以支撑一年，但是流量是收费的，所以流量很大情况下，可能用不到一年。下面就是具体配置了，分两版：centos和ubuntu Google Cloud配置建立好Compute Engine后，申请一个永久静态地址，然后配置SSH登录证书等内容。详情查看Google文档如果实在不想看文档，可以看这里 主要配置是在Google的Firewall rules里，这里决定了什么包可以访问到内部的instance，其后才是instance内部的iptables的配置。 这里我们建立一条新的rule，source filter填写0.0.0.0/0，代表任何地方发出的包，除非你只想在特定IP上访问，否则就填写这个。然后在Allowed protocols and ports里添加udp:500;udp:4500;esp。 这里udp:500是IPSEC协议指定的端口，会通过这个端口来发送Control plane的包 udp:4500是当source或者destination在NAT后时转而使用的端口，另外当NAT被检测到后，Data plane的数据包也会被使用端口，这个时候ESP的数据包会被包含在UDP包中，端口也是这个 最后esp就是允许Data plane的数据包通过 另外需要勾选允许ip forwarding。 安装StrongSwan从源文件编译安装安装PAM库和SSL库，以及make和gcc1234567#ubuntuapt-get updateapt-get install libpam0g-dev libssl-dev make gcc#centosyum -y updateyum -y install pam-devel openssl-devel make gcc 下载StrongSwan的源码并编译12345678910111213141516171819202122232425#下载wget http://download.strongswan.org/strongswan.tar.gztar xzf strongswan.tar.gzcd strongswan-*#生成 Makefile为编译做准备#OpenVZ使用以下参数./configure --enable-eap-identity --enable-eap-md5 \--enable-eap-mschapv2 --enable-eap-tls --enable-eap-ttls --enable-eap-peap \--enable-eap-tnc --enable-eap-dynamic --enable-eap-radius --enable-xauth-eap \--enable-xauth-pam --enable-dhcp --enable-openssl --enable-addrblock --enable-unity \--enable-certexpire --enable-radattr --enable-tools --enable-openssl --disable-gmp --enable-kernel-libipsec#其它服务器执行./configure --prefix=/usr --sysconfdir=/etc/strongswan --enable-eap-identity \--enable-eap-md5 --enable-eap-mschapv2 --enable-eap-tls --enable-eap-ttls \-- enable-eap-peap --enable-eap-tnc --enable-eap-dynamic \--enable-eap-radius --enable-xauth-eap --enable-xauth-pam --enable-dhcp \--enable-addrblock --enable-unity --enable-certexpire --enable-radattr \--enable-openssl --disable-gmp#编译并且安装make&amp;&amp;make install#生成ipsec和strongswan命令mv /usr/sbin/&#123;ipsec,strongswan&#125; 判断VPS是Openvz还是KVM还是Xen1.通过系统上的相关目录或文件判断执行：ls /proc/ ，一般Xen的VPS，/proc目录下面会有xen的目录，openvz的会有vz目录。2.执行：free -m 看内存，openvz的没有swap，当然也有xen的没有swap，但是xen的是可以加的，openvz不行。3.执行：uname -a 有些xen的VPS里面会显示有xen。4.执行：ifconfig 查看网卡，openvz的一般都是venet0: ，xen的一般都是eth。5.通过VPS控制面板查看，像SolusVM、vePortal控制面板上都显示虚拟技术。 完成后执行ipsec version查看是否安装成功。 生成证书生成CA私钥1ipsec pki --gen --outform pem &gt; ca.pem 利用私钥签名CA证书1ipsec pki --self --in ca.pem --dn &quot;C=com, O=myvpn, CN=VPN CA&quot; --ca --outform pem &gt;ca.cert.pem 生成server端私钥1ipsec pki --gen --outform pem &gt; server.pem 用CA证书签发server端证书这里需要将下面的地址更换为google cloud中申请到的静态公网ip地址。 12345SERVER_ADDR=&quot;[REPLACE_WITH_YOUR_OWN_IP_ADDRESS]&quot;ipsec pki --pub --in server.pem | ipsec pki --issue --cacert ca.cert.pem \--cakey ca.pem --dn &quot;C=com, O=myvpn, CN=$SERVER_ADDR&quot; \--san=&quot;$SERVER_ADDR&quot; --flag serverAuth --flag ikeIntermediate \--outform pem &gt; server.cert.pem 生成client端私钥1ipsec pki --gen --outform pem &gt; client.pem 利用CA证书签发client端证书1ipsec pki --pub --in client.pem | ipsec pki --issue --cacert ca.cert.pem --cakey ca.pem --dn &quot;C=com, O=myvpn, CN=VPN Client&quot; --outform pem &gt; client.cert.pem 生成client端p12证书1openssl pkcs12 -export -inkey client.pem -in client.cert.pem -name &quot;client&quot; -certfile ca.cert.pem -caname &quot;VPN CA&quot; -out client.cert.p12 安装证书1234567891011121314# if not root probably you need to prepend sudo in front of the following commands#for ubuntu cp -r ca.cert.pem /usr/local/etc/ipsec.d/cacerts/cp -r server.cert.pem /usr/local/etc/ipsec.d/certs/cp -r server.pem /usr/local/etc/ipsec.d/private/cp -r client.cert.pem /usr/local/etc/ipsec.d/certs/cp -r client.pem /usr/local/etc/ipsec.d/private/#for centoscp -r ca.cert.pem /etc/strongswan/ipsec.d/cacerts/cp -r server.cert.pem /etc/strongswan/ipsec.d/certs/cp -r server.pem /etc/strongswan/ipsec.d/private/cp -r client.cert.pem /etc/strongswan/ipsec.d/certs/cp -r client.pem /etc/strongswan/ipsec.d/private/ 配置StrongSwan配置ipsec.confubuntu:/usr/local/etc/ipsec.confcentos:/etc/strongswan/ipsec.conf替换或新添加为如下内容(rightsourceip为申请的静态公网ip地址)：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455config setup uniqueids=neverconn iOS_cert keyexchange=ikev1 # strongswan version &gt;= 5.0.2, compatible with iOS 6.0,6.0.1 fragmentation=yes left=%defaultroute leftauth=pubkey leftsubnet=0.0.0.0/0 leftcert=server.cert.pem right=%any rightauth=pubkey rightauth2=xauth rightsourceip=10.31.2.0/24 rightcert=client.cert.pem auto=addconn android_xauth_psk keyexchange=ikev1 left=%defaultroute leftauth=psk leftsubnet=0.0.0.0/0 right=%any rightauth=psk rightauth2=xauth rightsourceip=10.31.2.0/24 auto=addconn networkmanager-strongswan keyexchange=ikev2 left=%defaultroute leftauth=pubkey leftsubnet=0.0.0.0/0 leftcert=server.cert.pem right=%any rightauth=pubkey rightsourceip=10.31.2.0/24 rightcert=client.cert.pem auto=addconn windows7 keyexchange=ikev2 ike=aes256-sha1-modp1024! rekey=no left=%defaultroute leftauth=pubkey leftsubnet=0.0.0.0/0 leftcert=server.cert.pem right=%any rightauth=eap-mschapv2 rightsourceip=10.31.2.0/24 rightsendcert=never eap_identity=%any auto=add 配置strongswan.confubuntu:/usr/local/etc/ipsec.confcentos:/etc/strongswan/ipsec.conf替换或新添加为如下内容：12345678910111213charon &#123; load_modular = yes duplicheck.enable = no compress = yes plugins &#123; include strongswan.d/charon/*.conf &#125; dns1 = 8.8.8.8 dns2 = 8.8.4.4 nbns1 = 8.8.8.8 nbns2 = 8.8.4.4 &#125; include strongswan.d/*.conf 配置ipsec.secretsubuntu:/usr/local/etc/ipsec.confcentos:/etc/strongswan/ipsec.conf替换或新添加为如下内容：1234: RSA server.pem: PSK &quot;mykey&quot;: XAUTH &quot;mykey&quot;[用户名] %any : EAP &quot;[密码]&quot; 注意将PSK、XAUTH处的”mykey”编辑为唯一且私密的字符串，并且将[用户名]改为自己想要的登录名，[密码]改为自己想要的密码（[]符号去掉），可以添加多行，得到多个用户。 配置iptables修改系统转发sysctrl.conf打开/etc/sysctl.conf，然后uncomment包含net.ipv4.ip_forward=1的这一行。 保存后，执行sysctl -p。 修改iptables将INF替换为自己的网络接口.12345678910111213INF=&quot;Your own network interface&quot;iptables -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A FORWARD -s 10.31.2.0/24 -j ACCEPTiptables -A INPUT -i $INF -p esp -j ACCEPTiptables -A INPUT -i $INF -p udp --dport 500 -j ACCEPTiptables -A INPUT -i $INF -p tcp --dport 500 -j ACCEPTiptables -A INPUT -i $INF -p udp --dport 4500 -j ACCEPT# for l2tpiptables -A INPUT -i $INF -p udp --dport 1701 -j ACCEPT# for pptpiptables -A INPUT -i $INF -p tcp --dport 1723 -j ACCEPTiptables -A FORWARD -j REJECTiptables -t nat -A POSTROUTING -s 10.31.2.0/24 -o $INF -j MASQUERADE 【注意】：1.此处$INF为服务器网卡设备，对于OpenVZ主机请添venet0，其他主机添eth0，具体通过ifconfig查看使用的那个网卡2.此处ip：10.31.2.0/24仅举例，必须与上文ipsec.conf中的设置保持一致，但可是设置多对。 保存iptables且开机自动启动12345678910#for ubuntuiptables-save &gt; /etc/iptables.rulescat &gt; /etc/network/if-up.d/iptables&lt;&lt;EOF#!/bin/shiptables-restore &lt; /etc/iptables.rulesEOFchmod +x /etc/network/if-up.d/iptables#for centosservice iptables save 重启ipsec/iptables/strongswan服务123service iptables restartservice strongswan restartipsec restart 至此分讲完成。 WP8.1手机安装ca.cert.pem，进入设置VPN添加IKEv2连接，地址为证书中的地址或IP，通过用户名-密码连接。Windows连接也是一样，但注意将证书导入本地计算机而不是当前用户的“受信任的证书颁发机构”。iOS/Android/Mac OS X设备添加Cisco IPSec PSK验证方式，预共享密钥是/usr/local/etc/ipsec.secrets或者/etc/strongswan/ipsec.secrets中PSK后的字符串（不含引号），用户名密码同上，可以通过任意域名或IP连接，不需要证书. 自动化安装脚本下面附上自动化安装脚本，点我获取，如若有问题请创建issue沟通。]]></content>
      <categories>
        <category>翻墙</category>
      </categories>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-IO]]></title>
    <url>%2F2017%2F07%2F08%2Fjava-IO%2F</url>
    <content type="text"><![CDATA[Java IOJava IO即Java 输入输出系统。不管我们编写何种应用，都难免和各种输入输出相关的媒介打交道，其实和媒介进行IO的过程是十分复杂的，这要考虑的因素特别多，比如我们要考虑和哪种媒介进行IO（文件、控制台、网络），我们还要考虑具体和它们的通信方式（顺序、随机、二进制、按字符、按字、按行等等）。Java类库的设计者通过设计大量的类来攻克这些难题，这些类就位于java.io包中。 在JDK1.4之后，为了提高Java IO的效率，Java又提供了一套新的IO，Java New IO简称Java NIO。它在标准java代码中提供了高速的面向块的IO操作。本篇文章重点介绍Java IO，关于Java NIO请参考我的另两篇文章：Java NIO详解(一)Java NIO详解(二) 流在Java IO中，流是一个核心的概念。流从概念上来说是一个连续的数据流。你既可以从流中读取数据，也可以往流中写数据。流与数据源或者数据流向的媒介相关联。在Java IO中流既可以是字节流(以字节为单位进行读写)，也可以是字符流(以字符为单位进行读写)。 IO相关的媒介Java的IO包主要关注的是从原始数据源的读取以及输出原始数据到目标媒介。以下是最典型的数据源和目标媒介： 文件管道网络连接内存缓存System.in, System.out, System.error(注：Java标准输入、输出、错误输出) Java IO类库的框架Java IO的类型虽然java IO类库庞大，但总体来说其框架还是很清楚的。从是读媒介还是写媒介的维度看，Java IO可以分为： 输入流：InputStream和Reader输出流：OutputStream和Writer而从其处理流的类型的维度上看，Java IO又可以分为： 字节流：InputStream和OutputStream字符流：Reader和Writer下面这幅图就清晰的描述了JavaIO的分类： - 字节流 字符流 输入流 InputStream Reader 输出流 OutputStream Writer 我们的程序需要通过InputStream或Reader从数据源读取数据，然后用OutputStream或者Writer将数据写入到目标媒介中。其中，InputStream和Reader与数据源相关联，OutputStream和writer与目标媒介相关联。 以下的图说明了这一点： IO 类库上面我们介绍了Java IO中的四各类：InputStream、OutputStream、Reader、Writer，其实在我们的实际应用中，我们用到的一般是它们的子类，之所以设计这么多子类，目的就是让每一个类都负责不同的功能，以方便我们开发各种应用。各类用途汇总如下： 文件访问网络访问内存缓存访问线程内部通信(管道)缓冲过滤解析读写文本 (Readers / Writers)读写基本类型数据 (long, int etc.)读写对象下面我们就通过两张图来大体了解一下这些类的继承关系及其作用 图1：java io 类的集成关系 图2：java io中各个类所负责的媒介 Java IO的基本用法字节流通过上面的介绍我们已经知道，字节流对应的类应该是InputStream和OutputStream，而在我们实际开发中，我们应该根据不同的媒介类型选用相应的子类来处理。下面我们就用字节流来操作文件媒介： 例1，用字节流写文件 12345678910public static void writeByteToFile() throws IOException&#123; String hello= new String( &quot;hello word!&quot;); byte[] byteArray= hello.getBytes(); File file= new File( &quot;d:/test.txt&quot;); //因为是用字节流来写媒介，所以对应的是OutputStream //又因为媒介对象是文件，所以用到子类是FileOutputStream OutputStream os= new FileOutputStream( file); os.write( byteArray); os.close();&#125; 例2，用字节流读文件 12345678910public static void readByteFromFile() throws IOException&#123; File file= new File( &quot;d:/test.txt&quot;); byte[] byteArray= new byte[( int) file.length()]; //因为是用字节流来读媒介，所以对应的是InputStream //又因为媒介对象是文件，所以用到子类是FileInputStream InputStream is= new FileInputStream( file); int size= is.read( byteArray); System. out.println( &quot;大小:&quot;+size +&quot;;内容:&quot; +new String(byteArray)); is.close(); &#125; 字符流同样，字符流对应的类应该是Reader和Writer。下面我们就用字符流来操作文件媒介: 例3，用字符流读文件 12345678public static void writeCharToFile() throws IOException&#123; String hello= new String( &quot;hello word!&quot;); File file= new File( &quot;d:/test.txt&quot;); //因为是用字符流来读媒介，所以对应的是Writer，又因为媒介对象是文件，所以用到子类是FileWriter Writer os= new FileWriter( file); os.write( hello); os.close(); &#125; 例4，用字符流写文件 12345678910public static void readCharFromFile() throws IOException&#123; File file= new File( &quot;d:/test.txt&quot;); //因为是用字符流来读媒介，所以对应的是Reader //又因为媒介对象是文件，所以用到子类是FileReader Reader reader= new FileReader( file); char [] byteArray= new char[( int) file.length()]; int size= reader.read( byteArray); System. out.println( &quot;大小:&quot;+size +&quot;;内容:&quot; +new String(byteArray)); reader.close();&#125; 字节流转换为字符流 字节流可以转换成字符流，java.io包中提供的InputStreamReader类就可以实现，当然从其命名上就可以看出它的作用。其实这涉及到另一个概念，IO流的组合，后面我们详细介绍。下面看一个简单的例子： 例5 ，字节流转换为字符流 123456789101112public static void convertByteToChar() throws IOException&#123; File file= new File( &quot;d:/test.txt&quot;); //获得一个字节流 InputStream is= new FileInputStream( file); //把字节流转换为字符流，其实就是把字符流和字节流组合的结果。 Reader reader= new InputStreamReader( is); char [] byteArray= new char[( int) file.length()]; int size= reader.read( byteArray); System. out.println( &quot;大小:&quot;+size +&quot;;内容:&quot; +new String(byteArray)); is.close(); reader.close(); &#125; IO类的组合从上面字节流转换成字符流的例子中我们知道了IO流之间可以组合（或称嵌套），其实组合的目的很简单，就是把多种类的特性融合在一起以实现更多的功能。组合使用的方式很简单，通过把一个流放入另一个流的构造器中即可实现，两个流之间可以组合，三个或者更多流之间也可组合到一起。当然，并不是任意流之间都可以组合。关于组合就不过多介绍了，后面的例子中有很多都用到了组合，大家好好体会即可。 文件媒介操作File是Java IO中最常用的读写媒介，那么我们在这里就对文件再做进一步介绍。 File媒介例6 ，File操作 123456789101112131415161718192021222324252627282930public class FileDemo &#123; public static void main(String[] args) &#123; //检查文件是否存在 File file = new File( &quot;d:/test.txt&quot;); boolean fileExists = file.exists(); System. out.println( fileExists); //创建文件目录,若父目录不存在则返回false File file2 = new File( &quot;d:/fatherDir/subDir&quot;); boolean dirCreated = file2.mkdir(); System. out.println( dirCreated); //创建文件目录,若父目录不存则连同父目录一起创建 File file3 = new File( &quot;d:/fatherDir/subDir2&quot;); boolean dirCreated2 = file3.mkdirs(); System. out.println( dirCreated2); File file4= new File( &quot;d:/test.txt&quot;); //判断长度 long length = file4.length(); //重命名文件 boolean isRenamed = file4.renameTo( new File(&quot;d:/test2.txt&quot;)); //删除文件 boolean isDeleted = file4.delete(); File file5= new File( &quot;d:/fatherDir/subDir&quot;); //是否是目录 boolean isDirectory = file5.isDirectory(); //列出文件名 String[] fileNames = file5.list(); //列出目录 File[] files = file4.listFiles(); &#125;&#125; 随机读取File文件通过上面的例子我们已经知道，我们可以用FileInputStream（文件字符流）或FileReader（文件字节流）来读文件，这两个类可以让我们分别以字符和字节的方式来读取文件内容，但是它们都有一个不足之处，就是只能从文件头开始读，然后读到文件结束。 但是有时候我们只希望读取文件的一部分，或者是说随机的读取文件，那么我们就可以利用RandomAccessFile。RandomAccessFile提供了seek()方法，用来定位将要读写文件的指针位置，我们也可以通过调用getFilePointer()方法来获取当前指针的位置，具体看下面的例子: 例7，随机读取文件 1234567891011121314public static void randomAccessFileRead() throws IOException &#123; // 创建一个RandomAccessFile对象 RandomAccessFile file = new RandomAccessFile( &quot;d:/test.txt&quot;, &quot;rw&quot;); // 通过seek方法来移动读写位置的指针 file.seek(10); // 获取当前指针 long pointerBegin = file.getFilePointer(); // 从当前指针开始读 byte[] contents = new byte[1024]; file.read( contents); long pointerEnd = file.getFilePointer(); System. out.println( &quot;pointerBegin:&quot; + pointerBegin + &quot;\n&quot; + &quot;pointerEnd:&quot; + pointerEnd + &quot;\n&quot; + new String(contents)); file.close();&#125; 例8，随机写入文件 12345678910111213public static void randomAccessFileWrite() throws IOException &#123; // 创建一个RandomAccessFile对象 RandomAccessFile file = new RandomAccessFile( &quot;d:/test.txt&quot;, &quot;rw&quot;); // 通过seek方法来移动读写位置的指针 file.seek(10); // 获取当前指针 long pointerBegin = file.getFilePointer(); // 从当前指针位置开始写 file.write( &quot;HELLO WORD&quot;.getBytes()); long pointerEnd = file.getFilePointer(); System. out.println( &quot;pointerBegin:&quot; + pointerBegin + &quot;\n&quot; + &quot;pointerEnd:&quot; + pointerEnd + &quot;\n&quot; ); file.close();&#125; 管道媒介管道主要用来实现同一个虚拟机中的两个线程进行交流。因此，一个管道既可以作为数据源媒介也可作为目标媒介。 需要注意的是java中的管道和Unix/Linux中的管道含义并不一样，在Unix/Linux中管道可以作为两个位于不同空间进程通信的媒介，而在java中，管道只能为同一个JVM进程中的不同线程进行通信。和管道相关的IO类为：PipedInputStream和PipedOutputStream，下面我们来看一个例子： 例9，读写管道 123456789101112131415161718192021222324252627282930313233343536public class PipeExample &#123; public static void main(String[] args) throws IOException &#123; final PipedOutputStream output = new PipedOutputStream(); final PipedInputStream input = new PipedInputStream(output); Thread thread1 = new Thread( new Runnable() &#123; @Override public void run() &#123; try &#123; output.write( &quot;Hello world, pipe!&quot;.getBytes()); &#125; catch (IOException e) &#123; &#125; &#125; &#125;); Thread thread2 = new Thread( new Runnable() &#123; @Override public void run() &#123; try &#123; int data = input.read(); while( data != -1)&#123; System. out.print(( char) data); data = input.read(); &#125; &#125; catch (IOException e) &#123; &#125; finally&#123; try &#123; input.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); thread1.start(); thread2.start(); &#125;&#125; 网络媒介关于Java IO面向网络媒介的操作即Java 网络编程，其核心是Socket，同磁盘操作一样，java网络编程对应着两套API，即Java IO和Java NIO，关于这部分我会准备专门的文章进行介绍。 BufferedInputStream和BufferedOutputStreamBufferedInputStream顾名思义，就是在对流进行写入时提供一个buffer来提高IO效率。在进行磁盘或网络IO时，原始的InputStream对数据读取的过程都是一个字节一个字节操作的，而BufferedInputStream在其内部提供了一个buffer，在读数据时，会一次读取一大块数据到buffer中，这样比单字节的操作效率要高的多，特别是进程磁盘IO和对大量数据进行读写的时候。 使用BufferedInputStream十分简单，只要把普通的输入流和BufferedInputStream组合到一起即可。我们把上面的例2改造成用BufferedInputStream进行读文件，请看下面例子： 例10 ，用缓冲流读文件 123456789public static void readByBufferedInputStream() throws IOException &#123; File file = new File( &quot;d:/test.txt&quot;); byte[] byteArray = new byte[( int) file.length()]; //可以在构造参数中传入buffer大小 InputStream is = new BufferedInputStream( new FileInputStream(file),2*1024); int size = is.read( byteArray); System. out.println( &quot;大小:&quot; + size + &quot;;内容:&quot; + new String(byteArray)); is.close();&#125; 关于如何设置buffer的大小，我们应根据我们的硬件状况来确定。对于磁盘IO来说，如果硬盘每次读取4KB大小的文件块，那么我们最好设置成这个大小的整数倍。因为磁盘对于顺序读的效率是特别高的，所以如果buffer再设置的大写可能会带来更好的效率，比如设置成44KB或84KB。 还需要注意一点的就是磁盘本身就会有缓存，在这种情况下，BufferedInputStream会一次读取磁盘缓存大小的数据，而不是分多次的去读。所以要想得到一个最优的buffer值，我们必须得知道磁盘每次读的块大小和其缓存大小，然后根据多次试验的结果来得到最佳的buffer大小。 BufferedOutputStream的情况和BufferedInputStream一致，在这里就不多做描述了。 BufferedReader和BufferedWriterBufferedReader、BufferedWriter 的作用基本和BufferedInputStream、BufferedOutputStream一致，具体用法和原理都差不多 ，只不过一个是面向字符流一个是面向字节流。同样，我们将改造字符流中的例4，给其加上buffer功能，看例子： public static void readByBufferedReader() throws IOException { File file = new File( &quot;d:/test.txt&quot;); // 在字符流基础上用buffer流包装，也可以指定buffer的大小 Reader reader = new BufferedReader( new FileReader(file),2*1024); char[] byteArray = new char[( int) file.length()]; int size = reader.read( byteArray); System. out.println( &quot;大小:&quot; + size + &quot;;内容:&quot; + new String(byteArray)); reader.close(); } 参考链接]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github搭建个人maven仓库]]></title>
    <url>%2F2017%2F07%2F02%2Fgithub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAmaven%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[引言三步： deploy到本地目录把本地目录提交到gtihub上配置github地址为仓库地址配置local file maven仓库 deploy到本地 maven可以通过http, ftp, ssh等deploy到远程服务器，也可以deploy到本地文件系统里。例如把项目deploy到/home/jet/jar-dependcies/maven/github目录下： &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;jethan-mvn-repo&lt;/id&gt; &lt;url&gt;file:/home/jet/jar-dependcies/maven/github&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt; 推荐使用命令行来deploy，避免在项目里显式配置: mvn deploy -DaltDeploymentRepository=maven_xm_repo::default::file:/home/jet/jar-dependcies/maven/github 上面把项目deploy到本地目录/home/jet/workspace/gerrit/maven_xm_repo/repository/里，下面把这个目录提交到github上。 在Github上新建一个项目，然后把/home/jet/workspace/gerrit/maven_xm_repo/下的文件都提交到gtihub上。 cd /home/jet/workspace/gerrit/maven_xm_repo/ git init git add . git commit -m &#39;deploy xxx&#39; git remote add origin git@github.com:jethan/maven_xm_repo.git git push origin master 最终效果可以参考我的个人仓库： maven_xm_repo github maven仓库的使用 因为github使用了raw.githubusercontent.com这个域名用于raw文件下载。所以使用这个maven仓库，只要在pom.xml里增加： &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jethan-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/jethan/maven_xm_repo/master/repository&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 目录查看和搜索 值得注意的是，github因为安全原因，把raw文件下载和原来的github域名分开了，而raw.githubusercontent.com这个域名是不支持目录浏览的。所以，想要浏览文件目录，或者搜索的话，可以直接到github域名下的仓库去查看。 比如文件fastjson-1.2.5.jar： 浏览器urlmaven仓库url maven仓库工作的机制 下面介绍一些maven仓库工作的原理。典型的一个maven依赖下会有这三个文件： maven-metadata.xml maven-metadata.xml.md5 maven-metadata.xml.sha1 maven-metadata.xml里面记录了最后deploy的版本和时间。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;metadata modelVersion=&quot;1.1.0&quot;&gt; &lt;groupId&gt;io.github.hengyunabc&lt;/groupId&gt; &lt;artifactId&gt;mybatis-ehcache-spring&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;versioning&gt; &lt;snapshot&gt; &lt;timestamp&gt;20150804.095005&lt;/timestamp&gt; &lt;buildNumber&gt;1&lt;/buildNumber&gt; &lt;/snapshot&gt; &lt;lastUpdated&gt;20150804095005&lt;/lastUpdated&gt; &lt;/versioning&gt; &lt;/metadata&gt; 其中md5, sha1校验文件是用来保证这个meta文件的完整性。 maven在编绎项目时，会先尝试请求maven-metadata.xml，如果没有找到，则会直接尝试请求到jar文件，在下载jar文件时也会尝试下载jar的md5, sha1文件。 maven-metadata.xml文件很重要，如果没有这个文件来指明最新的jar版本，那么即使远程仓库里的jar更新了版本，本地maven编绎时用上-U参数，也不会拉取到最新的jar！ 所以并不能简单地把jar包放到github上就完事了，一定要先在本地Deploy，生成maven-metadata.xml文件，并上传到github上。 参考 maven的仓库关系 配置使用本地仓库 想要使用本地file仓库里，在项目的pom.xml里配置，如： &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;hengyunabc-maven-repo&lt;/id&gt; &lt;url&gt;file:/home/hengyunabc/code/maven-repo/repository/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 注意事项 maven的repository并没有优先级的配置，也不能单独为某些依赖配置repository。所以如果项目配置了多个repository，在首次编绎时，会依次尝试下载依赖。如果没有找到，尝试下一个，整个流程会很长。 所以尽量多个依赖放同一个仓库，不要每个项目都有一个自己的仓库。 参考1参考2]]></content>
      <categories>
        <category>构建工具</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2017%2F06%2F24%2Fgit%2F</url>
    <content type="text"><![CDATA[引言集中式vs分布式 Linus一直痛恨的CVS、VSS及SVN都是集中式的版本控制系统，而Git是分布式版本控制系统，集中式和分布式版本控制系统有什么区别呢？ 先说集中式版本控制系统，版本库是集中存放在中央服务器的，而干活的时候，用的都是自己的电脑，所以要先从中央服务器取得最新的版本，然后开始干活，干完活了，再把自己的活推送给中央服务器。中央服务器就好比是一个图书馆，你要改一本书，必须先从图书馆借出来，然后回到家自己改，改完了，再放回图书馆。 集中式版本控制系统最大的毛病就是必须联网才能工作，如果在局域网内还好，带宽够大，速度够快，可如果在互联网上，遇到网速慢的话，可能提交一个10M的文件就需要5分钟，这还不得把人给憋死啊。 那分布式版本控制系统与集中式版本控制系统有何不同呢？首先，分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件A，你的同事也在他的电脑上改了文件A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 和集中式版本控制系统相比，分布式版本控制系统的安全性要高很多，因为每个人电脑里都有完整的版本库，某一个人的电脑坏掉了不要紧，随便从其他人那里复制一个就可以了。而集中式版本控制系统的中央服务器要是出了问题，所有人都没法干活了。在实际使用分布式版本控制系统的时候，其实很少在两人之间的电脑上推送版本库的修改，因为可能你们俩不在一个局域网内，两台电脑互相访问不了，也可能今天你的同事病了，他的电脑压根没有开机。因此，分布式版本控制系统通常也有一台充当“中央服务器”的电脑，但这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。 当然，Git的优势不单是不必联网这么简单，后面我们还会看到Git极其强大的分支管理，把SVN等远远抛在了后面。 CVS作为最早的开源而且免费的集中式版本控制系统，直到现在还有不少人在用。由于CVS自身设计的问题，会造成提交文件不完整，版本库莫名其妙损坏的情况。同样是开源而且免费的SVN修正了CVS的一些稳定性问题，是目前用得最多的集中式版本库控制系统。除了免费的外，还有收费的集中式版本控制系统，比如IBM的ClearCase（以前是Rational公司的，被IBM收购了），特点是安装比Windows还大，运行比蜗牛还慢，能用ClearCase的一般是世界500强，他们有个共同的特点是财大气粗，或者人傻钱多。微软自己也有一个集中式版本控制系统叫VSS，集成在Visual Studio中。由于其反人类的设计，连微软自己都不好意思用了。 分布式版本控制系统除了Git以及促使Git诞生的BitKeeper外，还有类似Git的Mercurial和Bazaar等。这些分布式版本控制系统各有特点，但最快、最简单也最流行的依然是Git！ 很幸运的是毕业入职的第一家公司所在团队就使用git版本控制工具，intelligent idea开发工具。感谢Muhlin Chen，Ying Tan，Ken Logan，虽然你们不会看见。 在stackoverflow上又看到这样一段对于git工作模式的描述， 1A typical distributed workflow using Git is for a contributor to fork a project, build on it, publish the result to her public repository, and ask the &quot;upstream&quot; person (often the owner of the project where she forked from) to pull from her public repository. Requesting such a &quot;pull&quot; is made easy by the git request-pull command. 此时我不禁发现，对git的理解还是太浅薄。当初推荐小伙伴使用git时，我只能说出git比svn好，却不知为何更好。显然这样的表达是毫无说服力的。在网上一番搜索之后，发现一篇文章，探讨了四种常见的Git工作模式。我决定将本文翻译于此，算是对用了三年git的一个交代吧。 值得注意的是，如文章作者在一开始交代的，这四种方式只是四个典型的应用方式。我们不应该仅限于这五种方式，我们可以对这些方法进行融汇、修剪，创造出最适合自己团队的工作模式。 Git Workflows 分布式工作流由于不知道Git可能的工作模式，刚刚在工作中接触Git的人可能会感到有些困难。本文描述了最常见的几种Git工作模式，希望以此作为新人探索Git世界的一个起点。 当你在阅读本文的时候，这些工作模式只是一些指导意见，而不是不可违背的规则。我们希望通过告诉你什么是可能的，让你能够根据个人的需求混合、订制自己的工作模式。 转到一个分布式的版本控制系统看起来是一个令人畏惧的任务，但是你并不必改变现有的工作模式就可以享受Git带来的好处。你的团队可以按照与svn一样的模式进行工作。 然而，相比svn而言，在你的工作流程中使用Git会带来几个好处。第一，每个开发者都是整个项目的本地备份。这种隔离的工作环境可以使每个开发者独立的工作，他们可以提交修改到自己的本地仓库，可以忽视忘记上游的开发过程。直到合适的时候再通知上游。 第二，Git提供了健壮的分支和合并模型。与svn不同，Git的分支为集成代码和分享改动采用了一种fail-safe的机制。 工作原理就像Subversion一样，Centralized-workflow使用一个中央仓库作为代码提交的唯一入口。Svn中使用trunk，而Git中默认的开发分支叫作mster，所有的改动都会提交到这个分支。这种工作模式master以外的其他任何分支。 开发者首先clone这个中央仓库。在他们自己的本地备份中，他们修改文件、提交改动，就和在svn中一样。然而，这些提交仅仅储存在本地，它们和远程的那个中央仓库是完全隔离的。这就允许开发者可以在自己满意的一个时间点同步自己的代码到上游仓库中。 开发者通过push，将自己的master分支推送到中央仓库，来完成发布改动的过程。这和svn commit等价，除了git push会把所有本地提交里远程仓库中没有的都提交给中央仓库的master分支。 解决冲突 中央仓库代表官方工程，所以它的提交历史应该是正式且不可变的。如果一个开发者的本地提交偏离了中央仓库，Git将会拒绝push他的改动，因为这会覆盖掉官方的提交。 在开发者发布他们的修改之前，他们需要先取到中央仓库中本地没有的那些提交，将自己的改动建立在那些提交之上。这就好像是说，“我想把自己的改动添加到其他人已经做过的改动之上。”结果是一个绝对线性的提交历史，就跟传统的svn工作模式一样。 如果本地改动和上游的提交直接产生了冲突，Git将会暂停并让你手动的解决这些冲突。Git的好处之一是它使用了git status和git add同时用于生成提交和解决合并冲突。这使得新开发者更容易管理自己的合并。而且，如果他们陷入了麻烦，git就会停止合并过程，让开发者重试或者寻求帮助。 示例让我们来一步一步的看看一个典型的小团队如何在这种模式下合作。我们将会看到两个开发者，John和Mary，通过一个中央仓库，分别进行两个feature的开发并且分享他们的贡献。 Someone initializes the distributed repository 首先，有个人需要在一台服务器上创建中央仓库。如果这是一个新工程，你可以初始化一个空的仓库。否则，你需要导入一个已有的项目。 中央仓库应该是空的仓库，它不应该是一个已有的工作目录。我们可以这样创建，12ssh user@hostgit init --bare /path/to/repo.git 确保user是正确的SSH Username，host是你的服务器的域名或者IP，还有你希望存放仓库的位置。 Everybody clones the distributed repository 接下来，每个开发者创建一个整个工程的本地备份。我们可以通过命令git clone完成，1git clone ssh://user@host/path/to/repo.git 当你克隆了一个仓库，Git将会自动为你添加一个叫作origin的标签，这个标签指回到父仓库，以便你今后和其进行交互。 John works on his feature 在他的本地仓库中，John可以使用标准的Git提交流程进行开发，edit、stage、commit。如果你不熟悉工作目录，有一种方法可以让你指定提交的内容。这可以进行针对性的提交，即使你在本地进行了很多修改。 123git status # View the state of the repogit add # Stage a filegit commit # Commit a file 记住，因为这些命令只完成了本地的提交工作，John可以重复这个过程，随意进行提交，而不必担心远程的中央仓库发生了什么。这对于大feature而言非常有用，因为这些大feature往往需要打碎成更多更简单、更小的部分。 Mary works on her feature 于此同时，Mary也在她本地进行自己的feature开发，使用同样的edit、stage、commit流程。和John一样，她不必担心远程的中央仓库正在发生什么。而且也不担心John在他的本地仓库正在干什么，因为所有的本地仓库都是私有的。 John publishes his feature 一旦John完成了他的feature，他就应该把自己的本地提交发布到中央仓库，使其他团队成员可以访问。他可以通过git push来完成， 1git push origin master 记住，origin是当John clone的时候Git创建的指向中央仓库的远程连接。参数master告诉git，他希望让origin的master分支和他本地的master分支一样。因为中央仓库在John clone之后还没有被更新过，所以不会导致冲突，push将会顺利完成。 Mary tries to publish her feature 我们来看看当John成功发布之后，Mary试图发布的时候会发生什么。她可以使用完全一样的命令， 1git push origin master 但是，因为她本地的历史与中央仓库的历史发生了偏离，Git将会拒绝她的请求。 12345error: failed to push some refs to &apos;/path/to/repo.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Merge the remote changes (e.g. &apos;git pull&apos;)hint: before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 这阻止了Mary覆盖官方的提交。她需要首先将John的提交拿到本地，和本地的修改进行集成，然后重试。 Mary rebases on top of John’s commits Mary可以使用git pull将上游的提交混合到本地。这个命令有点像svn update，它把上游的提交历史取到本地，并试图和本地的修改合并。 1git pull --rebase origin master 选项—rebase告诉Git在同步了中央仓库的改动之后，把Mary的改动移到master分支的顶部。如下图所示， 如果你忘了那个选项，pull仍然可以工作，但是每次有人需要同步中央仓库的时候，都会出现很多”merge commit”。对于这种工作模式而言，最好rebase，而不要生成merge commit。 Mary resolves a merge conflict Rebase把本地的提交一次一个的放到更新过的master上。这意味着merge conflict只会发生在你的一次提交之上，而不是把你的所有提交作为整体进行合并。这可以使你的提交更有针对性，并且保持一个干净的提交历史。相应的，这使得找到bug是在哪里引进的更加容易，如果必要，可以对项目以最小代价进行回滚操作。 如果Mary和John分别工作在无关的feature上，rebase的过程不大可能会发生冲突。但是如果冲突发生了，Git将会在当前提交处暂停rebase的过程，并且输入如下信息，和一些相应的提示， 1CONFLICT (content): Merge conflict in &lt;some-file&gt; Git的一个伟大之处在于，任何人都可以解决他们自己的合并冲突。在我们的例子中，Mary可以简单的执行git status看看问题发生在哪里。冲突文件将会出现在Unmerged path一节。 12345# Unmerged paths:# (use &quot;git reset HEAD &lt;some-file&gt;...&quot; to unstage)# (use &quot;git add/rm &lt;some-file&gt;...&quot; as appropriate to mark resolution)## both modified: &lt;some-file&gt; 然后，她就可以把文件修改成自己需要的样子。一旦她修改完毕，她就可以将文件置入工作区，让git rebase继续做剩下的事情， 12git add &lt;some-file&gt;git rebase --continue 这就是需要做的所有事情了。Git将会移动到下一个提交，对所有产生冲突的提交重复刚才的步骤。 如果你遇到了某种情况，而且解决不了时，不必惊慌。只需要执行下面的命令，你就可以回到执行git pull —rebase之前的状态。 1git rebase --abort Mary successfully publishes her feature 当她完成与中央仓库的同步之后。Mary就可以成功发布她的改动了。 1git push origin master Feature分支工作流 一旦你熟悉了Centralized Workflow，在你的开发过程中加入feature分支成了一种简单的促进开发者协作的方法。 Feature Branch Workflow的核心想法在于所有feature都应该在自己的分支中开发，而不是都在master分支中。这种封装使得多个开发者可以在不干扰主代码库的前提下开发自己的feature。这同时意味着master分支将永远不会包含不健全的代码，这对于不断进行的集成开发是一个非常大的好处。 这种对feature开发的封装，也使得我们可以在开发中利用pull requests，这是一种发起关于某个分支的讨论的方法。这给了其他开发者一个在代码被合并到主项目之前审查某个feature的机会。或者，当你卡在某个feature开发中时，你可以发起一个pull request向你的同事征求意见。总而言之，pull requests以一种非常简单的方式为你的团队提供了评论彼此工作的条件。 工作原理Feature Branch Workflow仍然使用中央仓库，master分支仍然代表官方工程的历史。但是，并不是直接提交到本地的master分支，开发者每次开始一个新feature时，首先需要创建一个新的分支。Feature分支应该有描述性的名称，比如animated-menu-items或者issue-#1061.这是为了给每个分支提供一个清晰、明确的目的。 Git对于master分支和feature分支没有本质上的区分，所以开发者可以edit、stage并且commit改动到feature分支，就和在Centralized Workflow中一样。 除此之外，feature分支可以被合并到中央仓库中。这就可以在不动官方代码的前提下在开发者之间共享一个feature。因为master是唯一的特殊分支，在中央仓库中储存多个feature分支并不会带来什么问题。当然，这也是一种备份本地提交的好办法。 Pull Requests 除了隔离feature的开发环境之外，分支使得我们可以通过pull requests来讨论代码改动。一旦某人完成了一个feature，他们不需要立刻合并到master中。他们会合并到feature分支，然后发起一个pull request要求合并他们的改动到master中。这给了其他开发者一个在其进入主代码库之前审查代码改动的机会。 代码审查时pull requests的一个好处，但是它却是被设计做一种讨论代码的通用方法。你可以认为pull requests是针对某个分支的讨论。这意味着也可以在开发流程中更早的阶段使用它。比如说，如果一个开发者需要帮助，他就可以发起一个pull request。相关的人会被自动通知，他们就可以看到提交下面的问题了。 一旦一个pull request被接受了，发布一个feature的过程和Centralized Workflow是一样的。首先，需要确保本地的master分支和上游的master进行了同步，然后，你将feature分支合并到master，再合并到中央仓库的master中。 一些代码管理工具可以帮助我们处理pull requests，如Bitbucker或Stash。 示例下面的例子将演示如何将pull requests作为代码审查的一种形式，但是切记它还可以用于很多其他的目的。 Mary begins a new feature 在她开始开发一个feature之前，她需要一个隔离的分支。她可以新开一个分支， 1git checkout -b marys-feature master 这检出了一个叫作marys-feature的分支，基于master。选项-b告诉Git如果这个分支不存在，就创建一个。在这个分支上，Mary edit、stage并且commit，按照通常的方式，提交多次后建立起了她的feature。 123git statusgit add &lt;some-file&gt;git commit Mary goes to lunch 在早上Mary为她的分支进行了若干次提交。在她去吃午饭之前，应该把她的feature分支上传到中央仓库。这相当于进行了备份，但是如果Mary和其他开发者进行协作，那这使得其他开发者可以访问她的提交内容了。 1git push -u origin marys-feature 这个命令将marys-feature上传到中央仓库origin，选项-u将其添加为一个远程跟踪分支。设置好这个跟踪分支后，Mary可以直接执行git push，无需任何其他参数就可以上传feature了。 Mary finishes her feature 当Mary吃完午饭回来后，她完成了她的feature。在她将其合并入master之前，她需要发起一个pull request让组内其余的人知道她完成了。但是首先，她需要确保中央仓库已经有了她最新的提交， 1git push 然后，她在她的Git GUI中发起了一个pull request，请求合并marys-feature到master，其他组员将会自动收到提醒。pull requests允许在提交的下面进行评论，所以这对于问问题、讨论来说非常简单。 Bill receives the pull request Bill收到了pull request并且查看了marys-feature。他决定在集成到官方工程之前进行一些修改，然后他和Mary前前后后通过pull request进行了一番交流。 Mary makes the changes Mary通过edit、stage、commits完成了修改，并且上传到了中央仓库。她所有的活动都在pull request中有所展现，Bill仍然可以进行评论。 如果他愿意，Bill可以拿到marys-feature，在自己的本地备份中工作。任何他的提交也会出现在pull request中。 Mary publishes her feature 一旦Bill决定要接受这个pull request了，某个人需要合并这个feature到稳定的工程中去，这个人既可以是Bill也可以使Mary。 1234git checkout mastergit pullgit pull origin marys-featuregit push 首先，不论是谁都要检出master分支，然后确保它是最新的。然后，git pull origin marys-feature将中央仓库中的marys-feature合并到本地的master。有也可以简单的使用git merge marys-feature，但是上面的命令确保你总是拿到最新的feature branch。最后，更新过的master需要上传到origin去。 一些GUI能够自动化pull request的接受过程，仅仅需要点击Accept按钮，就可以触发一系列命令完成工作。如果你的不行，它至少也可以在合并代码之后自动关闭这个pull request。 Meanwhile, John is doing the exact same thing 当Mary和Bill开发marys-feature，在Mary的pull request中讨论时，John也在做同样的事情。通过隔离feature到不同的分支，所有人都可以独立的工作，如果必要和其他开发者共享代码改动也是很轻松的事情。 Gitflow工作流 Gitflow Workflow源自Vincent Driessen在nvie的文章。 Gitflow Workflow定义了一个用于工程发布的严格的分支模型。比起Feature Branch Workflow稍微复杂一些，其为管理较大项目提供了一个可靠的框架。 这种工作模式在Feature Branch Workflow的基础上没有增加新的概念或者命令。它只是为不同的分支定义了明确的角色，并且定义它们之间何时以及如何交互。与Feature Branch Workflow相比，它为准备、维护、发布定义了自己的分支。当然，你也可以享受到所有Feature Branch Workflow拥有的好处：pull requests，隔离环境，和更加有效的协作。 工作原理Gitflow Workflow仍然使用一个中央仓库。与其他工作模式相同，开发者可以在本地工作，然后再将分支push到中央仓库中。唯一的不同在于工程的分支结构。 Historical Branches 与单一master分支不同，本工作模式使用两个分支来记录工程的历史。master分支存储官方发布的历史，develop分支用于集成feature。要为master分支的每一个提交打上一个版本号作为tag也是很方便的。 本工作模式的其余部分都在考虑让这两个分支进行交互。 Feature Branches 每一个新feature应该在自己的分支上，这个分支可以被push到中央仓库以便备份或者协作之用。但是，feature分支把develop分支作为父分支，而不是从master分支上分裂出来。当一个feature完成后，它会被合并会develop分支。Features永远都不应该直接和master交互。 注意feature分支和develop分支就是Feature Branch Workflow。但是Gitflow Workflow并不只是这样。 Release Branches 当develop收集到了足够一次发布的features时，或者一个预先约定的发布日期到达时，你就从develop分裂出一个分支。这个分支的创建就代表着一个发布周期的开始，所以这个时间点之后任何新feature都不能加进来，除了修复bug，文档生成，和其他发布相关的任务才能进入这个分支。当这个分支可以发布时，这个分支将会合并到master，并且用版本号打上一个tag。除此之外，还应该合并会develop分支，因为这个分支可能包含了在创建之初还没有的进展。 使用一个特定的分支来准备发布，就可以让一个团队继续优化当前版本，而另一个团队继续为下个版本开发新feature。它同时也创建了良好定义的开发术语，比如我们可以说，“这周我们准备4.0版的发布”，实际上我们也可以在代码仓库中看到这个结构。 Maintenance Branches Maintenance或者说hotfix分支用来对生产环境的版本进行快速修复。这是唯一可以直接从master分裂的分支。一旦修复完成，就应该马上被合并到master和develop分支中，而且master应该用更新版本号打上一个tag。 拥有一个特定的开发线路供修复bug使得你的团队可以在不干扰其他工作流程，也不用等待下个发布周期的前提下处理issue。你可以认为maintenance分支是一个直接和master分支交互的发布分支。 示例下面的例子展示了这个工作模式如何处理一个发布周期。我们假设已经创建了一个中央仓库。 Create a develop branch 第一步是在默认master分支的基础上补全一个develop分支。一个简单的方法是在本地创建一个空的develop分支，然后push到服务器上， 12git branch developgit push -u origin develop 这个分支包含了整个工程的完整历史，而master只包含了一个削减过的版本。其他开发者现在应该clone中央仓库，并且创建一个develop的跟踪分支。 12git clone ssh://user@host/path/to/repo.gitgit checkout -b develop origin/develop 现在所有人都在本地建立起了一份历史分支的备份。 Mary and John begin new features 我们的例子开始于John和Mary要开发不同的feature。他们都需要创建各自的分支。他们不应该以master作为基础，而应该以develop作为基础， 1git checkout -b some-feature develop 他们两个都通过edit、stage、commit的方法，向各自的feature分支进行提交。 123git statusgit add &lt;some-file&gt;git commit Mary finishes her feature 在提交了若干次之后，Mary认为她的feature已经完成了。如果她的团队使用pull requests，这就是一个合适的发起一个pull request请求合并她的feature到develop的时间。否则，她可以合并到本地的develop然后push到中央仓库， 12345git pull origin developgit checkout developgit merge some-featuregit pushgit branch -d some-feature 第一个命令确保本地的develop分支是最新的。注意feature始终不应该直接合并到master中。冲突的解决方法和Centralized Workflow中描述的一样。 Mary begins to prepare a release 当John仍然在开发他的feature时，Mary开始准备项目的第一个官方发行。跟feature开发一样，她使用一个新的分支来封装发行的准备工作。这一步也是创建发行版本号的时候， 1git checkout -b release-0.1 develop 这个分支用来整理、测试、更新文档，以及为下一次发行做一切准备工作。这就是像是一个为了优化发行版本的feature分支。 一旦Mary创建了这个分支，并且push到了中央仓库，这个发行就变成feature-frozen了。任何不在develop中的功能都要推迟到下个发布周期。 Mary finishes the release 当release准备好上线时，Mary将其合并回master和develop，然后删除这个发行分支。合并会develop是非常重要的，因为重要的修改可能被加入了这个发行分支，他们对于新的feature而言是有用的。如果Mary的组织强调代码审查，这也是一个理想的发起pull request的位置。 1234567git checkout mastergit merge release-0.1git pushgit checkout developgit merge release-0.1git pushgit branch -d release-0.1 release分支好像是一个feature开发版本(develop)与公开发行版本(master)之间的缓冲区。不论什么时候你合并回master，你都应该为提交打上tag， 12git tag -a 0.1 -m &quot;Initial public release&quot; mastergit push --tags Git提供了一些脚本，可以在代码仓库发生了一些特定的事件时触发。你可以配置，使得每当有master分支或者一个tag被push到中央仓库时，自动的创建一个公开发行版本。 End-user discovers a bug 在发布之后，Mary回到下个版本的feature开发中。直到一个终端用户在现有版本中发现了一个bug。为了处理这个bug，Mary从master创建了一个maintenance分支，修复bug，然后直接合并回master。 12345git checkout -b issue-#001 master# Fix the buggit checkout mastergit merge issue-#001git push 和release分支一样，maintenance分支包括了重要的修改，这些修改也应该合并到develop中。然后，就可以删掉这个分支了， 1234git checkout developgit merge issue-#001git pushgit branch -d issue-#001 Forking工作流Forking Workflow和本文中谈到的其他工作模式都不相同。它并不是只有一个单独的服务器端的仓库来扮演中央代码库，在这个模式中，每个开发者都有一个自己的服务器端的仓库。这意味着每一个贡献者都有两个Git的仓库，一个私有的本地的，一个共有的服务器端的。 Forking Workflow最大的好处在于不用所有人都push到一个中央仓库。开发者可以push自己的服务器端仓库，只有项目的维护者才能push到官方的仓库。它允许维护者维护者接受其他开发者的提交，而不给他们对官方代码库的写权限。 这就构成了一种分布式的工作模式。为大型团队安全的协作提供了一种灵活的方式。 工作原理跟其他Git工作模式一样，Forking Workflow开始于服务器端的官方公开仓库。但是当一个新开发者想要在这个项目上工作时，他并不能呢个直接clone这个官方工程。 他需要先fork这个官方工程，在服务器上创建一份自己的备份。这个新的备份就是它的个人公开仓库，其他开发者都不允许push到这个仓库，但是他们可以pull这个仓库的改动到自己的代码中。当他们创建自己服务器端的备份后，开发者执行git clone在本地建立一个备份。这就是他们私人的开发环境，就跟别的工作模式一样。 当他们想要发布本地的提交时，他们push这个提交到他们自己的公开仓库中，而不是官方的那个。然后，他们发起一个pull request，让官方工程维护者知道有一个更新等待被集成。那个pull request也成为了一个关于提交代码讨论的地方。 有O把这个feature集成到官方代码库中，维护者pull贡献者的改到到本地，检查功能是否正常，是否打破工程，然后合并到本地master，然后push到服务器的公开仓库中。现在这个feature就是这个工程的一部分了，其他开发者也应该从官方仓库pull这些改动到本地来进行同步。 The official repository 在Forking Workflow中的“官方”一词只是一种习惯。从技术角度来说，Git并不认为官方的公共仓库和其他开发者的公共仓库有什么不同。实际上，唯一使得官方仓库官方的原因在于其实工程维护者的公开仓库。 Branching in the Forking Workflow 所有这些个人的公共仓库只是一种方便共享分支给其他开发者的方式。所有人仍然应该使用分支来隔离不同的feature，就像Feature Branch Workflow和Gitflow Workflow一样。唯一的区别是这些分支如何共享。在Forking Workflow中，他们被pull到另一个开发者的本地仓库中，而Feature Branch Workflow和Gitflow Workflow则是push到官方仓库中。 示例 github gitcafe 相关资料推荐 玩游戏，学习git分支 大牛的博客，详细简单基础]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用操作命令(一)]]></title>
    <url>%2F2017%2F06%2F19%2Flinux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[引言 ◆ 安装和登录命令：login、shutdown、halt、reboot、install、mount、umount、chsh、exit、last； ◆ 文件处理命令：file、mkdir、dd、rm、grep、find、mv、cp、ls、diff、cat、ln、tail、head、more、less、cd、管道； ◆ 系统管理相关命令：df、top、free、quota、at、lp、adduser、groupadd、kill、crontab； ◆ 网络操作命令：ifconfig、ip、ping、netstat、telnet、ftp、route、rlogin、rcp、finger、mail、nslookup； ◆ 系统安全相关命令：passwd、su、umask、chgrp、chmod、chown、chattr、sudo ps、who、which、whois； ◆ 其它命令：tar、unzip、gunzip、unarj、mtools、man、unendcode、uudecode。系统管理命令 stat ＃显示指定文件的详细信息，比ls更详细 who ＃显示在线登陆用户 whoami ＃显示当前操作用户 hostname ＃显示主机名 uname ＃显示系统信息 top ＃动态显示当前耗费资源最多进程信息 ps ＃显示瞬间进程状态 ps -aux du ＃查看目录大小 du -h /home带有单位显示目录信息 df ＃查看磁盘大小 df -h 带有单位显示磁盘信息 ifconfig ＃查看网络情况 ping ＃测试网络连通 netstat ＃显示网络状态信息 man ＃命令不会用了，找男人? 如：man ls clear ＃清屏 alias ＃对命令重命名 如：alias showmeit=”ps -aux” ，另外解除使用unaliax showmeit kill ＃杀死进程，可以先用ps 或 top命令查看进程的id，然后再用kill命令杀死进程。 常用基本指令 ls #显示文件或目录 -l #列出文件详细信息l(list) -a #列出当前目录下所有文件及目录，包括隐藏的a(all) mkdir #创建目录 -p #创建目录，若无父目录，则创建p(parent) cd #切换目录 touch #创建空文件 echo #创建带有内容的文件。 cat #查看文件内容 cp #拷贝 mv #移动或重命名 rm #删除文件 -r #递归删除，可删除子目录及文件 -f #强制删除 find #在文件系统中搜索某文件 wc #统计文本中行数、字数、字符数 grep #在文本文件中查找某个字符串 rmdir #删除空目录 tree #树形结构显示目录，需要安装tree包 pwd #显示当前目录 ln #创建链接文件 more、less #分页显示文本文件内容 head、tail #显示文件头、尾内容 ctrl+alt+F1 #命令行全屏模式 下面只介绍一些常用的基本命令 安装和登录命令shutdown/halt/reboot/exit/logout命令说明 shutdown -r #关机重启 -h #关机不重启 now #立刻关机 halt #关机 reboot #重启 exit #退出当前shell logout #退出登录shell mount/unmount命令说明 fdisk -l #查看磁盘情况 fdisk /dev/sda #为/dev/sda设备分区 m #显示所有命令 n #添加分区 p/e #主分区/逻辑分区 +50G #指定分区大小为50G p #打印分区列表 w #保存 reboot #重启 cat /etc/fstab #查看文件系统 mke2fs -t ext4 /dev/sda4 #格式化文件系统 mount /dev/sda4 /home #挂载到指定目录/home umount #取消挂载 #开机自动挂载 echo &quot;/dev/sda4 /home ext4 defaults 1 1&quot; &gt;&gt; /etc/fstab 文件处理命令awk一种编程语言，用于在linux/unix下对文本和数据进行处理 数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk最大的优势。 awk [options] &#39;script&#39; var=value file(s) awk [options] -f scriptfile var=value file(s) -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 {} 要执行的脚本内容 eg: cat /etc/passwd |awk -F &#39;:&#39; &#39;{print $1&quot;\t&quot;$7}&#39; awk模式和操作 awk脚本是由模式和操作组成的。 模式 模式可以是以下任意一个： /正则表达式/：使用通配符的扩展集。 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试。 模式匹配表达式：用运算符~（匹配）和~!（不匹配）。 BEGIN语句块、pattern语句块、END语句块：参见awk的工作原理 操作 操作由一个或多个命令、函数、表达式组成，之间由换行符或分号隔开，并位于大括号内，主要部分是： 变量或数组赋值 输出命令 内置函数 控制流语句 awk脚本基本结构 awk &#39;BEGIN{ print &quot;start&quot; } pattern{ commands } END{ print &quot;end&quot; }&#39; file 一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如： awk &#39;BEGIN{ i=0 } { i++ } END{ print i }&#39; filename awk &quot;BEGIN{ i=0 } { i++ } END{ print i }&quot; filename awk的工作原理 awk &#39;BEGIN{ commands } pattern{ commands } END{ commands }&#39; 第一步：执行BEGIN{ commands }语句块中的语句； 第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ commands }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。 第三步：当读至输入流末尾时，执行END{ commands }语句块。 BEGIN语句块在awk开始从输入流中读取行之前被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。 END语句块在awk从输入流中读取完所有的行之后即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。 pattern语句块中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行{ print }，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。 eg： [jet@jet ~]$ echo -e &quot;A line 1\nA line 2&quot; | awk &#39;BEGIN{ print &quot;Start&quot; } { print } END{ print &quot;End&quot; }&#39; Start A line 1 A line 2 End 当使用不带参数的print时，它就打印当前行，当print的参数是以逗号进行分隔时，打印时则以空格作为定界符。在awk的print语句块中双引号是被当作拼接符使用，例如： echo | awk &#39;{ var1=&quot;v1&quot;; var2=&quot;v2&quot;; var3=&quot;v3&quot;; print var1,var2,var3; }&#39; v1 v2 v3 双引号拼接使用： echo | awk &#39;{ var1=&quot;v1&quot;; var2=&quot;v2&quot;; var3=&quot;v3&quot;; print var1&quot;=&quot;var2&quot;=&quot;var3; }&#39; v1=v2=v3 { }类似一个循环体，会对文件中的每一行进行迭代，通常变量初始化语句（如：i=0）以及打印文件头部的语句放入BEGIN语句块中，将打印的结果等语句放在END语句块中。 awk内置变量（预定义变量） 说明：[A][N][P][G]表示第一个支持变量的工具，[A]=awk、[N]=nawk、[P]=POSIXawk、[G]=gawk $n 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 $0 这个变量包含执行过程中当前行的文本内容。 [N] ARGC 命令行参数的数目。 [G] ARGIND 命令行中当前文件的位置（从0开始算）。 [N] ARGV 包含命令行参数的数组。 [G] CONVFMT 数字转换格式（默认值为%.6g）。 [P] ENVIRON 环境变量关联数组。 [N] ERRNO 最后一个系统错误的描述。 [G] FIELDWIDTHS 字段宽度列表（用空格键分隔）。 [A] FILENAME 当前输入文件的名。 [P] FNR 同NR，但相对于当前文件。 [A] FS 字段分隔符（默认是任何空格）。 [G] IGNORECASE 如果为真，则进行忽略大小写的匹配。 [A] NF 表示字段数，在执行过程中对应于当前的字段数。 [A] NR 表示记录数，在执行过程中对应于当前的行号。 [A] OFMT 数字的输出格式（默认值是%.6g）。 [A] OFS 输出字段分隔符（默认值是一个空格）。 [A] ORS 输出记录分隔符（默认值是一个换行符）。 [A] RS 记录分隔符（默认是一个换行符）。 [N] RSTART 由match函数所匹配的字符串的第一个位置。 [N] RLENGTH 由match函数所匹配的字符串的长度。 [N] SUBSEP 数组下标分隔符（默认值是34）。 eg: echo -e &quot;line1 f2 f3nline2 f4 f5nline3 f6 f7&quot; | awk &#39;{print &quot;Line No:&quot;NR&quot;, No of fields:&quot;NF, &quot;$0=&quot;$0, &quot;$1=&quot;$1, &quot;$2=&quot;$2, &quot;$3=&quot;$3}&#39; Line No:1, No of fields:3 $0=line1 f2 f3 $1=line1 $2=f2 $3=f3 Line No:2, No of fields:3 $0=line2 f4 f5 $1=line2 $2=f4 $3=f5 Line No:3, No of fields:3 $0=line3 f6 f7 $1=line3 $2=f6 $3=f7 使用print $NF可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段，其他以此类推： echo -e &quot;line1 f2 f3n line2 f4 f5&quot; | awk &#39;{print $NF}&#39; f3 f5 echo -e &quot;line1 f2 f3n line2 f4 f5&quot; | awk &#39;{print $(NF-1)}&#39; f2 f4 打印每一行的第二和第三个字段： awk &#39;{ print $2,$3 }&#39; filename 统计文件中的行数： awk &#39;END{ print NR }&#39; filename 以上命令只使用了END语句块，在读入每一行的时，awk会将NR更新为对应的行号，当到达最后一行NR的值就是最后一行的行号，所以END语句块中的NR就是文件的行数。 一个每一行中第一个字段值累加的例子： seq 5 | awk &#39;BEGIN{ sum=0; print &quot;总和：&quot; } { print $1&quot;+&quot;; sum+=$1 } END{ print &quot;等于&quot;; print sum }&#39; 总和： 1+ 2+ 3+ 4+ 5+ 等于 15 将外部变量值传递给awk 借助-v选项，可以将外部值（并非来自stdin）传递给awk： VAR=10000 echo | awk -v VARIABLE=$VAR &#39;{ print VARIABLE }&#39; 另一种传递外部变量方法： var1=”aaa”var2=”bbb”echo | awk ‘{ print v1,v2 }’ v1=$var1 v2=$var2 当输入来自于文件时使用： awk ‘{ print v1,v2 }’ v1=$var1 v2=$var2 filename 以上方法中，变量之间用空格分隔作为awk的命令行参数跟随在BEGIN、{}和END语句块之后。 awk运算与判断 作为一种程序设计语言所应具有的特点之一，awk支持多种运算，这些运算与C语言提供的基本相同。awk还提供了一系列内置的运算函数（如log、sqr、cos、sin等）和一些用于对字符串进行操作（运算）的函数（如length、substr等等）。这些函数的引用大大的提高了awk的运算功能。作为对条件转移指令的一部分，关系判断是每种程序设计语言都具备的功能，awk也不例外，awk中允许进行多种测试，作为样式匹配，还提供了模式匹配表达式~（匹配）和~!（不匹配）。作为对测试的一种扩充，awk也支持用逻辑运算符。 算术运算符 运算符 描述符 + - 加，减 * / &amp; 乘，除与求余 + - ！ 一元加 ，减和逻辑非 ^ *** 求冥 ++ — 自增,自减 作为前缀或后缀 eg： awk &#39;BEGIN{a=&quot;b&quot;;print a++,++a;}&#39; 0 2 注意：所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0 赋值运算符 运算符 描述符 = += -= *= /= %= ^= **= 赋值语句 eg： a+=5; 等价于：a=a+5; 逻辑运算符 运算符 描述 &#124;&#124; 逻辑或 &amp;&amp; 逻辑与 eg： awk &#39;BEGIN{a=1;b=2;print (a&gt;5 &amp;&amp; b&lt;=2),(a&gt;5 || b&lt;=2);}&#39; 0 1 正则运算符 运算符 描述 ~ ~! 匹配正则表达式和不匹配正则表达式 eg： awk &#39;BEGIN{a=&quot;100testa&quot;;if(a ~ /^100*/){print &quot;ok&quot;;}}&#39; ok 关系运算符 运算符 描述 &lt; &lt;= &gt; &gt;= != == 关系运算符 eg： awk &#39;BEGIN{a=11;if(a &gt;= 9){print &quot;ok&quot;;}}&#39; ok 注意：> < 可以作为字符串比较，也可以用作数值比较，关键看操作数如果是字符串就会转换为字符串比较。两个都为数字才转为数值比较。字符串比较：按照ASCII码顺序比较。 其它运算符 运算符 描述 $ 字段引用 空格 字符串连接符 ?: C条件表达式 in 数组中是否存在某键值 eg： awk &#39;BEGIN{a=&quot;b&quot;;print a==&quot;b&quot;?&quot;ok&quot;:&quot;err&quot;;}&#39; ok awk &#39;BEGIN{a=&quot;b&quot;;arr[0]=&quot;b&quot;;arr[1]=&quot;c&quot;;print (a in arr);}&#39; 0 awk &#39;BEGIN{a=&quot;b&quot;;arr[0]=&quot;b&quot;;arr[&quot;b&quot;]=&quot;c&quot;;print (a in arr);}&#39; 1 运算级优先级表 awk高级输入输出 读取下一条记录 awk中next语句使用：在循环逐行匹配，如果遇到next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。net语句一般用于多行合并： cat text.txt a b c d e awk &#39;NR%2==1{next}{print NR,$0;}&#39; text.txt 2 b 4 d 当记录行号除以2余1，就跳过当前行。下面的print NR,$0也不会执行。下一行开始，程序有开始判断NR%2值。这个时候记录行号是：2 ，就会执行下面语句块：&#39;print NR,$0&#39; 分析发现需要将包含有“web”行进行跳过，然后需要将内容与下面行合并为一行： [jet@jet oschina_hexo_server]$ cat test.txt web01[192.168.2.100] httpd ok tomcat ok sendmail ok web02[192.168.2.101] httpd ok postfix ok web03[192.168.2.102] mysqld ok httpd ok 0 [jet@jet oschina_hexo_server]$ awk &#39;/^web/{T=$0;next;}{print T&quot;:\t&quot;$0;}&#39; test.txt web01[192.168.2.100] : httpd ok web01[192.168.2.100] : tomcat ok web01[192.168.2.100] : sendmail ok web02[192.168.2.101] : httpd ok web02[192.168.2.101] : postfix ok web03[192.168.2.102] : mysqld ok web03[192.168.2.102] : httpd ok web03[192.168.2.102] : 0 web03[192.168.2.102] : 简单地读取一条记录 awk getline用法：输出重定向需用到getline函数。getline从标准输入、管道或者当前正在处理的文件之外的其他输入文件获得输入。它负责从输入获得下一行的内容，并给NF,NR和FNR等内建变量赋值。如果得到一条记录，getline函数返回1，如果到达文件的末尾就返回0，如果出现错误，例如打开文件失败，就返回-1。 getline语法：getline var，变量var包含了特定行的内容。 awk getline从整体上来说，用法说明： 当其左右无重定向符|或&lt;时：getline作用于当前文件，读入当前文件的第一行给其后跟的变量var或$0（无变量），应该注意到，由于awk在处理getline之前已经读入了一行，所以getline得到的返回结果是隔行的。 当其左右有重定向符|或&lt;时：getline则作用于定向输入文件，由于该文件是刚打开，并没有被awk读入一行，只是getline读入，那么getline返回的是该文件的第一行，而不是隔行。 eg： 执行linux的date命令，并通过管道输出给getline，然后再把输出赋值给自定义变量out，并打印它： awk &#39;BEGIN{ &quot;date&quot; | getline out; print out }&#39; test 执行shell的date命令，并通过管道输出给getline，然后getline从管道中读取并将输入赋值给out，split函数把变量out转化成数组mon，然后打印数组mon的第二个元素： awk &#39;BEGIN{ &quot;date&quot; | getline out; split(out,mon); print mon[2] }&#39; test 命令ls的输出传递给geline作为输入，循环使getline从ls的输出中读取一行，并把它打印到屏幕。这里没有输入文件，因为BEGIN块在打开输入文件前执行，所以可以忽略输入文件。 awk &#39;BEGIN{ while( &quot;ls&quot; | getline) print }&#39; 关闭文件 awk中允许在程序中关闭一个输入或输出文件，方法是使用awk的close语句。 close(&quot;filename&quot;) filename可以是getline打开的文件，也可以是stdin，包含文件名的变量或者getline使用的确切命令。或一个输出文件，可以是stdout，包含文件名的变量或使用管道的确切命令。 输出到一个文件 awk中允许用如下方式将结果输出到一个文件： echo | awk &#39;{printf(&quot;hello word!n&quot;) &gt; &quot;datafile&quot;}&#39; 或 echo | awk &#39;{printf(&quot;hello word!n&quot;) &gt;&gt; &quot;datafile&quot;}&#39; 设置字段定界符 默认的字段定界符是空格，可以使用-F &quot;定界符&quot;明确指定一个定界符： awk -F: &#39;{ print $NF }&#39; /etc/passwd 或 awk &#39;BEGIN{ FS=&quot;:&quot; } { print $NF }&#39; /etc/passwd 在BEGIN语句块中则可以用OFS=“定界符”设置输出字段的定界符。 流程控制语句 在linux awk的while、do-while和for语句中允许使用break,continue语句来控制流程走向，也允许使用exit这样的语句来退出。break中断当前正在执行的循环并跳到循环外执行下一条语句。if 是流程选择用法。awk中，流程控制语句，语法结构，与c语言类型。有了这些语句，其实很多shell程序都可以交给awk，而且性能是非常快的。下面是各个语句用法。 条件判断语句 if(表达式) 语句1 else 语句2 格式中语句1可以是多个语句，为了方便判断和阅读，最好将多个语句用{}括起来。awk分枝结构允许嵌套，其格式为： if(表达式) {语句1} else if(表达式) {语句2} else {语句3} eg： awk &#39;BEGIN{ test=100; if(test&amp;gt;90){ print &quot;very good&quot;; } else if(test&amp;gt;60){ print &quot;good&quot;; } else{ print &quot;no pass&quot;; } }&#39; very good 每条命令语句后面可以用;分号结尾。 循环语句 while语句 while(表达式) {语句} eg： awk &#39;BEGIN{ test=100; total=0; while(i&amp;lt;=test){ total+=i; i++; } print total; }&#39; 5050 for循环 for循环有两种格式： 格式1： for(变量 in 数组) {语句} eg： awk &#39;BEGIN{ for(k in ENVIRON){ print k&quot;=&quot;ENVIRON[k]; } }&#39; TERM=linux G_BROKEN_FILENAMES=1 SHLVL=1 pwd=/root/text ... logname=root HOME=/root SSH_CLIENT=192.168.1.21 53087 22 【注】ENVIRON是awk常量，是子典型数组。 格式2： for(变量;条件;表达式) {语句} eg： awk &#39;BEGIN{ total=0; for(i=0;i&amp;lt;=100;i++){ total+=i; } print total; }&#39; do循环 do {语句} while(条件) eg： awk &#39;BEGIN{ total=0; i=0; do {total+=i;i++;} while(i&amp;lt;=100) print total; }&#39; 5050 其他语句 break 当 break 语句用于 while 或 for 语句时，导致退出程序循环。 continue 当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代。 next 能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程。 exit 语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行。 数组应用 数组是awk的灵魂，处理文本中最不能少的就是它的数组处理。因为数组索引（下标）可以是数字和字符串在awk中数组叫做关联数组(associative arrays)。awk 中的数组不必提前声明，也不必声明大小。数组元素用0或空字符串来初始化，这根据上下文而定。 数组的定义 数字做数组索引（下标）： Array[1]=&quot;sun&quot; Array[2]=&quot;kai&quot; 字符串做数组索引（下标）： Array[&quot;first&quot;]=&quot;www&quot; Array[&quot;last&quot;]=&quot;name&quot; Array[&quot;birth&quot;]=&quot;1987&quot; 使用中print Array[1]会打印出sun；使用print Array[2]会打印出kai；使用print[&quot;birth&quot;]会得到1987。 读取数组的值 { for(item in array) {print array[item]}; } #输出的顺序是随机的 { for(i=1;i&lt;=len;i++) {print array[i]}; } #Len是数组的长度 数组相关函数 得到数组长度: awk &#39;BEGIN{info=&quot;it is a test&quot;;lens=split(info,tA,&quot; &quot;);print length(tA),lens;}&#39; 4 4 length返回字符串以及数组长度，split进行分割字符串为数组，也会返回分割得到数组长度。 awk &#39;BEGIN{info=&quot;it is a test&quot;;split(info,tA,&quot; &quot;);print asort(tA);}&#39; 4 asort对数组进行排序，返回数组长度。 输出数组内容（无序，有序输出）： awk ‘BEGIN{info=”it is a test”;split(info,tA,” “);for(k in tA){print k,tA[k];}}’4 test1 it2 is3 a for…in输出，因为数组是关联数组，默认是无序的。所以通过for…in得到是无序的数组。如果需要得到有序数组，需要通过下标获得。 awk &#39;BEGIN{info=&quot;it is a test&quot;;tlen=split(info,tA,&quot; &quot;);for(k=1;k&lt;=tlen;k++){print k,tA[k];}}&#39; 1 it 2 is 3 a 4 test 注意：数组下标是从1开始，与C数组不一样。 判断键值存在以及删除键值： #错误的判断方法： awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;if(tB[&quot;c&quot;]!=&quot;1&quot;){print &quot;no found&quot;;};for(k in tB){print k,tB[k];}}&#39; no found a a1 b b1 c 以上出现奇怪问题，tB[“c”]没有定义，但是循环时候，发现已经存在该键值，它的值为空，这里需要注意，awk数组是关联数组，只要通过数组引用它的key，就会自动创建改序列。 #正确判断方法： awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;if( &quot;c&quot; in tB){print &quot;ok&quot;;};for(k in tB){print k,tB[k];}}&#39; a a1 b b1 if(key in array)通过这种方法判断数组中是否包含key键值。 #删除键值： [chengmo@localhost ~]$ awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;delete tB[&quot;a&quot;];for(k in tB){print k,tB[k];}}&#39; b b1 delete array[key]可以删除，对应数组key的，序列值。 二维、多维数组使用 awk的多维数组在本质上是一维数组，更确切一点，awk在存储上并不支持多维数组。awk提供了逻辑上模拟二维数组的访问方式。例如，array[2,4]=1这样的访问是允许的。awk使用一个特殊的字符串SUBSEP(�34)作为分割字段，在上面的例子中，关联数组array存储的键值实际上是2�344。 类似一维数组的成员测试，多维数组可以使用if ( (i,j) in array)这样的语法，但是下标必须放置在圆括号中。类似一维数组的循环访问，多维数组使用for ( item in array )这样的语法遍历数组。与一维数组不同的是，多维数组必须使用split()函数来访问单独的下标分量。 awk &#39;BEGIN{ for(i=1;i&amp;lt;=9;i++){ for(j=1;j&amp;lt;=9;j++){ tarr[i,j]=i*j; print i,&quot;*&quot;,j,&quot;=&quot;,tarr[i,j]; } } }&#39; 1 * 1 = 1 1 * 2 = 2 1 * 3 = 3 1 * 4 = 4 1 * 5 = 5 1 * 6 = 6 ... 9 * 6 = 54 9 * 7 = 63 9 * 8 = 72 9 * 9 = 81 可以通过array[k,k2]引用获得数组内容。 另一种方法： awk ‘BEGIN{ for(i=1;i&lt;=9;i++){ for(j=1;j&lt;=9;j++){ tarr[i,j]=ij; } } for(m in tarr){ split(m,tarr2,SUBSEP); print tarr2[1],”“,tarr2[2],”=”,tarr[m]; } }’ 内置函数 awk内置函数，主要分以下3种类似：算数函数、字符串函数、其它一般函数、时间函数 算术函数 格式 描述 atan2( y, x ) 返回 y/x 的反正切。 cos( x ) 返回 x 的余弦；x 是弧度。 sin( x ) 返回 x 的正弦；x 是弧度。 exp( x ) 返回 x 幂函数。 log( x ) 返回 x 的自然对数。 sqrt( x ) 返回 x 平方根。 int( x ) 返回 x 的截断至整数的值。 rand( ) 返回任意数字 n，其中 0 &lt;= n &lt; 1。 srand( [expr] ) 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。 举例说明： awk &#39;BEGIN{OFMT=&quot;%.3f&quot;;fs=sin(1);fe=exp(10);fl=log(10);fi=int(3.1415);print fs,fe,fl,fi;}&#39; 0.841 22026.466 2.303 3 OFMT 设置输出数据格式是保留3位小数。 获得随机数： awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 78 awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 31 awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 41 字符串函数 格式 描述 gsub( Ere, Repl, [ In ] ) 除了正则表达式所有具体值被替代这点，它和 sub 函数完全一样地执行。 sub( Ere, Repl, [ In ] ) 用 Repl 参数指定的字符串替换 In 参数指定的字符串中的由 Ere 参数指定的扩展正则表达式的第一个具体值。sub 函数返回替换的数量。出现在 Repl 参数指定的字符串中的 &amp;（和符号）由 In 参数指定的与 Ere 参数的指定的扩展正则表达式匹配的字符串替换。如果未指定 In 参数，缺省值是整个记录（$0 记录变量）。 index( String1, String2 ) 在由 String1 参数指定的字符串（其中有出现 String2 指定的参数）中，返回位置，从 1 开始编号。如果 String2 参数不在 String1 参数中出现，则返回 0（零）。 length [(String)] 返回 String 参数指定的字符串的长度（字符形式）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 blength [(String)] 返回 String 参数指定的字符串的长度（以字节为单位）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 substr( String, M, [ N ] ) 返回具有 N 参数指定的字符数量子串。子串从 String 参数指定的字符串取得，其字符以 M 参数指定的位置开始。M 参数指定为将 String 参数中的第一个字符作为编号 1。如果未指定 N 参数，则子串的长度将是 M 参数指定的位置到 String 参数的末尾 的长度。 match( String, Ere ) 在 String 参数指定的字符串（Ere 参数指定的扩展正则表达式出现在其中）中返回位置（字符形式），从 1 开始编号，或如果 Ere 参数不出现，则返回 0（零）。RSTART 特殊变量设置为返回值。RLENGTH 特殊变量设置为匹配的字符串的长度，或如果未找到任何匹配，则设置为 -1（负一）。 split( String, A, [Ere] ) 将 String 参数指定的参数分割为数组元素 A[1], A[2], . . ., A[n]，并返回 n 变量的值。此分隔可以通过 Ere 参数指定的扩展正则表达式进行，或用当前字段分隔符（FS 特殊变量）来进行（如果没有给出 Ere 参数）。除非上下文指明特定的元素还应具有一个数字值，否则 A 数组中的元素用字符串值来创建。 tolower( String ) 返回 String 参数指定的字符串，字符串中每个大写字符将更改为小写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 toupper( String ) 返回 String 参数指定的字符串，字符串中每个小写字符将更改为大写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 sprintf(Format, Expr, Expr, . . . ) 根据 Format 参数指定的 printf 子例程格式字符串来格式化 Expr 参数指定的表达式并返回最后生成的字符串。 注：Ere都可以是正则表达式。 gsub,sub使用 awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;gsub(/[0-9]+/,&quot;!&quot;,info);print info}&#39; this is a test!test! 在 info中查找满足正则表达式，/[0-9]+/用””替换，并且替换后的值，赋值给info 未给info值，默认是$0 查找字符串（index使用） awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print index(info,&quot;test&quot;)?&quot;ok&quot;:&quot;no found&quot;;}&#39; ok 未找到，返回0 正则表达式匹配查找(match使用） awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print match(info,/[0-9]+/)?&quot;ok&quot;:&quot;no found&quot;;}&#39; ok 截取字符串(substr使用） [wangsl@centos5 ~]$ awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print substr(info,4,10);}&#39; s is a tes 从第 4个 字符开始，截取10个长度字符串 字符串分割（split使用） awk &#39;BEGIN{info=&quot;this is a test&quot;;split(info,tA,&quot; &quot;);print length(tA);for(k in tA){print k,tA[k];}}&#39; 4 4 test 1 this 2 is 3 a 分割info，动态创建数组tA，这里比较有意思，awk for …in循环，是一个无序的循环。 并不是从数组下标1…n ，因此使用时候需要注意。 格式化字符串输出（sprintf使用） 格式化字符串格式： 其中格式化字符串包括两部分内容：一部分是正常字符，这些字符将按原样输出; 另一部分是格式化规定字符，以”%”开始，后跟一个或几个规定字符,用来确定输出内容格式。 格式 描述 %d 十进制有符号整数 %u 十进制无符号整数 %f 浮点数 %s 字符串 %c 单个字符 %p 指针的值 %e 指数形式的浮点数 %x %X无符号以十六进制表示的整数 %o 无符号以八进制表示的整数 %g 自动选择合适的表示法 awk &#39;BEGIN{n1=124.113;n2=-1.224;n3=1.2345; printf(&quot;%.2f,%.2u,%.2g,%X,%on&quot;,n1,n2,n3,n1,n1);}&#39; 124.11,18446744073709551615,1.2,7C,174 一般函数 格式 描述 close( Expression ) 用同一个带字符串值的 Expression 参数来关闭由 print 或 printf 语句打开的或调用 getline 函数打开的文件或管道。如果文件或管道成功关闭，则返回 0；其它情况下返回非零值。如果打算写一个文件，并稍后在同一个程序中读取文件，则 close 语句是必需的。 system(command ) 执行 Command 参数指定的命令，并返回退出状态。等同于 system 子例程。 Expression&#124;getline [ Variable ] 从来自 Expression 参数指定的命令的输出中通过管道传送的流中读取一个输入记录，并将该记录的值指定给 Variable 参数指定的变量。如果当前未打开将 Expression 参数的值作为其命令名称的流，则创建流。创建的流等同于调用 popen 子例程，此时 Command 参数取 Expression 参数的值且 Mode 参数设置为一个是 r 的值。只要流保留打开且 Expression 参数求得同一个字符串，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。 getline [ Variable ] &lt; Expression 从 Expression 参数指定的文件读取输入的下一个记录，并将 Variable 参数指定的变量设置为该记录的值。只要流保留打开且 Expression 参数对同一个字符串求值，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。 getline [ Variable ] 将 Variable 参数指定的变量设置为从当前输入文件读取的下一个输入记录。如果未指定 Variable 参数，则 $0 记录变量设置为该记录的值，还将设置 NF、NR 和 FNR 特殊变量。 打开外部文件（close用法） awk &#39;BEGIN{while(&quot;cat /etc/passwd&quot;|getline){print $0;};close(&quot;/etc/passwd&quot;);}&#39; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin 逐行读取外部文件(getline使用方法） awk &#39;BEGIN{while(getline &lt; &quot;/etc/passwd&quot;){print $0;};close(&quot;/etc/passwd&quot;);}&#39; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin awk &#39;BEGIN{print &quot;Enter your name:&quot;;getline name;print name;}&#39; Enter your name: chengmo chengmo 调用外部应用程序(system使用方法） awk &#39;BEGIN{b=system(&quot;ls -al&quot;);print b;}&#39; total 42092 drwxr-xr-x 14 chengmo chengmo 4096 09-30 17:47 . drwxr-xr-x 95 root root 4096 10-08 14:01 .. b返回值，是执行结果。 时间函数 格式 描述 函数名 说明 mktime( YYYY MM dd HH MM ss[ DST]) 生成时间格式 strftime([format [, timestamp]]) 格式化时间输出，将时间戳转为时间字符串 具体格式，见下表. systime() 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数 建指定时间(mktime使用） awk &#39;BEGIN{tstamp=mktime(&quot;2001 01 01 12 12 12&quot;);print strftime(&quot;%c&quot;,tstamp);}&#39; 2001年01月01日 星期一 12时12分12秒 awk &#39;BEGIN{tstamp1=mktime(&quot;2001 01 01 12 12 12&quot;);tstamp2=mktime(&quot;2001 02 01 0 0 0&quot;);print tstamp2-tstamp1;}&#39; 2634468 求2个时间段中间时间差，介绍了strftime使用方法 awk &#39;BEGIN{tstamp1=mktime(&quot;2001 01 01 12 12 12&quot;);tstamp2=systime();print tstamp2-tstamp1;}&#39; 308201392 strftime日期和时间格式说明符 格式 描述 %a 星期几的缩写(Sun) %A 星期几的完整写法(Sunday) %b 月名的缩写(Oct) %B 月名的完整写法(October) %c 本地日期和时间 %d 十进制日期 %D 日期 08/20/99 %e 日期，如果只有一位会补上一个空格 %H 用十进制表示24小时格式的小时 %I 用十进制表示12小时格式的小时 %j 从1月1日起一年中的第几天 %m 十进制表示的月份 %M 十进制表示的分钟 %p 12小时表示法(AM/PM) %S 十进制表示的秒 %U 十进制表示的一年中的第几个星期(星期天作为一个星期的开始) %w 十进制表示的星期几(星期天是0) %W 十进制表示的一年中的第几个星期(星期一作为一个星期的开始) %x 重新设置本地日期(08/20/99) %X 重新设置本地时间(12：00：00) %y 两位数字表示的年(99) %Y 当前月份 %Z 时区(PDT) %% 百分号(%) sed 对数据行进行替换、删除、新增、选取等操作 a 新增，在新的下一行出现 c 取代，取代 n1,n2 之间的行 eg: sed &#39;1,2c Hi&#39; ab d 删除 i 插入，在新的上一行出现 eg: #指定时间段查看日志 sed -n &#39;/2016-10-21 14:18:29/,/2016-10-21 18:18:29/p&#39; catalina.out #替换当前目录下所有文件中的 /usr/local为/data/dshp sed -i &quot;s/\/usr\/local/\/data\/dshp/g&quot; . paste 合并文件，需确保合并的两文件行数相同 -d 指定不同于空格或tab键的域分隔符 -s 按行合并，单独一个文件为一行 rename 重命名文件 #批量重命名相同前缀的文件 #将当前目录下所有以central开头的文件中的central替换为distributed rename central distributed central* wc wc 计算数字 利用wc指令我们可以计算文件的Byte数、字数或是列数，若不指定文件名称，或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据 wc(选项)(参数) #(选项) -c或--bytes或——chars：只显示Bytes数； -l或——lines：只显示列数； -w或——words：只显示字数。 #(参数) 文件：需要统计的文件列表。 统计当前文件夹下文件的个数 ls -l |grep &quot;^-&quot;|wc -l 统计当前文件夹下目录的个数 ls -l |grep &quot;^d&quot;|wc -l 统计当前文件夹下文件的个数，包括子文件夹里的 ls -lR|grep &quot;^-&quot;|wc -l 统计文件夹下目录的个数，包括子文件夹里的 ls -lR|grep &quot;^d&quot;|wc -l ls -l #长列表输出当前文件夹下文件信息(注意这里的文件，不同于一般的文件，可能是目录、链接、设备文件等) grep &quot;^-&quot; #这里将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d wc -l #统计当前目录下指定文件后缀的行数，既可以统计项目的代码量 find . -name &quot;*.java&quot; -or -name &quot;*.jsp&quot; -or -name &quot;*.xml&quot; -or -name &quot;*.c&quot; |xargs grep -v &quot;^$&quot;|wc -l #统计输出信息的行数，因为已经过滤得只剩一般文件了，所以统计结果就是一般文件信息的行数，又由于一行信息对应一个文件，所以也就是文件的个数 uniq uniq 去除文件中相邻的重复行 清空/新建文件，将内容重定向输入进去 &amp;&gt; 正确、错误都重定向过去 后面追加 file 该命令用于判断接在file命令后的文件的基本数据，因为在Linux下文件的类型并不是以后缀为分的，所以这个命令对我们来说就很有用了，它的用法非常简单，基本语法如下： [plain] view plain copy print? file filename #例如： file ./test mkdir 创建新目录 mkdir [选项] 目录… -p #递归创建目录，若父目录不存在则依次创建 eg: mkdir -p ~/temp/test -m #自定义创建目录的权限 eg:mkdir -m 777 temp -v #显示创建目录的详细信息 eg:mkdir -m 662 -pv ~/temp/test grep 用正则表达式搜索文本，并把匹配的行打印出来 grep ‘正则表达式’ 文件名 | -c 只输出匹配行的计数。 -I 不区分大小写(只适用于单字符)。 -l 只显示文件名 -v 显示不包含匹配文本的所有行。 -n 显示匹配行数据及其行号 eg: # 取出文件urls.txt中包含mysql的行，并把找到的关键字加上颜色 grep --color=auto &#39;mysql&#39; urls.txt # 把ls -l的输出中包含字母file（不区分大小写）的内容输出 ls -l | grep -i file # 查找当前目录下所有包含mysql的文件并逐行显示,文件路径+行号+匹配内容 grep -rn &quot;mysql&quot; ./* find 在文件树种查找文件，并作出相应的处理 find [PATH] [option] [action] 选项与参数： 与时间有关的选项：共有 -atime, -ctime 与 -mtime 和-amin,-cmin与-mmin，以 -mtime 说明 -mtime n ：n 为数字，意义为在 n 天之前的『一天之内』被更动过内容的档案； -mtime +n ：列出在 n 天之前(不含 n 天本身)被更动过内容的档案档名； -mtime -n ：列出在 n 天之内(含 n 天本身)被更动过内容的档案档名。 -newer file ：file 为一个存在的档案，列出比 file 还要新的档案档名 eg： find ./ -mtime 0 # 在当前目录下查找今天之内有改动的文件 与使用者或组名有关的参数： -uid n ：n 为数字，这个数字是用户的账号 ID，亦即 UID -gid n ：n 为数字，这个数字是组名的 ID，亦即 GID -user name ：name 为使用者账号名称！例如 dmtsai -group name：name 为组名，例如 users ； -nouser ：寻找档案的拥有者不存在 /etc/passwd 的人！ -nogroup ：寻找档案的拥有群组不存在于 /etc/group 的档案！ eg： find /home/jet -user jet # 在目录/home/jet中找出所有者为jet的文件 与档案权限及名称有关的参数： -name filename #搜寻文件名为 filename 的档案（可使用通配符） -size [+-]SIZE #搜寻比 SIZE 还要大(+)或小(-)的档案。这个 SIZE 的规格有： c: 代表 byte k: 代表 1024bytes。所以，要找比 50KB还要大的档案，就是『 -size +50k 』 -type TYPE #搜寻档案的类型为 TYPE 的，类型主要有： 一般正规档案 (f) 装置档案 (b, c) 目录 (d) 连结档 (l) socket (s) FIFO (p) -perm mode #搜寻档案权限『刚好等于』 mode的档案，这个mode为类似chmod的属性值 举例来说，-rwsr-xr-x 的属性为4755！ -perm -mode #搜寻档案权限『必须要全部囊括 mode 的权限』的档案 举例来说，我们要搜寻-rwxr--r-- 亦即 0744 的档案，使用-perm -0744，当一个档案的权限为 -rwsr-xr-x ，亦即 4755 时，也会被列出来，因为 -rwsr-xr-x 的属性已经囊括了 -rwxr--r-- 的属性了。 -perm +mode #搜寻档案权限『包含任一 mode 的权限』的档案 举例来说，我们搜寻-rwxr-xr-x ，亦即 -perm +755 时，但一个文件属性为 -rw-------也会被列出来，因为他有 -rw.... 的属性存在！ eg： find / -name passwd # 查找文件名为passwd的文件 find . -perm 0755 # 查找当前目录中文件权限的0755的文件 find . -size +12k # 查找当前目录中大于12KB的文件，注意c表示byte 额外可进行的动作： -exec command #command 为其他指令，-exec 后面可再接额外的指令来处理搜寻到的结果。 -print ：将结果打印到屏幕上，这个动作是预设动作！ eg: find / -perm +7000 -exec ls -l {} \; #额外指令以-exec开头，以\;结尾{}代替前面找到的内容 | xargs -i 默认的前面输出用{}代替 eg: # 将当前目录下所有以.log结尾的文件移动到文件夹logs中，logs文件夹需要先建立好，不然会生成一个logs空文件 find . -name &quot;*.log&quot; | xargs -i mv {} logs # 查找当前目录下所有log结尾的文件并删除 find . -name *.log | xargs rm dd 用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换(convert and copy a file ) 语法：dd [选项] if =输入文件（或设备名称）。 of =输出文件（或设备名称）。 ibs = bytes 一次读取bytes字节，即读入缓冲区的字节数。 skip = blocks 跳过读入缓冲区开头的ibs*blocks块。 obs = bytes 一次写入bytes字节，即写入缓冲区的字节数。 bs = bytes 同时设置读/写缓冲区的字节数（等于设置ibs和obs）。 cbs = byte 一次转换bytes字节。 count=blocks 只拷贝输入的blocks块。 conv = ASCII 把EBCDIC码转换为ASCIl码。 conv = ebcdic 把ASCIl码转换为EBCDIC码。 conv = ibm 把ASCIl码转换为alternate EBCDIC码。 conv = block 把变动位转换成固定字符。 conv = ublock 把固定位转换成变动位。 conv = ucase 把字母由小写转换为大写。 conv = lcase 把字母由大写转换为小写。 conv = notrunc 不截短输出文件。 conv = swab 交换每一对输入字节。 conv = noerror 出错时不停止处理。 conv = sync 把每个输入记录的大小都调到ibs的大小（用NUL填充）。 例1：要把一张软盘的内容拷贝到另一张软盘上，利用/tmp作为临时存储区。把源盘插入驱动器中，输入下述命令： $ dd if =/dev/fd0 of = /tmp/tmpfile 拷贝完成后，将源盘从驱动器中取出，把目标盘插入，输入命令： $ dd if = /tmp/tmpfile of =/dev/fd0 软盘拷贝完成后，应该将临时文件删除： $ rm /tmp/tmpfile 例2：把net.i这个文件写入软盘中，并设定读/写缓冲区的数目。 （注意：软盘中的内容会被完全覆盖掉） $ dd if = net.i of = /dev/fd0 bs = 16384 例3：将文件sfile拷贝到文件 dfile中。 $ dd if=sfile of=dfile 例4：创建一个100M的空文件 $ dd if=/dev/zero of=hello.txt bs=100M count=1 # 创建一个大小为1k的空文件 $ dd if=/dev/zero of=./test.txt bs=1k count=1 $ ls -l total 4 -rw-rw-r--. 1 jet jet 1024 Jun 20 16:36 test.txt # 将access_log中错误信息丢弃 $ find / -name access_log 2&gt;/dev/null &gt;-&gt;&gt;-/dev/null-/dev/zero /dev/null: 它是空设备，也称为位桶（bit bucket），外号叫无底洞，任何写入它的输出都会被抛弃。如果不想让消息以标准输出显示或写入文件，那么可以将消息重定向到位桶。 /dev/zero: 是一个输入设备，该设备无穷尽地提供0，可以使用任何你需要的数目，用于向设备或文件写入字符串0，你可你用它来初始化文件。 &lt; ：由 &lt; 的右边读入参数档案； &gt; ：将原本由屏幕输出的正确数据输出到 &gt; 右边的 file ( 文件名称 ) 或 device ( 装置，如 printer )去； &gt;&gt; ：将原本由屏幕输出的正确数据输出到 &gt;&gt; 右边，与 &gt; 不同的是，该档案将不会被覆盖，而新的数据将以『增加的方式』增加到该档案的最后面； 2&gt; ：将原本应该由屏幕输出的错误数据输出到 2&gt; 的右边去。 说明 [jet @jet oschina_hexo_server]# ls -al &gt; test.txt # 将显示的结果输出到 test.txt 档案中，若该档案以存在则覆盖！ [jet @jet oschina_hexo_server]# ls -al &gt;&gt; test.txt # 将显示的结果追加到 test.txt 档案中，该档案为累加的，旧数据保留！ [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt; test.err # 将显示的数据，正确的输出到 test.txt 错误的数据输出到 test.err [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt;&amp;1 # 将显示的数据，不论正确或错误均输出到 test.txt 当中！ [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt; /dev/null # 将显示的数据，正确的输出到 test.txt 错误的数据则予以丢弃！ 【注意】错误与正确档案输出到同一个档案中，则必须以上面的方法来写！ 不能写成其它格式！这个观念相当的重要，尤其是在 /etc/crontab 当中执行的时候，如果我们已经知道错误的讯息为何，又不想要让错误的讯息一直填满 root 的信箱，就必须以 2&gt; 搭配 /dev/null 这个垃圾桶黑洞装置，来将数据丢弃！这个相当的重要！ rm 删除文件 rm [选项] 文件 -r 【--recursive】删除文件夹即递归的删除目录下面文件以及子目录下文件。 -f 【--force】强制删除不提示，忽略不存在的文件。 -i 【--interactive】交互模式删除文件，删除文件前给出提示 -v 【-verbose】详细显示进行步骤 cd 切换工作目录 cd . #返回上层目录 cd .. #返回上层目录 cd 回车 #返回主目录同cd ~ cd / #根目录 cd ~/git/ #主目录下的git目录 cd - #回到之前的目录 ls 列出相关目录下的所有目录和文件 ls [选项] [目录名] -a 列出包括.a开头的隐藏文件的所有文件 -A 通-a，但不列出&quot;.&quot;和&quot;..&quot; -l 列出文件的详细信息 -c 根据ctime排序显示 -t 根据文件修改时间排序 ---color[=WHEN] 用色彩辨别文件类型 WHEN 可以是’never’、’always’或’auto’其中之一 白色：表示普通文件 蓝色：表示目录 绿色：表示可执行文件 红色：表示压缩文件 浅蓝色：链接文件 红色闪烁：表示链接的文件有问题 黄色：表示设备文件 灰色：表示其它文件 mv 移动或重命名文件 mv [选项] 源文件或目录 目录或多个源文件 -b 覆盖前做备份 -f 如存在不询问而强制覆盖 -i 如存在则询问是否覆盖 -u 较新才覆盖 -t 将多个源文件移动到统一目录下，目录参数在前，文件参数在后 eg: mv a /tmp/ 将文件a移动到 /tmp目录下 mv a b 将a命名为b mv /home/zenghao test1.txt test2.txt test3.txt cp 将源文件复制至目标文件，或将多个源文件复制至目标目录。 cp [选项] 源文件或目录 目录或多个源文件 -r -R #递归复制该目录及其子目录内容 -p #连同档案属性一起复制过去 -f #不询问而强制复制 -s #生成快捷方式 -a #将档案的所有特性都一起复制 eg: cp -a file1 file2 #连同文件的所有特性把文件file1复制成文件file2 cp file1 file2 file3 dir #把文件file1、file2、file3复制到目录dir中 touch 创建空文件或更新文件时间 touch [选项] 文件 -a #只修改存取时间 -m #值修改变动时间 -r #eg:touch -r a b ,使b的时间和a相同 -t #指定特定的时间 eg:touch -t 201211142234.50 log.log #-t time [[CC]YY]MMDDhhmm[.SS],C:年前两位 pwd 查看当前所在路径 rmdir 删除空目录 -v 显示执行过程 -p 若自父母删除后父目录为空则一并删除 rm 删除一个或多个文件或目录 rm [选项].. 文件 -f 忽略不存在的文件，不给出提示 -i 交互式删除 -r 将列出的目录及其子目录递归删除 -v 列出详细信息 echo 显示内容到屏幕 -n 输出后不换行 -e 遇到转义字符特殊处理 eg: echo &quot;hello\nworld&quot; 显示hello\nworld ehco -e &quot;hello\nworld&quot; 显示hello(换行了)world cat 一次显示整个文件或从键盘创建一个文件或将几个文件合并成一个文件 cat [选项] [文件].. -n 编号文件内容再输出 -E 在结束行提示$ tac cat的反向显示 more 按页查看文章内容，从前向后读取文件，因此在启动时就加载整个文件 +n 从第n行开始显示 -n 每次查看n行数据 +/String 搜寻String字符串位置，从其前两行开始查看 -c 清屏再显示 -p 换页时清屏 less 可前后移动地逐屏查看文章内容，在查看前不会加载整个文件 -m 显示类似于more命令的百分比 -N 显示行号 / 字符串：向下搜索“字符串”的功能 ? 字符串：向上搜索“字符串”的功能 n 重复前一个搜索（与 / 或 ? 有关） N 反向重复前一个搜索（与 / 或 ? 有关） b 向后翻一页 d 向后翻半页 nl 将输出内容自动加上行号 nl [选项]… [文件]… -b -b a 不论是否有空行，都列出行号（类似 cat -n) -b t 空行则不列行号（默认） -n 有ln rn rz三个参数，分别为再最左方显示，最右方显示不加0，最右方显示加0 head 显示档案开头，默认开头10行 head [参数]… [文件]… -v 显示文件名 -c number 显示前number个字符,若number为负数,则显示除最后number个字符的所有内容 -number/n (+)number 显示前number行内容， -n number 若number为负数，则显示除最后number行数据的所有内容 tail 显示文件结尾内容 tail [必要参数] [选择参数] [文件] -v #显示详细的处理信息 -q #不显示处理信息 -num/-n (-)num #显示最后num行内容 -n +num #从第num行开始显示后面的数据 -c #显示最后c个字符 -f #循环读取 # 实时查看日志，从文件最后50行开始 tail -fn 50 ExecuteConfig.log vi 编辑文件 :w filename #将文章以指定的文件名保存起来 :q #退出 :q! #强制退出 :wq #保存并退出 :set nu #显示行号 :set nonu #隐藏行号 /git #在文档中查找git 按n跳到下一个，shift+n上一个 yyp #复制光标所在行，并粘贴 h(左移一个字符←)、j(下一行↓)、k(上一行↑)、l(右移一个字符→) 命令行模式功能键 1. 插入模式 按「i」切换进入插入模式「insert mode」，按&quot;i&quot;进入插入模式后是从光标当前位置开始输入文件； 按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字； 按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。 2. 从插入模式切换为命令行模式 按「ESC」键。 3. 移动光标 vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。 按「ctrl」+「b」#屏幕往&quot;后&quot;移动一页。 按「ctrl」+「f」#屏幕往&quot;前&quot;移动一页。 按「ctrl」+「u」#屏幕往&quot;后&quot;移动半页。 按「ctrl」+「d」#屏幕往&quot;前&quot;移动半页。 按数字「0」#移到文章的开头。 按「G」#移动到文章的最后。 按「$」#移动到光标所在行的&quot;行尾&quot;。 按「^」#移动到光标所在行的&quot;行首&quot; 按「w」#光标跳到下个字的开头 按「e」#光标跳到下个字的字尾 按「b」#光标回到上个字的开头 按「#l」#光标移到该行的第#个位置，如：5l,56l。 4. 删除文字 「x」#每按一次，删除光标所在位置的&quot;后面&quot;一个字符。 「#x」#例如，「6x」表示删除光标所在位置的&quot;后面&quot;6个字符。 「X」#大写的X，每按一次，删除光标所在位置的&quot;前面&quot;一个字符。 「#X」#例如，「20X」表示删除光标所在位置的&quot;前面&quot;20个字符。 「dd」#删除光标所在行。 「#dd」#从光标所在行开始删除#行 5. 复制 「yw」#将光标所在之处到字尾的字符复制到缓冲区中。 「#yw」#复制#个字到缓冲区 「yy」#复制光标所在行到缓冲区。 「#yy」#例如，「6yy」表示拷贝从光标所在的该行&quot;往下数&quot;6行文字。 「p」#将缓冲区内的字符贴到光标所在位置。注意：所有与&quot;y&quot;有关的复制命令都必须与&quot;p&quot;配合才能完成复制与粘贴功能。 6. 替换 「r」#替换光标所在处的字符。 「R」#替换光标所到之处的字符，直到按下「ESC」键为止。 7. 恢复上一次操作 「u」#如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次&quot;u&quot;可以执行多次回复。 8. 更改 「cw」#更改光标所在处的字到字尾处 「c#w」#例如，「c3w」表示更改3个字 9. 跳至指定的行 「ctrl」+「g」 #列出光标所在行的行号。 「#G」：例如，「15G」，表示移动光标至文章的第15行行首。 10.视图模式 「ctrl」+「v」#进入视图模式，可以移动方向键选中多行，按d键可以删除 11.清空文件 :.,$d 回车 #命令模式输入 .,$d 后回车 12.替换文本内容 #可以替换当前行的所有/usr/local为/data/dshp，命令模式输入s/old\new/g :s/\/usr\/local/\/data\/dshp/g 管道 将一个命令的标准输出作为另一个命令的标准输入。也就是把几个命令组合起来使用，后一个命令除以前一个命令的结果。 grep -r “close” /home/* | more 在home目录下所有文件中查找，包括close的文件，并分页输出。 xargs 管道实现的是将前面的stdout作为后面的stdin，但是有些命令不接受管道的传递方式，最常见的就是ls命令。有些时候命令希望管道传递的是参数，但是直接用管道有时无法传递到命令的参数位，这时候需要xargs，xargs实现的是将管道传输过来的stdin进行处理然后传递到命令的参数位上。也就是说xargs完成了两个行为：处理管道传输过来的stdin；将处理后的传递到正确的位置上。 -0 当sdtin含有特殊字元时候，将其当成一般字符，如/、空格等 例如：root@localhost:~/test#echo &quot;//&quot;|xargs echo root@localhost:~/test#echo &quot;//&quot;|xargs -0 echo / -a file 从文件中读入作为sdtin，（看例一） -e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。（例二） -p 当每次执行一个argument的时候询问一次用户。（例三） -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。（例四） -t 表示先打印命令，然后再执行。（例五） -i 或者是-I，这得看Linux支持了，将xargs的每项名称，一般是一行一行赋值给{}，可以用{}代替。（例六） -r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。（例七） -s num 命令行的最好字符数，指的是xargs后面那个命令的最大命令行字符数。（例八） -L num Use at most max-lines nonblank input lines per command line.-s是含有空格的。 -l 同-L -d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符（例九） -x exit的意思，主要是配合-s使用。 -P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 示例例一： root@localhost:~/test#cat test #!/bin/sh echo &quot;hello world/n&quot; root@localhost:~/test#xargs -a test echo #!/bin/sh echo hello world/n 例二： root@localhost:~/test#cat txt /bin tao shou kun root@localhost:~/test#cat txt|xargs -E &#39;shou&#39; echo /bin tao 例三： root@localhost:~/test#cat txt|xargs -p echo echo /bin tao shou kun ff ?...y /bin tao shou kun ff 例四： root@localhost:~/test#cat txt|xargs -n1 echo /bin tao shou kun root@localhost:~/test3#cat txt|xargs echo /bin tao shou kun 例五： root@localhost:~/test#cat txt|xargs -t echo echo /bin tao shou kun /bin tao shou kun 例六： $ ls | xargs -t -i mv {} {}.bak 例七： root@localhost:~/test#echo &quot;&quot;|xargs -t mv mv mv: missing file operand Try `mv --help&#39; for more information. root@localhost:~/test#echo &quot;&quot;|xargs -t -r mv root@localhost:~/test# （直接退出） 例八： root@localhost:~/test#cat test |xargs -i -x -s 14 echo &quot;{}&quot; exp1 exp5 file xargs: argument line too long linux-2 例九： root@localhost:~/test#cat txt |xargs -i -p echo {} echo /bin tao shou kun ?...y root@localhost:~/test#cat txt |xargs -i -p -d &quot; &quot; echo {} echo /bin ?...y echo tao ?.../bin y echo shou ?...tao 再如： root@localhost:~/test#cat test |xargs -i -p -d &quot; &quot; echo {} echo exp1 exp5 file linux-2 ngis_post tao test txt xen-3 ?...y root@localhost:~/test#cat test |xargs -i -p echo {} echo exp1 ?...y echo exp5 ?...exp1 y echo file ?...exp5 y diff 以逐行的方式，比较文本文件的异同处。指定要比较目录，则diff会比较目录中相同文件名的文件，但不会比较其中子目录。 diff [参数] [文件1或目录1] [文件2或目录2] -&lt;行数&gt; 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或--text diff预设只会逐行比较文本文件。 -b或--ignore-space-change 不检查空格字符的不同。 -B或--ignore-blank-lines 不检查空白行。 -c 显示全部内文，并标出不同之处。 -C&lt;行数&gt;或--context&lt;行数&gt; 与执行&quot;-c-&lt;行数&gt;&quot;指令相同。 -d或--minimal 使用不同的演算法，以较小的单位来做比较。 -D&lt;巨集名称&gt;或ifdef&lt;巨集名称&gt; 此参数的输出格式可用于前置处理器巨集。 -e或--ed 此参数的输出格式可用于ed的script文件。 -f或-forward-ed 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。 -H或--speed-large-files 比较大文件时，可加快速度。 -l&lt;字符或字符串&gt;或--ignore-matching-lines&lt;字符或字符串&gt; 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。 -i或--ignore-case 不检查大小写的不同。 -l或--paginate 将结果交由pr程序来分页。 -n或--rcs 将比较结果以RCS的格式来显示。 -N或--new-file 在比较目录时，若文件A仅出现在某个目录中，预设会显示： Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。 -p 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。 -P或--unidirectional-new-file 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。 -q或--brief 仅显示有无差异，不显示详细的信息。 -r或--recursive 比较子目录中的文件。 -s或--report-identical-files 若没有发现任何差异，仍然显示信息。 -S&lt;文件&gt;或--starting-file&lt;文件&gt; 在比较目录时，从指定的文件开始比较。 -t或--expand-tabs 在输出时，将tab字符展开。 -T或--initial-tab 在每行前面加上tab字符以便对齐。 -u,-U&lt;列数&gt;或--unified=&lt;列数&gt; 以合并的方式来显示文件内容的不同。 -v或--version 显示版本信息。 -w或--ignore-all-space 忽略全部的空格字符。 -W&lt;宽度&gt;或--width&lt;宽度&gt; 在使用-y参数时，指定栏宽。 -x&lt;文件名或目录&gt;或--exclude&lt;文件名或目录&gt; 不比较选项中所指定的文件或目录。 -X&lt;文件&gt;或--exclude-from&lt;文件&gt; 您可以将文件或目录类型存成文本文件，然后在=&lt;文件&gt;中指定此文本文件。 -y或--side-by-side 以并列的方式显示文件的异同之处。 --help 显示帮助。 --left-column 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。 --suppress-common-lines 在使用-y参数时，仅显示不同之处 示例： 比较两个文件 [root@localhost test3]# diff log2014.log log2013.log 3c3 &lt; 2014-03 --- &gt; 2013-03 8c8 &lt; 2013-07 --- &gt; 2013-08 11,12d10 &lt; 2013-11 &lt; 2013-12 【注】 上面的”3c3”和”8c8”表示log2014.log和log20143log文件在3行和第8行内容有所不同；”11,12d10”表示第一个文件比第二个文件多了第11和12行。 并排格式输出 [root@localhost test3]# diff log2014.log log2013.log -y -W 50 2013-01 2013-01 2013-02 2013-02 2014-03 | 2013-03 2013-04 2013-04 2013-05 2013-05 2013-06 2013-06 2013-07 2013-07 2013-07 | 2013-08 2013-09 2013-09 2013-10 2013-10 2013-11 &lt; 2013-12 &lt; [root@localhost test3]# diff log2013.log log2014.log -y -W 50 2013-01 2013-01 2013-02 2013-02 2013-03 | 2014-03 2013-04 2013-04 2013-05 2013-05 2013-06 2013-06 2013-07 2013-07 2013-08 | 2013-07 2013-09 2013-09 2013-10 2013-10 &gt; 2013-11 &gt; 2013-12 【注】 “|”表示前后2个文件内容有不同 “&lt;”表示后面文件比前面文件少了1行内容 “&gt;”表示后面文件比前面文件多了1行内容 ln 某一个文件在另外一个位置建立一个同步的链接，通常给/usr/bin/下建立软连接，相当于给某个应用程序配置环境变量一样，可以不带路径直接运行命令 ln [参数] [源文件或目录] [目标文件或目录] -s 建立软连接 -v 显示详细的处理过程 which 查看可执行文件的位置，在PATH变量指定的路径中查看系统命令是否存在及其位置 which 可执行文件名称 whereis 定位可执行文件、源代码文件、帮助文件在文件系统中的位置 whereis [-bmsu] [BMS 目录名 -f ] 文件名 -b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 -B 指定搜索可执行文件的路径。 -M 指定搜索帮助文件的路径。 -S 指定搜索源代码文件的路径。 locate 通过搜寻数据库快速搜寻档案 -r 使用正规运算式做寻找的条件 系统管理相关命令ps 列出当前进程的快照 a 显示所有的进程 -a 显示同一终端下的所有程序 e 显示环境变量 f 显示进程间的关系 -H 显示树状结构 r 显示当前终端的程序 T 显示当前终端的所有程序 -au 显示更详细的信息 -aux 显示所有包含其他使用者的行程 -u 指定用户的所有进程 eg: ps aux # 查看系统所有的进程数据 ps ax # 查看不与terminal有关的所有进程 ps -lA # 查看系统所有的进程数据 ps axjf # 查看连同一部分进程树状态 ps aux | grep tomcat ps -ef | grep tomcat df 显示指定磁盘文件的可用空间,如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示 df [选项] [文件] -a 显示全部文件系统 -h 文件大小友好显示，即会显示单位 -l 只显示本地文件系统 -i 显示inode信息 -T 显示文件系统类型 du 显示每个文件和目录的磁盘使用空间 du [选项] [文件] -h 方便阅读的方式，会带单位 -s 只显示总和的大小 eg: #按照文件/文件夹从大到小降序排列，方便查找大文件 du -sh * | sort -nr | head top 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 top [参数] free 显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer free [参数] quota 显示用户或者工作组的磁盘配额信息。输出信息包括磁盘使用和配额限制 quota(选项)(参数) -g：列出群组的磁盘空间限制； -q：简明列表，只列出超过限制的部分； -u：列出用户的磁盘空间限制； -v：显示该用户或群组，在所有挂入系统的存储设备的空间限制； -V：显示版本信息。 我们可以限制某一群组所能使用的最大磁盘配额，而且可以再限制某一使用者的最大磁盘配额 ，好比做一个收费的应用，vip可以得到空间更大一些。另外，以 Link 的方式，来使邮件可以作为限制的配额（更改/var/spool/mail 这个路径），不2，需要重新再规划一个硬盘！直接使用 Link 的方式指向 /home （或者其它已经做好的 quota 磁盘）就可以！这通常是用在原本规划不好，但是却又不想要更动原有主机架构的情况中！ 要求：Linux 主机里面主要针对 quser1 及 quser2 两个使用者来进行磁盘配额， 且这两个使用者都是挂在 qgroup 组里面的。每个使用者总共有 50MB 的磁盘空间 (不考虑 inode) 限制！并且 soft limit 为 45 MB；而宽限时间设定为 1 天， 但是在一天之内必须要将多余的文件删除掉，否则将无法使用剩下的空间 ；gquota 这个组考虑最大限额，所以设定为 90 MB！（注意，这样设置的好处是富有弹性，好比现在的邮件服务，那么多用户，承诺给用户每人最大空间为数GB，然而人们不可能每人都会使用那么大的空间，所以邮件服务的总空间，实际上肯定不是注册客户数乘以数GB，否则这样得多大啊。） [root@jet ~]# groupadd qgroup [root@jet ~]# useradd -m -g qgroup quser1 [root@jet ~]# useradd -m -g qgroup quser2 [root@jet ~]# passwd quser1 [root@jet ~]# passwd quser2 [root@jet ~]# df #用/disk2测试 Filesystem 1K-blocks Used Available Use% Mounted on /dev/hda1 5952252 3193292 2451720 57% / /dev/hdb1 28267608 77904 26730604 1% /disk2 /dev/hda5 9492644 227252 8775412 3% /disk1 [root@jet ~]# vi /etc/fstab LABEL=/ / ext3 defaults 1 1 LABEL=/disk1 /disk1 ext3 defaults 1 2 LABEL=/disk2 /disk2 ext3 defaults,usrquota,grpquota 1 2 /dev/hda3 swap swap defaults 0 0 注意多了usrquota,grpquota，在defaults,usrquota,grpquota之间都没有空格，务必正确书写。这样就算加入了 quota 的磁盘格式了！不过，由于真正的 quota 在读取的时候是读取/etc/mtab这个文件的，而该文件需要重新开机之后才能够以/etc/fstab 的新数据进行改写！所以这个时候可以选择：重新开机 (reboot)。 重新remount filesystem来驱动设定值。 [root@jet ~]# umount /dev/hdb1 [root@jet ~]# mount -a [root@jet ~]# grep &#39;/disk2&#39; /etc/mtab /dev/hdb1 /disk2 ext3 rw,usrquota,grpquota 0 0 事实上，也可以利用 mount 的 remount 功能。 [root@jet ~]# mount -o remount /disk2 这样就已经成功的将 filesystem 的 quota 功能加入。 扫瞄磁盘的使用者使用状况，并产生重要的 aquota.group 与 aquota.user： [root@jet ~]# quotacheck -avug quotacheck: Scanning /dev/hdb1 [/disk2] done quotacheck: Checked 3 directories and 4 files [root@localhost ~]# ll /disk2 -rw------- 1 root root 6144 Sep 6 11:44 aquota.group -rw------- 1 root root 6144 Sep 6 11:44 aquota.user 使用 quotacheck 就可以轻易的将所需要的数据给他输出了！但奇怪的是，在某些 Linux 版本中，不能够以 aquota.user(group) 来启动quota ，可能是因为旧版 quota 的关系， 所以就另外做了一个 link 文件按来欺骗 quota，这个动作非必要。（主要是学习这个思维很重要） [root@localhost ~]# cd /disk2 [root@localhost ~]# ln -s aquota.user quota.user [root@localhost ~]# ln -s aquota.group quota.group 启动 quota 的限额： [root@localhost ~]# quotaon -avug /dev/hdb1 [/disk2]: group quotas turned on /dev/hdb1 [/disk2]: user quotas turned on ===&gt; 看到turned on，才是真的成功！ 编辑使用者的可使用空间： [root@localhost ~]# edquota -u quser1 Disk quotas for user quser1 (uid 502): Filesystem blocks soft hard inodes soft hard /dev/hdb1 0 45000 50000 0 0 0 [root@localhost ~]# edquota -p quser1 quser2 ===&gt; 直接复制给quser2 接下来要来设定宽限时间，还是使用 edquota [root@localhost ~]# edquota -t Grace period before enforcing soft limits for users: time units may be: days, hours, minutes, or seconds Filesystem Block grace period Inode grace period /dev/hdb1 1days 7days 使用quota -v来查询： [root@localhost ~]# quota -vu quser1 quser2 Disk quotas for user quser1 (uid 502): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 45000 50000 0 0 0 Disk quotas for user quser2 (uid 503): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 45000 50000 0 0 0 注意，由于使用者尚未超过45 MB，所以 grace ( 宽限时间 ) 就不会出现。 编辑群组可使用的空间： [root@localhost ~]# edquota -g qgroup Disk quotas for group qgroup (gid 502): Filesystem blocks soft hard inodes soft hard /dev/hdb1 0 80000 90000 0 0 0 [root@localhost ~]# quota -vg qgroup Disk quotas for group qgroup (gid 502): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 80000 90000 0 0 0 kill 用于向某个工作（%jobnumber）或者是某个PID（数字）传送一个信号，它通常与ps和jobs命令一起使用 kill [参数] [进程号] 1：SIGHUP，启动被终止的进程 2：SIGINT，相当于输入ctrl+c，中断一个程序的进行 9：SIGKILL，强制中断一个进程的进行 15：SIGTERM，以正常的结束进程方式来终止进程 17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行 eg: kill -9 19785 killall 杀死同一进程组内的所有进程。其允许指定要终止的进程的名称，而非PID -i ：交互式的意思，若需要删除时，会询问用户 -e ：表示后面接的command name要一致，但command name不能超过15个字符 -I ：命令名称忽略大小写 eg: killall -SIGHUP syslogd # 重新启动syslogd useradd 创建的新的系统用户 帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 在Slackware中，adduser指令是个script程序，利用交谈的方式取得输入的用户帐号资料，然后再交由真正建立帐号的useradd命令建立新用户，如此可方便管理员建立用户帐号。在Red Hat Linux中，adduser命令则是useradd命令的符号连接，两者实际上是同一个指令 useradd(选项)(参数) -c&lt;备注&gt;：加上备注文字。备注文字会保存在passwd的备注栏位中； -d&lt;登入目录&gt;：指定用户登入时的启始目录； -D：变更预设值； -e&lt;有效期限&gt;：指定帐号的有效期限； -f&lt;缓冲天数&gt;：指定在密码过期后多少天即关闭该帐号； -g&lt;群组&gt;：指定用户所属的群组； -G&lt;群组&gt;：指定用户所属的附加群组； -m：自动建立用户的登入目录； -M：不要自动建立用户的登入目录； -n：取消建立以用户名称为名的群组； -r：建立系统帐号； -s：指定用户登入后所使用的shell； -u：指定用户id。 新建用户加入组： useradd –g sales jack –G company,employees //-g：加入主要组、-G：加入次要组 建立一个新用户账户，并设置ID： useradd caojh -u 544 需要说明的是，设定ID值时尽量要大于500，以免冲突。因为Linux安装后会建立一些特殊用户，一般0到499之间的值留给bin、mail这样的系统账号 groupadd 创建一个新的工作组，新工作组的信息将被添加到系统文件中 groupadd(选项)(参数) -g：指定新建工作组的id； -r：创建系统工作组，系统工作组的组ID小于500； -K：覆盖配置文件“/ect/login.defs”； -o：允许添加组ID号不唯一的工作组。 实例 建立一个新组，并设置组ID加入系统： groupadd -g 344 linuxde 此时在/etc/passwd文件中产生一个组ID（GID）是344的项目 groupdel 删除指定的工作组 要修改的系统文件包括/ect/group和/ect/gshadow。若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组 groupdel(参数) eg: groupadd damon //创建damon工作组 groupdel damon //删除这个工作组 crontab 提交和管理用户的需要周期性执行的任务 与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 crontab(选项)(参数) -e：编辑该用户的计时器设置； -l：列出该用户的计时器设置； -r：删除该用户的计时器设置； -u&lt;用户名称&gt;：指定要设定计时器的用户名称。 Linux下的任务调度分为两类：系统任务调度和用户任务调度。 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=&quot;&quot;HOME=/ # run-parts 51 * * * * root run-parts /etc/cron.hourly 24 7 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly 【注】 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下： /etc/cron.deny 该文件中所列用户不允许使用crontab命令 /etc/cron.allow 该文件中所列用户允许使用crontab命令 /var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： minute hour day month week command 顺序：分 时 日 月 周 minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正 斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 service crond start #启动服务 service crond stop #关闭服务 service crond restart #重启服务 service crond reload #重新载入配置 service crond status #查看crontab服务状态 ntsysv #查看crontab服务是否已设置为开机启动 chkconfig –level 35 crond on #开机启动 eg: * * * * * command #每1分钟执行一次command 3,15 * * * * command #每小时的第3和第15分钟执行 3,15 8-11 * * * command #在上午8点到11点的第3和第15分钟执行 3,15 8-11 */2 * * command #每隔两天的上午8点到11点的第3和第15分钟执行 3,15 8-11 * * 1 command #每个星期一的上午8点到11点的第3和第15分钟执行 30 21 * * * /etc/init.d/smb restart #每晚的21:30重启smb 45 4 1,10,22 * * /etc/init.d/smb restart #每月1、10、22日的4 : 45重启smb 10 1 * * 6,0 /etc/init.d/smb restart #每周六、周日的1:10重启smb 0,30 18-23 * * * /etc/init.d/smb restart #每天18 : 00至23 : 00之间每隔30分钟重启smb 0 23 * * 6 /etc/init.d/smb restart #每星期六的晚上11:00 pm重启smb * */1 * * * /etc/init.d/smb restart #每一小时重启smb * 23-7/1 * * * /etc/init.d/smb restart #晚上11点到早上7点之间，每隔一小时重启smb 0 11 4 * mon-wed /etc/init.d/smb restart #每月的4号与每周一到周三的11点重启smb 0 4 1 jan * /etc/init.d/smb restart #一月一号的4点重启smb 01 * * * * root run-parts /etc/cron.hourly #每小时执行/etc/cron.hourly目录内的脚本 网络操作命令ifconfig 配置和显示Linux内核中网络接口的网络参数 用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 ifconfig(参数) add&lt;地址&gt;：设置网络设备IPv6的ip地址； del&amp;lt;地址&amp;gt;：删除网络设备IPv6的IP地址； down：关闭指定的网络设备； &amp;lt;hw&amp;lt;网络设备类型&amp;gt;&amp;lt;硬件地址&amp;gt;：设置网络设备的类型与硬件地址； io_addr&amp;lt;I/O地址&amp;gt;：设置网络设备的I/O地址； irq&amp;lt;IRQ地址&amp;gt;：设置网络设备的IRQ； media&amp;lt;网络媒介类型&amp;gt;：设置网络设备的媒介类型； mem_start&amp;lt;内存地址&amp;gt;：设置网络设备在主内存所占用的起始地址； metric&amp;lt;数目&amp;gt;：指定在计算数据包的转送次数时，所要加上的数目； mtu&amp;lt;字节&amp;gt;：设置网络设备的MTU； netmask&amp;lt;子网掩码&amp;gt;：设置网络设备的子网掩码； tunnel&amp;lt;地址&amp;gt;：建立IPv4与IPv6之间的隧道通信地址； up：启动指定的网络设备； -broadcast&amp;lt;地址&amp;gt;：将要送往指定地址的数据包当成广播数据包来处理； -pointopoint&amp;lt;地址&amp;gt;：与指定地址的网络设备建立直接连线，此模式具有保密功能； -promisc：关闭或启动指定网络设备的promiscuous模式； IP地址：指定网络设备的IP地址； 网络设备：指定网络设备的名称。 显示网络设备信息（激活状态的）： [root@jet ~]# ifconfig eth0 Link encap:Ethernet HWaddr 00:16:3E:00:1E:51 inet addr:10.160.7.81 Bcast:10.160.15.255 Mask:255.255.240.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:61430830 errors:0 dropped:0 overruns:0 frame:0 TX packets:88534 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:3607197869 (3.3 GiB) TX bytes:6115042 (5.8 MiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:56103 errors:0 dropped:0 overruns:0 frame:0 TX packets:56103 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5079451 (4.8 MiB) TX bytes:5079451 (4.8 MiB) 【注】eth0表示第一块网卡，其中HWaddr表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是00:16:3E:00:1E:51。 inet addr**用来表示网卡的IP地址，此网卡的IP地址是10.160.7.81，广播地址Bcast:10.160.15.255，掩码地址Mask:255.255.240.0。 lo是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址）。第二行：网卡的IP地址、子网、掩码。第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节。第四、五行：接收、发送数据包情况统计。第七行：接收、发送数据字节数统计信息。 启动关闭指定网卡： ifconfig eth0 up ifconfig eth0 down ifconfig eth0 up为启动网卡eth0，ifconfig eth0 down为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 为网卡配置和删除IPv6地址： ifconfig eth0 add 33ffe:3240:800:1005::2/64 #为网卡eth0配置IPv6地址 ifconfig eth0 del 33ffe:3240:800:1005::2/64 #为网卡eth0删除IPv6地址 用ifconfig修改MAC地址： ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE 配置IP地址： [root@localhost ~]# ifconfig eth0 192.168.2.10 [root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 [root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255 启用和关闭arp协议： ifconfig eth0 arp #开启网卡eth0 的arp协议 ifconfig eth0 -arp #关闭网卡eth0 的arp协议 设置最大传输单元： ifconfig eth0 mtu 1500 #设置能通过的最大数据包大小为 1500 bytes netstat 打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况 netstat(选项) -a或--all：显示所有连线中的Socket； -A&amp;lt;网络类型&amp;gt;或--&amp;lt;网络类型&amp;gt;：列出该网络类型连线中的相关地址； -c或--continuous：持续列出网络状态； -C或--cache：显示路由器配置的快取信息； -e或--extend：显示网络其他相关信息； -F或--fib：显示FIB； -g或--groups：显示多重广播功能群组组员名单； -h或--help：在线帮助； -i或--interfaces：显示网络界面信息表单； -l或--listening：显示监控中的服务器的Socket； -M或--masquerade：显示伪装的网络连线； -n或--numeric：直接使用ip地址，而不通过域名服务器； -N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称； -o或--timers：显示计时器； -p或--programs：显示正在使用Socket的程序识别码和程序名称； -r或--route：显示Routing Table； -s或--statistice：显示网络工作信息统计表； -t或--tcp：显示TCP传输协议的连线状况； -u或--udp：显示UDP传输协议的连线状况； -v或--verbose：显示指令执行过程； -V或--version：显示版本信息； -w或--raw：显示RAW传输协议的连线状况； -x或--unix：此参数的效果和指定&quot;-A unix&quot;参数相同； --ip或--inet：此参数的效果和指定&quot;-A inet&quot;参数相同。 eg: netstat -a #列出所有端口 netstat -at #列出所有tcp端口 netstat -au #列出所有udp端口 netstat -l #只显示监听端口 netstat -lt #只列出所有监听 tcp 端口 netstat -lu #只列出所有监听 udp 端口 netstat -lx #只列出所有监听 UNIX 端口 netstat -s #显示所有端口的统计信息 netstat -st #显示TCP端口的统计信息 netstat -su #显示UDP端口的统计信息 netstat -pt #在netstat输出中显示 PID 和进程名称 netstat -p可以与其它开关一起使用，就可以添加“PID/进程名称”到netstat输出中，这样debugging的时候可以很方便的发现特定端口运行的程序。 在netstat输出中不显示主机，端口和用户名(host, port or user) 当你不想让主机，端口和用户名显示，使用netstat -n。将会使用数字代替那些名称。同样可以加速输出，因为不用进行比对查询。 netstat -an 如果只是不想让这三个名称中的一个被显示，使用以下命令: netsat -a --numeric-ports netsat -a --numeric-hosts netsat -a --numeric-users 持续输出netstat信息 netstat -c #每隔一秒输出网络信息 显示系统不支持的地址族(Address Families) netstat --verbose 在输出的末尾，会有如下的信息： netstat: no support for `AF IPX&#39; on this system. netstat: no support for `AF AX25&#39; on this system. netstat: no support for `AF X25&#39; on this system. netstat: no support for `AF NETROM&#39; on this system. 显示核心路由信息 netstat -r 使用netstat -rn显示数字格式，不查询主机名称。 找出程序运行的端口 并不是所有的进程都能找到，没有权限的会不显示，使用 root 权限查看所有的信息。 netstat -ap | grep ssh 找出运行在指定端口的进程： netstat -an | grep &#39;:80&#39; 显示网络接口列表 netstat -i 显示详细信息，像是ifconfig使用 netstat -ie。 IP和TCP分析 查看连接某服务端口最多的的IP地址： netstat -ntu | grep :80 | awk &#39;{print $5}&#39; | cut -d: -f1 | awk &#39;{++ip[$1]} END {for(i in ip) print ip[i],&quot;\t&quot;,i}&#39; | sort -nr TCP各种状态列表： netstat -nt | grep -e 127.0.0.1 -e 0.0.0.0 -e ::: -v | awk &#39;/^tcp/ {++state[$NF]} END {for(i in state) print i,&quot;\t&quot;,state[i]}&#39; 查看phpcgi进程数，如果接近预设值，说明不够用，需要增加： netstat -anpo | grep &quot;php-cgi&quot; | wc -l telnet 登录远程主机，对远程主机进行管理 telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。 telnet(选项)(参数) -8：允许使用8位字符资料，包括输入与输出； -a：尝试自动登入远端系统； -b&lt;主机别名&gt;：使用别名指定远端主机名称； -c：不读取用户专属目录里的.telnetrc文件； -d：启动排错模式； -e&lt;脱离字符&gt;：设置脱离字符； -E：滤除脱离字符； -f：此参数的效果和指定&quot;-F&quot;参数相同； -F：使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机； -k&lt;域名&gt;：使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名； -K：不自动登入远端主机； -l&lt;用户名称&gt;：指定要登入远端主机的用户名称； -L：允许输出8位字符资料； -n&lt;记录文件&gt;：指定文件记录相关信息； -r：使用类似rlogin指令的用户界面； -S&lt;服务类型&gt;：设置telnet连线所需的ip TOS信息； -x：假设主机有支持数据加密的功能，就使用它； -X&lt;认证形态&gt;：关闭指定的认证形态。 eg: [root@jet oschina_hexo_server]# telnet 124.251.54.61 5001 Trying 124.251.54.61... Connected to 124.251.54.61. Escape character is &#39;^]&#39;. SSH-2.0-OpenSSH_6.7 ping 测试主机之间网络的连通性 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 ping(选项)(参数) -d：使用Socket的SO_DEBUG功能； -c&lt;完成次数&gt;：设置完成要求回应的次数； -f：极限检测； -i&lt;间隔秒数&gt;：指定收发信息的间隔时间； -I&lt;网络界面&gt;：使用指定的网络界面送出数据包； -l&lt;前置载入&gt;：设置在送出要求信息之前，先行发出的数据包； -n：只输出数值； -p&lt;范本样式&gt;：设置填满数据包的范本样式； -q：不显示指令执行过程，开头和结尾的相关信息除外； -r：忽略普通的Routing Table，直接将数据包送到远端主机上； -R：记录路由过程； -s&lt;数据包大小&gt;：设置数据包的大小； -t&lt;存活数值&gt;：设置存活数值TTL的大小； -v：详细显示指令的执行过程。 eg: [root@jet oschina_hexo_server]# ping www.baidu.com PING www.a.shifen.com (220.181.112.244) 56(84) bytes of data. 64 bytes from 220.181.112.244: icmp_seq=1 ttl=45 time=13.9 ms 64 bytes from 220.181.112.244: icmp_seq=2 ttl=45 time=3.27 ms 64 bytes from 220.181.112.244: icmp_seq=3 ttl=45 time=2.46 ms 64 bytes from 220.181.112.244: icmp_seq=4 ttl=45 time=3.39 ms 64 bytes from 220.181.112.244: icmp_seq=5 ttl=45 time=2.42 ms 64 bytes from 220.181.112.244: icmp_seq=6 ttl=45 time=2.70 ms 64 bytes from 220.181.112.244: icmp_seq=7 ttl=45 time=2.54 ms 64 bytes from 220.181.112.244: icmp_seq=8 ttl=45 time=3.78 ms 64 bytes from 220.181.112.244: icmp_seq=9 ttl=45 time=3.20 ms 64 bytes from 220.181.112.244: icmp_seq=10 ttl=45 time=2.46 ms 64 bytes from 220.181.112.244: icmp_seq=11 ttl=45 time=2.56 ms 64 bytes from 220.181.112.244: icmp_seq=12 ttl=45 time=2.74 ms 64 bytes from 220.181.112.244: icmp_seq=13 ttl=45 time=2.42 ms 64 bytes from 220.181.112.244: icmp_seq=14 ttl=45 time=2.46 ms #ctrl + c 结束 --- www.a.shifen.com ping statistics --- 14 packets transmitted, 14 received, 0% packet loss, time 13555ms rtt min/avg/max/mdev = 2.420/3.596/13.902/2.889 ms ftp 用来设置文件系统相关功能 ftp服务器在网上较为常见，Linux ftp命令的功能是用命令的方式来控制在本地机和远程机之间传送文件，这里详细介绍Linux ftp命令的一些经常使用的命令，相信掌握了这些使用Linux进行ftp操作将会非常容易。 ftp(选项)(参数) -d：详细显示指令执行过程，便于排错或分析程序执行的情况； -i：关闭互动模式，不询问任何问题； -g：关闭本地主机文件名称支持特殊字符的扩充特性； -n：不使用自动登录； -v：显示指令执行过程。 FTP&gt;ascii: 设定以ASCII方式传送文件(缺省值) FTP&gt;bell: 每完成一次文件传送,报警提示. FTP&gt;binary: 设定以二进制方式传送文件. FTP&gt;bye: 终止主机FTP进程,并退出FTP管理方式. FTP&gt;case: 当为ON时,用MGET命令拷贝的文件名到本地机器中,全部转换为小写字母. FTP&gt;cd: 同UNIX的CD命令. FTP&gt;cdup: 返回上一级目录. FTP&gt;chmod: 改变远端主机的文件权限. FTP&gt;close: 终止远端的FTP进程,返回到FTP命令状态, 所有的宏定义都被删除. FTP&gt;delete: 删除远端主机中的文件. FTP&gt;dir [remote-directory] [local-file] 列出当前远端主机目录中的文件.如果有本地文件,就将结果写至本地文件. FTP&gt;get [remote-file] [local-file] 从远端主机中传送至本地主机中. FTP&gt;help [command] 输出命令的解释. FTP&gt;lcd: 改变当前本地主机的工作目录,如果缺省,就转到当前用户的HOME目录. FTP&gt;ls [remote-directory] [local-file] 同DIR. FTP&gt;macdef: 定义宏命令. FTP&gt;mdelete [remote-files] 删除一批文件. FTP&gt;mget [remote-files] 从远端主机接收一批文件至本地主机. FTP&gt;mkdir directory-name 在远端主机中建立目录. FTP&gt;mput local-files 将本地主机中一批文件传送至远端主机. FTP&gt;open host [port] 重新建立一个新的连接. FTP&gt;prompt: 交互提示模式. FTP&gt;put local-file [remote-file] 将本地一个文件传送至远端主机中. FTP&gt;pwd: 列出当前远端主机目录. FTP&gt;quit: 同BYE. FTP&gt;recv remote-file [local-file] 同GET. FTP&gt;rename [from] [to] 改变远端主机中的文件名. FTP&gt;rmdir directory-name 删除远端主机中的目录. FTP&gt;send local-file [remote-file] 同PUT. FTP&gt;status: 显示当前FTP的状态. FTP&gt;system: 显示远端主机系统类型. FTP&gt;user user-name [password] [account] 重新以别的用户名登录远端主机. FTP&gt;? [command]: 同HELP. [command]指定需要帮助的命令名称。如果没有指定 command，ftp 将显示全部命令的列表。 FTP&gt;! 从 ftp 子系统退出到外壳。 sftp 交互式的文件传输程序 命令的运行和使用方式与ftp命令相似，但是，sftp命令对传输的所有信息使用ssh加密，它还支持公钥认证和压缩等功能 -B：指定传输文件时缓冲区的大小； -l：使用ssh协议版本1； -b：指定批处理文件； -C：使用压缩； -o：指定ssh选项； -F：指定ssh配置文件； -R：指定一次可以容忍多少请求数； -v：升高日志等级。 推荐一款sftp连接工具 FlashFXP iptables Linux上常用的防火墙软件 是netfilter项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。 iptables(选项)(参数) -t&lt;表&gt;：指定要操纵的表； -A：向规则链中添加条目； -D：从规则链中删除条目； -i：向规则链中插入条目； -R：替换规则链中的条目； -L：显示规则链中已有的条目； -F：清楚规则链中已有的条目； -Z：清空规则链中的数据包计算器和字节计数器； -N：创建新的用户自定义规则链； -P：定义规则链中的默认目标； -h：显示帮助信息； -p：指定要匹配的数据包协议类型； -s：指定要匹配的数据包源ip地址； -j&lt;目标&gt;：指定要跳转的目标； -i&lt;网络接口&gt;：指定数据包进入本机的网络接口； -o&lt;网络接口&gt;：指定数据包要离开本机所使用的网络接口。 iptables命令选项输入顺序： iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作 表名包括： raw：高级功能，如：网址过滤。 mangle：数据包修改（QOS），用于实现服务质量。 net：地址转换，用于网关路由器。 filter：包过滤，用于防火墙规则。 规则链名包括： I NPUT链：处理输入数据包。 OUTPUT链：处理输出数据包。 PORWARD链：处理转发数据包。 PREROUTING链：用于目标地址转换（DNAT）。 POSTOUTING链：用于源地址转换（SNAT）。 动作包括： accept：接收数据包。 DROP：丢弃数据包。 REDIRECT：重定向、映射、透明代理。 SNAT：源地址转换。 DNAT：目标地址转换。 MASQUERADE：IP伪装（NAT），用于ADSL。 LOG：日志记录。 清除已有iptables规则 iptables -F iptables -X iptables -Z 开放指定的端口 iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机) iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #允许已建立的或相关连的通行 iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问 iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口 iptables -A INPUT -p tcp --dport 80 -j ACCEPT #允许访问80端口 iptables -A INPUT -p tcp --dport 21 -j ACCEPT #允许ftp服务的21端口 iptables -A INPUT -p tcp --dport 20 -j ACCEPT #允许FTP服务的20端口 iptables -A INPUT -j reject #禁止其他未允许的规则访问 iptables -A FORWARD -j REJECT #禁止其他未允许的规则访问 屏蔽IP iptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽单个IP的命令 iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令 iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令 iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是 查看已添加的 iptables规则 iptables -L -n -v Chain INPUT (policy DROP 48106 packets, 2690K bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 191K 90M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 1499K 133M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 4364K 6351M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 6256 327K ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- * lo 0.0.0.0/0 0.0.0.0/0 删除已添加的iptables规则 将所有iptables以序号标记显示，执行： iptables -L -n --line-numbers 比如要删除INPUT里序号为8的规则，执行： iptables -D INPUT 8 mail 命令行的电子邮件发送和接收工具 操作的界面不像elm或pine那么容易使用，但功能非常完整。 mail(选项)(参数) -b&lt;地址&gt;：指定密件副本的收信人地址； -c&lt;地址&gt;：指定副本的收信人地址； -f&lt;邮件文件&gt;：读取指定邮件文件中的邮件； -i：不显示终端发出的信息； -I：使用互动模式； -n：程序使用时，不使用mail.rc文件中的设置； -N：阅读邮件时，不显示邮件的标题； -s&lt;邮件主题&gt;：指定邮件的主题； -u&lt;用户帐号&gt;：读取指定用户的邮件； -v：执行时，显示详细的信息。 mail -s &quot;Hello from jethan.bid by shell&quot; admin@oschina.io hello,this is the content of mail. welcome to jethan.bid 【注】 第一行是输入的命令，-s表示邮件的主题，后面的admin@oschina.io则是邮件的接收人，输入完这行命令后回车，会进入邮件正文的编写，我们可以输入任何文字，比如上面的两行。当邮件正文输入完成后，需要按CTRL+D结束输入，此时会提示你输入Cc地址，即邮件抄送地址，没有直接回车就完成了邮件的发送 使用管道进行邮件发送 echo &quot;hello,this is the content of mail.welcome to jethan.bid&quot; | mail -s &quot;Hello from jethan.bid by pipe&quot; admin@oschina.io 【注】 使用管道直接敲入这行命令即可完成邮件的发送，其中echo后的是邮件正文。 使用文件进行邮件发送 mail -s &quot;Hello from jethan.bid by file&quot; admin@oschina.io &lt; mail.txt 使用上面的命令后，我们就可以把mail.txt文件的内容作为邮件的内容发送给admin@oschina.io了。 使用上述三种方式都可以给外部邮箱进行邮件发送，但因为前面2种都是直接在shell中敲入邮件内容，因此无法输入中文，即使我们使用粘贴的方式输入了中文，那么收到的邮件也是乱码的。但第3种方式，我们可以在window下编辑好邮件内容后，放到linux下，再进行发送，这样就可以正常发送中文了。不过目前邮件的中文标题暂时没有找到解决办法。 因为mail程序本身就是调用sendmail来进行邮件发送的，因此我们可以在mail命令中使用sendmail的参数进行配置，比如我想使用特定的发件人发送邮件，可以使用如下命令： mail -s &quot;Hello from linuxde.net with sender&quot; admin@oschina.io -- -f jet@oschina.io&lt; mail.txt 【注】 上面的命令中，我们使用了– -f jet@oschina.io这样的参数，这是sendmail的选项，其中-f表示邮件的发送人邮件地址 很多情况下，我们也需要使用邮件来发送附件，在linux下使用mail命令发送附件也很简单，不过首先需要安装uuencode软件包，这个程序是对二进制文件进行编码使其适合通过邮件进行发送，在CentOS上安装该软件包如下： yum install sharutils 安装完成后我们就可以来进行附件的发送了，使用如下命令： uuencode test.txt test | mail -s &quot;hello,see the attachement&quot; admin@oschina.io 完成后就可以把text.txt文件作为邮件的附件发送出去了。uuencode有两个参数，第一个是要发送的文件，第二个是显示的文件名称。 这里我主要介绍的是在CentOS下使用mail发送电子邮件的一些使用方法，需要的要求是你的linux必须安装了sendmail并开启了，同时保证可以连接外网。另外，文章中提到的命令本人都经过亲自测试，保证完全可用，不过你需要将命令中的电子邮件地址换成自己的电子邮件地址 如果出现错误[Postfix] – warning: mail_queue_enter: create file maildrop Permission denied [jet@jet oschina_hexo_server]$ lpostdrop: warning: mail_queue_enter: create file maildrop/820792.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/821453.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/821762.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/822488.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/822928.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/823425.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/823907.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/824427.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/824928.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/825368.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/825899.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/826355.3848: Permission denied root@jet:/var/spool/postfix# postfix check postfix/postfix-script: warning: not owned by group postdrop: /var/spool/postfix/public postfix/postfix-script: warning: not owned by group postdrop: /var/spool/postfix/maildrop root@jet:/var/spool/postfix# /etc/init.d/postfix stop root@jet:/var/spool/postfix# killall -9 postdrop root@jet:/var/spool/postfix# chgrp -R postdrop /var/spool/postfix/public root@jet:/var/spool/postfix# chgrp -R postdrop /var/spool/postfix/maildrop/ root@jet:/var/spool/postfix# postfix check root@jet:/var/spool/postfix# postfix start root@jet:/var/spool/postfix# postfix reload chmod g+s /usr/sbin/postqueue chmod g+s /usr/sbin/postdrop root@gandalf:/var/spool/postfix# postfix check #此时没有警告了 nslookup 命令是常用域名查询工具，就是查DNS信息用的命令 nslookup4有两种工作模式，即“交互模式”和“非交互模式”。在“交互模式”下，用户可以向域名服务器查询各类主机、域名的信息，或者输出域名中的主机列表。而在“非交互模式”下，用户可以针对一个主机或域名仅仅获取特定的名称或所需信息。 进入交互模式，直接输入nslookup命令，不加任何参数，则直接进入交互模式，此时nslookup会连接到默认的域名服务器（即/etc/resolv.conf 的第一个dns地址）。或者输入nslookup -nameserver/ip。进入非交互模式，就直接输入nslookup 域名就可以了。 nslookup(选项)(参数) -sil：不显示任何警告信息。 eg: [jet@jet oschina_hexo_server]$ nslookup jethan.bid Server: 114.114.114.114 Address: 114.114.114.114#53 Non-authoritative answer: Name: jethan.bid Address: 103.21.119.115 ip 显示或操纵Linux主机的路由、网络设备、策略路由和隧道，是Linux下较新的功能强大的网络配置工具 ip(选项)(参数) #(参数) 网络对象：指定要管理的网络对象； 具体操作：对指定的网络对象完成具体操作； help：显示网络对象支持的操作命令的帮助信息。 #(选项) -V：显示指令版本信息； -s：输出更详细的信息； -f：强制使用指定的协议族； -4：指定使用的网络层协议是IPv4协议； -6：指定使用的网络层协议是IPv6协议； -0：输出信息每条记录输出一行，即使内容较多也不换行显示； -r：显示主机时，不使用IP地址，而使用主机的域名。 用ip命令显示网络设备的运行状态 [jet@jet oschina_hexo_server]$ ip -s link list 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 RX: bytes packets errors dropped overrun mcast 830 14 0 0 0 0 TX: bytes packets errors dropped carrier collsns 830 14 0 0 0 0 2: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:8b:22:5e brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 121209665 198774 0 0 0 0 TX: bytes packets errors dropped carrier collsns 227857521 157867 0 0 0 0 显示核心路由表 [jet@jet oschina_hexo_server]$ ip route list 10.111.24.0/24 dev eth3 proto kernel scope link src 10.111.24.222 metric 1 default via 10.111.24.1 dev eth3 proto static 显示邻居表 [jet@jet oschina_hexo_server]$ ip neigh list fe80::ac9e:6493:cb07:4add dev eth3 lladdr 54:ee:75:03:60:e7 STALE fe80::1870:3ef3:ba4f:e55b dev eth3 lladdr bc:9f:ef:8e:d3:13 STALE fe80::a28d:16ff:fe84:ac43 dev eth3 lladdr a0:8d:16:84:ac:43 STALE fe80::ca6:b92b:4736:a95a dev eth3 lladdr 24:a2:e1:39:ba:21 STALE fe80::83c:b2a6:cc95:fae7 dev eth3 lladdr 54:4e:90:7a:6b:e3 STALE fe80::8486:b8c:5150:15c1 dev eth3 lladdr 28:d2:44:74:a0:d4 STALE fe80::1c2f:e7e6:82ee:17f0 dev eth3 lladdr 20:3c:ae:b0:87:88 STALE fe80::78d2:a70e:cba3:5373 dev eth3 lladdr 28:d2:44:68:36:3c STALE fe80::9c19:a5e5:918b:b069 dev eth3 lladdr 50:7b:9d:e6:7c:ac STALE fe80::1c12:cb63:3897:fcb0 dev eth3 lladdr fc:d8:48:92:44:c9 STALE fe80::184c:1bc1:a201:2fa7 dev eth3 lladdr 90:b0:ed:77:94:4a STALE fe80::792a:9e54:8679:cb84 dev eth3 lladdr b8:ee:65:04:81:89 STALE fe80::1098:1ed2:b23a:4ef dev eth3 lladdr 5c:ad:cf:6e:11:6a STALE fe80::28bc:6b21:4fce:f51a dev eth3 lladdr 28:d2:44:c9:c0:ed STALE fe80::1895:8cec:208d:5eec dev eth3 lladdr f4:5c:89:8e:ad:a5 STALE fe80::1c44:bede:96a5:499a dev eth3 lladdr 20:ab:37:93:1f:43 STALE fe80::5a:d7d:e3f6:6f9f dev eth3 lladdr 14:2d:27:f8:4b:79 STALE fe80::61e9:b002:68c1:7ba dev eth3 lladdr 28:d2:44:6e:11:d7 STALE fe80::1cbe:22d4:b466:d564 dev eth3 lladdr 40:33:1a:ad:1b:8c STALE fe80::1863:6370:356f:c6ec dev eth3 lladdr 24:24:0e:de:6c:86 STALE fe80::426c:8fff:fe3f:304e dev eth3 lladdr 40:6c:8f:3f:30:4e STALE fe80::1824:b631:d44a:5bf3 dev eth3 lladdr 9c:fc:01:e7:c8:21 STALE fe80::1038:72ee:9f9e:51ea dev eth3 lladdr 70:48:0f:60:e7:0d STALE fe80::a069:d8a9:5fc0:219d dev eth3 lladdr ac:22:0b:c9:bb:6c STALE fe80::8e4:27f0:1ac7:97cc dev eth3 lladdr 28:d2:44:6d:fb:6f STALE fe80::59a0:8d02:8cb7:430c dev eth3 lladdr 28:d2:44:68:74:80 STALE fe80::3884:7711:39ce:2b96 dev eth3 lladdr 48:5a:b6:df:8f:8d STALE fe80::f8c1:52f5:5286:9bae dev eth3 lladdr 28:d2:44:75:5d:ec STALE fe80::c9c:69:70b5:2796 dev eth3 lladdr 2c:20:0b:bf:2d:16 STALE fe80::ee01:eeff:fe0a:fe55 dev eth3 lladdr ec:01:ee:0a:fe:55 STALE fe80::3ea3:48ff:fe99:5bf6 dev eth3 lladdr 3c:a3:48:99:5b:f6 STALE fe80::1092:6d00:3bfa:ad5f dev eth3 lladdr 7c:04:d0:32:44:24 STALE fe80::c3e:8ab7:92f8:d0f6 dev eth3 lladdr 64:b0:a6:26:a5:f0 STALE fe80::ca6:3dd9:1fe:f6a0 dev eth3 lladdr 24:24:0e:be:1a:57 STALE fe80::d265:caff:fece:62c9 dev eth3 lladdr d0:65:ca:ce:62:c9 STALE fe80::163e:bfff:fefc:ac78 dev eth3 lladdr 14:3e:bf:fc:ac:78 STALE fe80::9ef3:87ff:fec0:c6fa dev eth3 lladdr 9c:f3:87:c0:c6:fa STALE fe80::1415:65e3:71b0:4c9e dev eth3 lladdr d8:1d:72:52:6f:84 STALE 10.111.24.248 dev eth3 lladdr bc:75:74:5e:fb:2e STALE 10.111.24.220 dev eth3 lladdr 14:3e:bf:fc:ac:78 STALE 10.111.24.1 dev eth3 lladdr 10:47:80:28:02:e0 STALE 10.111.24.227 dev eth3 lladdr f4:8e:38:ba:47:3f REACHABLE scp 远程拷贝文件的命令 和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 scp(选项)(参数) #(选项) -1：使用ssh协议版本1； -2：使用ssh协议版本2； -4：使用ipv4； -6：使用ipv6； -B：以批处理模式运行； -C：使用压缩； -F：指定ssh配置文件； -l：指定宽带限制； -o：指定使用的ssh选项； -P：指定远程主机的端口号； -p：保留文件的最后修改时间，最后访问时间和权限模式； -q：不显示复制进度； -r：以递归方式复制； -v 详细显示输出的具体情况。 #(参数) 源文件：指定要复制的源文件。 目标文件：目标文件。格式为user@host：filename（文件名为目标文件的名称）。 (1) 复制文件： 命令格式： scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file 或者 scp local_file remote_ip:remote_folder 或者 scp local_file remote_ip:remote_file 第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名 第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名 (2) 复制目录： 命令格式： scp -r local_folder remote_username@remote_ip:remote_folder 或者 scp -r local_folder remote_ip:remote_folder 第1个指定了用户名，命令执行后需要输入用户密码； 第2个没有指定用户名，命令执行后需要输入用户名和密码； eg: #从 本地 上传到 远程 scp /home/daisy/full.tar.gz root@172.19.2.75:/home/root #从 远程 下载到 本地 scp root@172.19.2.75:/home/root/full.tar.gz /home/daisy/ wget 直接从网络上下载文件 wget [参数] [URL地址] -o FILE 把记录写到FILE文件中 eg : wget -O a.txt URL wget --limit-rate=300k URL 限速下载 系统安全相关命令vmstat 对操作系统的虚拟内存、进程、CPU活动进行监控 iostat 对系统的磁盘操作活动进行监视,汇报磁盘活动统计情况，同时也会汇报出CPU使用情况 -p[磁盘] 显示磁盘和分区的情况 watch 重复执行某一命令以观察变化 watch [参数] [命令] -n 时隔多少秒刷新 -d 高亮显示动态变化 at 在一个指定的时间执行一个指定任务，只能执行一次 at [参数] [时间] HH:MM[am|pm] + number [minutes|hours|days|weeks] 强制在某年某月某日的某时刻进行该项任务 atq 查看系统未执行的任务 atrm n 删除编号为n的任务 at -c n 显示编号为n的任务的内容 passwd 用于设置用户的认证信息，包括用户密码、密码过期时间等 系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 passwd(选项)(参数) #(选项) -l 使密码失效 -u 与-l相对，用户解锁 -S 列出登陆用户passwd文件内的相关参数 -n 后面接天数，shadow 的第 4 字段，多久不可修改密码天数 -x 后面接天数，shadow 的第 5 字段，多久内必须要更动密码 -w 后面接天数，shadow 的第 6 字段，密码过期前的警告天数 -i 后面接『日期』，shadow 的第 7 字段，密码失效日期 使用管道刘设置密码：echo &quot;zeng&quot; | passwd --stdin zenghao #(参数) 用户名：需要设置密码的用户名。 与用户、组账户信息相关的文件 存放用户信息： /etc/passwd /etc/shadow 存放组信息： /etc/group /etc/gshadow 用户信息文件分析（每项用:隔开） 例如：jack:X:503:504:::/home/jack/:/bin/bash jack //用户名 X //口令、密码 503 //用户 （0代表root、普通新建用户从500开始） 504 //所在组 : //描述 /home/jack/ //用户主目录 /bin/bash //用户缺省Shell 组信息文件分析 例如：jack:$!$:???:13801:0:99999:7:*:*: jack //组名 $!$ //被加密的口令 13801 //创建日期与今天相隔的天数 0 //口令最短位数 99999 //用户口令 7 //到7天时提醒 * //禁用天数 * //过期天数 如果是普通用户执行passwd只能修改自己的密码。如果新建用户后，要为新用户创建密码，则用passwd用户名，注意要以root用户的权限来创建 [root@jet ~]# passwd linuxde //更改或创建linuxde用户的密码； Changing password for user linuxde. New UNIX password: //请输入新密码； Retype new UNIX password: //再输入一次； passwd: all authentication tokens updated successfully. //成功； 普通用户如果想更改自己的密码，直接运行passwd即可，比如当前操作的用户是jet [jet@jet ~]$ passwd Changing password for user linuxde. //更改jet用户的密码； (current) UNIX password: //请输入当前密码； New UNIX password: //请输入新密码； Retype new UNIX password: //确认新密码； passwd: all authentication tokens updated successfully. //更改成功； 比如我们让某个用户不能修改密码，可以用-l选项来锁定： [root@localhost ~]# passwd -l linuxde //锁定用户jet不能更改密码； Locking password for user linuxde. passwd: Success //锁定成功； [jet@jet ~]# su linuxde //通过su切换到jet用户； [jet@jet ~]$ passwd //jet来更改密码； Changing password for user jet. Changing password for linuxde (current) UNIX password: //输入jet的当前密码； passwd: Authentication token manipulation error //失败，不能更改密码； 清除密码 [root@jet ~]# passwd -d jet //清除jet用户密码； Removing password for user jet. passwd: Success //清除成功； [root@jet ~]# passwd -S jet //查询jet用户密码状态； Empty password. //空密码，也就是没有密码； 【注】 当我们清除一个用户的密码时，登录时就无需密码，这一点要加以注意。 su 切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码 su [参数] user -c&lt;指令&gt;或--command=&lt;指令&gt;：执行完指定的指令后，即恢复原来的身份； -f或——fast：适用于csh与tsch，使shell不用去读取启动文件； -l或——login：改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,logname。此外，也会变更PATH变量； -m,-p或--preserve-environment：变更身份时，不要变更环境变量； -s或--shell=：指定要执行的shell； --help：显示帮助； --version；显示版本信息。 eg: #变更帐号为root并在执行ls指令后退出变回原使用者： su -c ls root #变更帐号为root并传入-f选项给新执行的shell： su root -f #变更帐号为test并改变工作目录至test的家目录： su -test sudo 以其他身份来执行命令，预设的身份为root 在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。 sudo(选项)(参数) -b：在后台执行指令； -h：显示帮助； -H：将HOME环境变量设为新身份的HOME环境变量； -k：结束密码的有效期限，也就是下次再执行sudo时便需要输入密码；。 -l：列出目前用户可执行与无法执行的指令； -p：改变询问密码的提示符号； -s：执行指定的shell； -u&lt;用户&gt;：以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份； -v：延长密码有效期限5分钟； -V ：显示版本信息。 配置sudo必须通过编辑/etc/sudoers文件，而且只有超级用户才可以修改它，还必须使用visudo编辑。之所以使用visudo有两个原因，一是它能够防止两个用户同时修改它；二是它也能进行有限的语法检查。所以，即使只有你一个超级用户，你也最好用visudo来检查一下语法。 visudo默认的是在vi里打开配置文件，用vi来修改文件。我们可以在编译时修改这个默认项。visudo不会擅自保存带有语法错误的配置文件，它会提示你出现的问题，并询问该如何处理，就像： &gt;&gt;&gt; sudoers file: syntax error, line 22 &lt;&lt; 此时我们有三种选择：键入“e”是重新编辑，键入“x”是不保存退出，键入“Q”是退出并保存。如果真选择Q，那么sudo将不会再运行，直到错误被纠正。 现在，我们一起来看一下神秘的配置文件，学一下如何编写它。让我们从一个简单的例子开始：让用户Foobar可以通过sudo执行所有root可执行的命令。以root身份用visudo打开配置文件，可以看到类似下面几行： # Runas alias specification # User privilege specificationroot ALL=(ALL)ALL 我们一看就明白个差不多了，root有所有权限，只要仿照现有root的例子就行，我们在下面加一行（最好用tab作为空白）： foobar ALL=(ALL) ALL 保存退出后，切换到foobar用户，我们用它的身份执行命令： [foobar@localhost ~]$ ls /root ls: /root: 权限不够 [foobar@localhost ~]$ sudo ls /root PassWord: anaconda-ks.cfg Desktop install.log install.log.syslog 好了，我们限制一下foobar的权利，不让他为所欲为。比如我们只想让他像root那样使用ls和ifconfig，把那一行改为： foobar localhost= /sbin/ifconfig, /bin/ls 再来执行命令： [foobar@localhost ~]$ sudo head -5 /etc/shadow Password: Sorry, user foobar is not allowed to execute &#39;/usr/bin/head -5 /etc/shadow&#39; as root on localhost.localdomain. [foobar@localhost ~]$ sudo /sbin/ifconfigeth0 Linkencap:Ethernet HWaddr 00:14:85:EC:E9:9B... 现在让我们来看一下那三个ALL到底是什么意思。第一个ALL是指网络中的主机，我们后面把它改成了主机名，它指明foobar可以在此主机上执行后面的命令。第二个括号里的ALL是指目标用户，也就是以谁的身份去执行命令。最后一个ALL当然就是指命令名了。例如，我们想让foobar用户在linux主机上以jimmy或rene的身份执行kill命令，这样编写配置文件： foobar linux=(jimmy,rene) /bin/kill 但这还有个问题，foobar到底以jimmy还是rene的身份执行？这时我们应该想到了sudo -u了，它正是用在这种时候。 foobar可以使用sudo -u jimmy kill PID或者sudo -u rene kill PID，但这样挺麻烦，其实我们可以不必每次加-u，把rene或jimmy设为默认的目标用户即可。再在上面加一行： Defaults:foobar runas_default=rene Defaults后面如果有冒号，是对后面用户的默认，如果没有，则是对所有用户的默认。就像配置文件中自带的一行： Defaults env_reset 另一个问题是，很多时候，我们本来就登录了，每次使用sudo还要输入密码就显得烦琐了。我们可不可以不再输入密码呢？当然可以，我们这样修改配置文件： foobar localhost=NOPASSWD: /bin/cat, /bin/ls 再来sudo一下： [foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg Desktop install.log install.log.syslog 当然，你也可以说“某些命令用户foobar不可以运行”，通过使用!操作符，但这不是一个好主意。因为，用!操作符来从ALL中“剔出”一些命令一般是没什么效果的，一个用户完全可以把那个命令拷贝到别的地方，换一个名字后再来运行。 日志与安全 sudo为安全考虑得很周到，不仅可以记录日志，还能在有必要时向系统管理员报告。但是，sudo的日志功能不是自动的，必须由管理员开启。这样来做： touch /var/log/sudo vi /etc/syslog.conf 在syslog.conf最后面加一行（必须用tab分割开）并保存： local2.debug /var/log/sudo 重启日志守候进程， ps aux grep syslogd 把得到的syslogd进程的PID（输出的第二列是PID）填入下面： kill –HUP PID 这样，sudo就可以写日志了： [foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg Desktop install.log install.log.syslog $cat /var/log/sudoJul 28 22:52:54 localhost sudo: foobar : TTY=pts/1 ; pwd=/home/foobar ; USER=root ; command=/bin/ls /root 不过，有一个小小的“缺陷”，sudo记录日志并不是很忠实： [foobar@localhost ~]$ sudo cat /etc/shadow &gt; /dev/null cat /var/log/sudo...Jul 28 23:10:24 localhost sudo: foobar : TTY=pts/1 ; PWD=/home/foobar ; USER=root ; COMMAND=/bin/cat /etc/shadow 重定向没有被记录在案！为什么？因为在命令运行之前，shell把重定向的工作做完了，sudo根本就没看到重定向。这也有个好处，下面的手段不会得逞： [foobar@localhost ~]$ sudo ls /root &gt; /etc/shadowbash: /etc/shadow: 权限不够 sudo 有自己的方式来保护安全。以root的身份执行sudo-V，查看一下sudo的设置。因为考虑到安全问题，一部分环境变量并没有传递给sudo后面的命令，或者被检查后再传递的，比如：PATH，HOME，SHELL等。当然，你也可以通过sudoers来配置这些环境变量。 chgrp 改变文件或目录所属的用户组 该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组 chgrp(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quiet或——silent：不显示错误信息； -h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件； -R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理； -v或——verbose：显示指令执行过程； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； #(参数) 组：指定新工作名称； 文件：指定要改变所属组的文件列表。多个文件或者目录之间使用空格隔开。 eg： chgrp users -R ./dir #递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users chown 改变某个文件或目录的所有者和所属的组 该命令可以向某个用户授权，使该用户变成指定文件的所有者或者改变文件所属的组。用户可以是用户或者是用户D，用户组可以是组名或组id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。 chown(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quite或——silent：不显示错误信息； -h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件； -R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -v或——version：显示指令执行过程； --dereference：效果和“-h”参数相同； --help：在线帮助； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同； --version：显示版本信息。 #(参数) 用户：组：指定所有者和所属工作组。当省略“：组”，仅改变文件所有者； 文件：指定要改变所有者和工作组的文件列表。支持多个文件和目标，支持shell通配符。 #将目录/usr/meng及其下面的所有文件、子目录的文件主改成 liu： chown -R liu /usr/meng #文件的属主和属组属性设置 chown user:market f01 //把文件f01给uesr，添加到market组 ll -d f1 查看目录f1的属性 chmod 变更文件或目录的权限 在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 权限范围的表示法如下：u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户 ；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”； - 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 chmod(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quiet或——silent：不显示错误信息； -R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理； -v或——verbose：显示指令执行过程； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； &lt;权限范围&gt;+&lt;权限设置&gt;：开启权限范围的文件或目录的该选项权限设置； &lt;权限范围&gt;-&lt;权限设置&gt;：关闭权限范围的文件或目录的该选项权限设置； &lt;权限范围&gt;=&lt;权限设置&gt;：指定权限范围的文件或目录的该选项权限设置； #(参数) 权限模式：指定文件的权限模式； 文件：要改变权限的文件。 eg： chmod 0755 file # 把file的文件权限改变为-rxwr-xr-x chmod g+w file # 向file的文件权限中加入用户组可写权限 Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ linux文件的用户权限的分析图 文件权限管理 三种基本权限 R 读 数值表示为4 W 写 数值表示为2 X 可执行 数值表示为1 如图所示，copyright.html文件的权限为-rw-rw-r— -rw-rw-r-- 一共十个字符，分成四段。 第一个字符“-”表示普通文件；这个位置还可能会出现“l”链接；“d”表示目录 第二三四个字符“rw-”表示当前所属用户的权限。 所以用数值表示为4+2=6 第五六七个字符“rw-”表示当前所属组的权限。 所以用数值表示为4+2=6 第八九十个字符“r-–”表示其他用户权限。 所以用数值表示为4 所以操作此文件的权限用数值表示为664 用户及用户组管理 /etc/passwd 存储用户账号 /etc/group 存储组账号 /etc/shadow 存储用户账号的密码 /etc/gshadow 存储用户组账号的密码 useradd 添加用户名 userdel 删除用户名 adduser 添加用户名 groupadd 添加组名 groupdel 删除组名 passwd root 给root设置密码 su root su – root /etc/profile 系统环境变量 bash_profile 用户环境变量 .bashrc 用户环境变量 su user 切换用户，加载配置文件.bashrc su – user 切换用户，加载配置文件/etc/profile ，加载bash_profile who 显示目前登录系统的用户信息 执行who命令可得知目前有那些用户登入系统，单独执行who命令会列出登入帐号，使用的终端机，登入时间以及从何处登入或正在使用哪个X显示器。 who(选项)(参数) #(选项) -H或--heading：显示各栏位的标题信息列； -i或-u或--idle：显示闲置时间，若该用户在前一分钟之内有进行任何动作，将标示成&quot;.&quot;号，如果该用户已超过24小时没有任何动作，则标示出&quot;old&quot;字符串； -m：此参数的效果和指定&quot;am i&quot;字符串相同； -q或--count：只显示登入系统的帐号名称和总人数； -s：此参数将忽略不予处理，仅负责解决who指令其他版本的兼容性问题； -w或-T或--mesg或--message或--writable：显示用户的信息状态栏； --help：在线帮助； --version：显示版本信息。 #(参数) 文件：指定查询文件。 whoami 打印当前有效的用户名称，相当于执行id -un命令 whoami(选项) (选项) --help：在线帮助； --version：显示版本信息。 which 查找并显示给定命令的绝对路径 环境变量PATH中保存了查找命令时需要遍历的目录。which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which(选项)(参数) #(选项) -n&lt;文件名长度&gt;：制定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名； -p&lt;文件名长度&gt;：与-n参数相同，但此处的&lt;文件名长度&gt;包含了文件的路径； -w：指定输出时栏位的宽度； -V：显示版本信息。 #(参数) 指令名：指令名列表。 查找文件、显示命令路径： [root@jet ~]# which pwd /bin/pwd [root@jet ~]# which adduser /usr/sbin/adduser 【注】 which是根据使用者所配置的 PATH 变量内的目录去搜寻可运行档的！所以，不同的 PATH 配置内容所找到的命令当然不一样的！ ntpdate设置本地日期和时间 服务器的时间不对的时候，可以使用ntpdate工具来校正时间。 ntpdate ip/site eg: /usr/sbin/ntpdate time.windows.com 以下是一些可用的NTP服务器地址： Name IP Location 210.72.145.44 210.72.145.44 中国（国家授时中心） 133.100.11.8 133.100.11.8 日本（福冈大学） time-a.nist.gov 129.6.15.28 NIST,Gaithersburg,Maryland time-b.nist.gov 129.6.15.29 NIST,Gaithersburg,Maryland time-a.timefreq.bldrdoc.gov 132.163.4.101 NIST,Boulder,Colorado time-b.timefreq.bldrdoc.gov 132.163.4.102 NIST,Boulder,Colorado time-c.timefreq.bldrdoc.gov 132.163.4.103 NIST,Boulder,Colorado utcnist.colorado.edu 128.138.140.44 UniversityofColorado,Boulder time.nist.gov 192.43.244.18 NCAR,Boulder,Colorado time-nw.nist.gov 131.107.1.10 Microsoft,Redmond,Washington nist1.symmetricom.com 69.25.96.13 Symmetricom,SanJose,California nist1-dc.glassey.com 216.200.93.8 Abovenet,Virginia nist1-ny.glassey.com 208.184.49.9 Abovenet,NewYorkCity nist1-sj.glassey.com 207.126.98.204 Abovenet,SanJose,California nist1.aol-ca.truetime.com 207.200.81.113 TrueTime,AOLfacility,Sunnyvale,California nist1.aol-va.truetime.com 64.236.96.53 TrueTime,AOLfacility,Virginia 其它命令gzip 压缩文件 gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，文件经它压缩过后，其名称后面会多处“.gz”扩展名，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。 gzip(选项)(参数) #(选项) -a或——ascii：使用ASCII文字模式； -d或--decompress或----uncompress：解开压缩文件； -f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接； -h或——help：在线帮助； -l或——list：列出压缩文件的相关信息； -L或——license：显示版本与版权信息； -n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记； -N或——name：压缩文件时，保存原来的文件名称及时间戳记； -q或——quiet：不显示警告信息； -r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串； -t或——test：测试压缩文件是否正确无误； -v或——verbose：显示指令执行过程； -V或——version：显示版本信息； -&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高； --best：此参数的效果和指定“-9”参数相同； --fast：此参数的效果和指定“-1”参数相同。 #(参数) 文件列表：指定要压缩的文件列表。 把test6目录下的每个文件压缩成.gz文件 gzip * 把上例中每个压缩的文件解压，并列出详细的信息 gzip -dv * 详细显示例1中每个压缩的文件的信息，并不解压 gzip -l * 压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz gzip -r log.tar 递归的压缩目录 gzip -rv test6 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。 递归地解压目录 gzip -dr test6 gunzip 解压缩文件 gunzip是个使用广泛的解压缩程序，它用于解开被gzip压缩过的文件，这些压缩文件预设最后的扩展名为.gz。事实上gunzip就是gzip的硬连接，因此不论是压缩或解压缩，都可通过gzip指令单独完成。 gunzip(选项)(参数) #(选项) -a或——ascii：使用ASCII文字模式； -c或--stdout或--to-stdout：把解压后的文件输出到标准输出设备； -f或-force：强行解开压缩文件，不理会文件名称或硬连接是否存在以及该文件是否为符号连接； -h或——help：在线帮助； -l或——list：列出压缩文件的相关信息； -L或——license：显示版本与版权信息； -n或--no-name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其忽略不予处理； -N或——name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其回存到解开的文件上； -q或——quiet：不显示警告信息； -r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串； -t或——test：测试压缩文件是否正确无误； -v或——verbose：显示指令执行过程； -V或——version：显示版本信息； #(参数) 文件列表：指定要解压缩的压缩包。 首先将/etc目录下的所有文件以及子目录进行压缩，备份压缩包etc.zip到/opt目录，然后对etc.zip文件进行gzip压缩，设置gzip的压缩级别为9。 zip –r /opt/etc.zip /etc gzip -9v /opt/etc.zip 查看上述etc.zip.gz文件的压缩信息。 gzip -l /opt/etc.zip.gz compressed uncompressed ratio uncompressed_name 11938745 12767265 6.5% /opt/etc.zip 解压上述etc.zip.gz文件到当前目录。 [root@mylinux ~]#gzip –d /opt/etc.zip.gz 或者执行 [root@mylinux ~]#gunzip /opt/etc.zip.gz 通过上面的示例可以知道gzip –d等价于gunzip命令。 bzip2 创建和管理（包括解压缩）“.bz2”格式的压缩包 我们遇见Linux压缩打包方法有很多种，以下讲解了Linux压缩打包方法中的Linux bzip2命令的多种范例供大家查看，相信大家看完后会有很多收获。 bzip2(选项)(参数) #(选项) -c或——stdout：将压缩与解压缩的结果送到标准输出； -d或——decompress：执行解压缩； -f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数； -h或——help：在线帮助； -k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数； -s或——small：降低程序执行时内存的使用量； -t或——test：测试.bz2压缩文件的完整性； -v或——verbose：压缩或解压缩文件时，显示详细的信息； -z或——compress：强制执行压缩； -V或——version：显示版本信息； --repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果； --repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。 #(参数) 文件：指定要压缩的文件。 压缩指定文件filename: bzip2 filename 或 bzip2 -z filename 这里，压缩的时候不会输出，会将原来的文件filename给删除，替换成filename.bz2.如果以前有filename.bz2则不会替换并提示错误（如果想要替换则指定-f选项，例如bzip2 -f filename；如果filename是目录则也提醒错误不做任何操作；如果filename已经是压过的了有bz2后缀就提醒一下，不再压缩，没有bz2后缀会再次压缩。 解压指定的文件filename.bz2: bzip2 -d filename.bz2 或 bunzip2 filename.bz2 这里，解压的时候没标准输出，会将原来的文件filename.bz2给替换成filename。如果以前有filename则不会替换并提示错误（如果想要替换则指定-f选项，例如bzip2 -df filename.bz2。 压缩解压的时候将结果也输出： $bzip2 -v filename 输入之后，输出如下： filename: 0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out. 这里，加上-v选项就会输出了,只用压缩举例了，解压的时候同理bzip2 -dv filename.bz2不再举例了。 模拟解压实际并不解压： bzip2 -tv filename.bz2 输入之后，输出如下： filename.bz2: ok 这里，-t指定要进行模拟解压，不实际生成结果，也就是说类似检查文件,当然就算目录下面有filename也不会有什么错误输出了，因为它根本不会真的解压文件。为了在屏幕上输出，这里加上-v选项了,如果是真的解压bzip2 -dv filename.bz2则输出的是把”ok”替换成了”done”。 压缩解压的时候，除了生成结果文件，将原来的文件也保存: bzip2 -k filename 这里，加上-k就保存原始的文件了，否则原始文件会被结果文件替代。只用压缩举例了，解压的时候同理$bzip2 -dk filename.bz2不再举例了。 解压到标准输出： bzip2 -dc filename.bz2 输入之后，输出如下： hahahhaahahha 这里，使用-c指定到标准输出，输出的是文件filename的内容，不会将filename.bz2删除。 压缩到标准输出： bzip2 -c filename bzip2: I won&#39;t write compressed data to a terminal. bzip2: For help, type: `bzip2 --help&#39;. 这里，使用-c指定压缩到标准输出不删除原有文件，不同的是，压缩后的文件无法输出到标准输出。 使用bzip2的时候将所有后面的看作文件(即使文件名以’-‘开头)： bzip2 -- -myfilename 这里主要是为了防止文件名中-产生以为是选项的歧义。 bzcat 读取数据而无需解压 解压缩指定的.bz2文件，并显示解压缩后的文件内容。保留原压缩文件，并且不生成解压缩后的文件 bzcat(参数) #(参数) .bz2压缩文件：指定要显示内容的.bz2压缩文件。 将/tmp/man.config以bzip2格式压缩： bzip2 -z man.config 此时man.config会变成man.config.bz2 将上面的压缩文件内容读出来： bzcat man.config.bz2 此时屏幕上会显示 man.config.bz2 解压缩之后的文件内容。 tar 为linux的文件和目录创建档案 利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 tar(选项)(参数) #(选项) -A或--catenate：新增文件到以存在的备份文件； -B：设置区块大小； -c或--create：建立新的备份文件； -C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 -d：记录文件的差别； -x或--extract或--get：从备份文件中还原文件； -t或--list：列出备份文件的内容； -z或--gzip或--ungzip：通过gzip指令处理备份文件； -Z或--compress或--uncompress：通过compress指令处理备份文件； -f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件； -v或--verbose：显示指令执行过程； -r：添加文件到已经压缩的文件； -u：添加改变了和现有的文件到已经存在的压缩文件； -j：支持bzip2解压文件； -v：显示操作过程； -l：文件系统边界设置； -k：保留原有文件不覆盖； -m：保留文件不被覆盖； -w：确认压缩文件的正确性； -p或--same-permissions：用原来的文件权限还原文件； -P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号； -N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里； --exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 #(参数) 文件或目录：指定要打包的文件或目录列表。 将文件全部打包成tar包： tar -jcvf filename.tar.bz2 要被压缩的档案或目录名称 #压 缩 tar -jtvf filename.tar.bz2 #查 询 tar -jxvf filename.tar.bz2 -C 欲解压缩的目录 #解压缩 tar -cvf log.tar log2012.log #仅打包，不压缩！ tar -zcvf log.tar.gz log2012.log #打包后，以 gzip 压缩 tar -jcvf log.tar.bz2 log2012.log #打包后，以 bzip2 压缩 在选项f之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加z选项，则以.tar.gz或.tgz来代表gzip压缩过的tar包；如果加j选项，则以.tar.bz2来作为tar包名。 查阅上述tar包内有哪些文件： tar -ztvf log.tar.gz 由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上z这个选项了。 将tar包解压缩： tar -zxvf /opt/soft/test/log.tar.gz 在预设的情况下，我们可以将压缩档在任何地方解开的 只将tar内的部分文件解压出来： tar -zxvf /opt/soft/test/log30.tar.gz log2013.log 我可以透过tar -ztvf来查阅 tar 包内的文件名称，如果单只要一个文件，就可以透过这个方式来解压部分文件！ 文件备份下来，并且保存其权限： tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log 这个-p的属性是很重要的，尤其是当您要保留原本文件的属性时。 在文件夹当中，比某个日期新的文件才备份： tar -N “2012/11/13” -zcvf log17.tar.gz test 备份文件夹内容是排除部分文件： tar —exclude scf/service -zcvf scf.tar.gz scf/* 其实最简单的使用 tar 就只要记忆底下的方式即可： 压 缩：tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查 询：tar -jtv -f filename.tar.bz2 解压缩：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 users 显示当前登录系统地用户 who 登录在本机的用户与来源 -H或--heading 显示各栏位的标题信息列。 w 登录在本机的用户及其运行的程序 -s 使用简洁格式列表，不显示用户登入时间，终端机阶段作业和程序所耗费的CPU时间。 -h 不显示各栏位的标题信息列。 write 给当前联机的用户发消息 wall 给所有登录再本机的用户发消息 last 查看用户的登陆日志 lastlog 查看每个用户最后的登陆时间 finger 查看用户信息 -s 显示用户的注册名、实际姓名、终端名称、写状态、停滞时间、登录时间等信息 -l 除了用-s选项显示的信息外，还显示用户主目录、登录shell、邮件状态等信息，以及用户主目录下的.plan、.project和.forward文件的内容。 -p 除了不显示.plan文件和.project文件以外，与-l选项相同 hostname 查看主机名 alias 添加别名 unalias 清除别名 chage **修改用户密码的相关属性 -l 列出该账号的详细密码参数； -d 后面接日期，修改 shadow 第三字段(最近一次更改密码的日期)，格式YYYY-MM-DD -E 后面接日期，修改 shadow 第八字段(账号失效日)，格式 YYYY-MM-DD -I 后面接天数，修改 shadow 第七字段(密码失效日期) -m 后面接天数，修改 shadow 第四字段(密码最短保留天数) -M 后面接天数，修改 shadow 第五字段(密码多久需要进行变更) -W 后面接天数，修改 shadow 第六字段(密码过期前警告日期) usermod 修改用户的相关属性 -c 后面接账号的说明，即 /etc/passwd 第五栏的说明栏，可以加入一些账号的说明。 -d 后面接账号的家目录，即修改 /etc/passwd 的第六栏； -e 后面接日期，格式是 YYYY-MM-DD 也就是在 /etc/shadow 内的第八个字段数据啦！ -f 后面接天数为 shadow 的第七字段。 -g 后面接初始群组，修改 /etc/passwd 的第四个字段，亦即是GID的字段！ -G 后面接次要群组，修改这个使用者能够支持的群组 -l 后面接账号名称。亦即是修改账号名称， /etc/passwd 的第一栏！ -s 后面接 Shell 的实际档案，例如 /bin/bash 或 /bin/csh 等等。 -u 后面接 UID 数字啦！即 /etc/passwd 第三栏的资料； -L 冻结密码 -U 解冻密码 id 查看用户相关的id信息，还可以用来判断用户是否存在 groups 查看登陆用户支持的群组， 第一个输出的群组为有效群组 newgrp 切换有效群组 groupmod 修改组信息 -g 修改既有的 GID 数字 -n 修改既有的组名 groupdel 删除群组 gpasswd 群组管理员功能 root管理员动作： -gpasswd groupname 设定密码 -gpasswd [-A user1,...] [-M user3,...] groupname -A 将 groupname 的主控权交由后面的使用者管理(该群组的管理员) -M 将某些账号加入这个群组当中 -gpasswd [-r] groupname -r 将 groupname 的密码移除 群组管理员动作： - gpasswd [-ad] user groupname -a 将某位使用者加入到 groupname 这个群组当中 -d 将某位使用者移除出 groupname 这个群组当中 chfn 修改个人信息 cut Print selected parts of lines from each FILE to standard output -b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。 -c ：以字符为单位进行分割。 -d ：自定义分隔符，默认为制表符。 -f ：与-d一起使用，指定显示哪个区域。 sort sort -n 依照数值的大小排序。 -o&lt;输出文件&gt; 将排序后的结果存入指定的文件。 -r 以相反的顺序来排序。 -t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。 -k 选择以哪个区间进行排序。 set 显示环境变量和普通变量 env 显示环境变量 export 把普通变量变成环境变量 unset 删除一个环境变量 aaa(){} 定义函数 read read -p 接提示字符 -t 接等待的秒数 declare/typeset declare、typeset -i 声明为整数 -a 声明为数组 -f 声明为函数 -r 声明为只读 ulimit 限制使用者的某些系统资源 -f 此 shell 可以建立的最大档案容量 (一般可能设定为 2GB)单位为 Kbytes eg: ulimit -f 1024 限制使用者仅能建立 1MBytes 以下的容量的档案 date 显示或设定系统的日期与时间 date [参数]… [+格式] %H 小时(以00-23来表示)。 %M 分钟(以00-59来表示)。 %P AM或PM。 %D 日期(含年月日) %U 该年中的周数。 date -s “2015-10-17 01:01:01″ //时间设定 date +%Y%m%d //显示前天年月日 date +%Y%m%d --date=&quot;+1 day/month/year&quot; //显示前一天/月/年的日期 date +%Y%m%d --date=&quot;-1 day/month/year&quot; //显示后一天/月/年的日期 date -d &#39;2 weeks&#39; 2周后的日期 cal 查看日历 -1 显示当月的月历 -3 显示前、当、后一个月的日历 -m 显示星期一为一个星期的第一天 -s （默认）星期天为第一天 -j 显示当月是一年中的第几天的日历 -y 显示当前年份的日历 gcc 命令 对于一个用Linux开发C程序的人来说，这个命令就非常重要了，它用于把C语言的源程序文件，编译成可执行程序，由于g++的很多参数跟它非常相似，所以这里只介绍gcc的参数，它的常用参数如下： [plain] view plain copy print? -o ：output之意，用于指定生成一个可执行文件的文件名 -c ：用于把源文件生成目标文件（.o)，并阻止编译器创建一个完整的程序 -I ：增加编译时搜索头文件的路径 -L ：增加编译时搜索静态连接库的路径 -S ：把源文件生成汇编代码文件 -lm：表示标准库的目录中名为libm.a的函数库 -lpthread ：连接NPTL实现的线程库 -std= ：用于指定把使用的C语言的版本 # 例如： # 把源文件test.c按照c99标准编译成可执行程序test gcc -o test test.c -lm -std=c99 #把源文件test.c转换为相应的汇编程序源文件test.s gcc -S test.c time 测算一个命令（即程序）的执行时间 它的使用非常简单，就像平时输入命令一样，不过在命令的前面加入一个time即可，例如： [plain] view plain copy print? time ./process time ps aux 在程序或命令运行结束后，在最后输出了三个时间，它们分别是：user：用户CPU时间，命令执行完成花费的用户CPU时间，即命令在用户态中执行时间总和；system：系统CPU时间，命令执行完成花费的系统CPU时间，即命令在核心态中执行时间总和；real：实际时间，从command命令行开始执行到运行终止的消逝时间； 注：用户CPU时间和系统CPU时间之和为CPU时间，即命令占用CPU执行的时间总和。实际时间要大于CPU时间，因为Linux是多任务操作系统，往往在执行一条命令时，系统还要处理其它任务。另一个需要注意的问题是即使每次执行相同命令，但所花费的时间也是不一样，其花费时间是与系统运行相关的。 查看内存溢出 查看内存溢出 jmap -heap pid #打印heap的概要信息 jmap -histo pid #打印每个class的实例数目，内存占用，类全名信息 jmap -dump:format=b,file=heap.bin pid #输出heap信息到heap.bin文件 jhat -J-mx768m heap.bin #分析heap.bin文件 jstack -l pid &gt; deadlock.jstack #输出stack信息到deadlock.jstack vi deadlock.jstack #使用vi查看]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用操作命令]]></title>
    <url>%2F2017%2F06%2F19%2Flinux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[引言 ◆ 安装和登录命令：login、shutdown、halt、reboot、install、mount、umount、chsh、exit、last； ◆ 文件处理命令：file、mkdir、dd、rm、grep、find、mv、cp、ls、diff、cat、ln、tail、head、more、less、cd、管道； ◆ 系统管理相关命令：df、top、free、quota、at、lp、adduser、groupadd、kill、crontab； ◆ 网络操作命令：ifconfig、ip、ping、netstat、telnet、ftp、route、rlogin、rcp、finger、mail、nslookup； ◆ 系统安全相关命令：passwd、su、umask、chgrp、chmod、chown、chattr、sudo ps、who、which、whois； ◆ 其它命令：tar、unzip、gunzip、unarj、mtools、man、unendcode、uudecode。系统管理命令 stat ＃显示指定文件的详细信息，比ls更详细 who ＃显示在线登陆用户 whoami ＃显示当前操作用户 hostname ＃显示主机名 uname ＃显示系统信息 top ＃动态显示当前耗费资源最多进程信息 ps ＃显示瞬间进程状态 ps -aux du ＃查看目录大小 du -h /home带有单位显示目录信息 df ＃查看磁盘大小 df -h 带有单位显示磁盘信息 ifconfig ＃查看网络情况 ping ＃测试网络连通 netstat ＃显示网络状态信息 man ＃命令不会用了，找男人? 如：man ls clear ＃清屏 alias ＃对命令重命名 如：alias showmeit=”ps -aux” ，另外解除使用unaliax showmeit kill ＃杀死进程，可以先用ps 或 top命令查看进程的id，然后再用kill命令杀死进程。 常用基本指令 ls #显示文件或目录 -l #列出文件详细信息l(list) -a #列出当前目录下所有文件及目录，包括隐藏的a(all) mkdir #创建目录 -p #创建目录，若无父目录，则创建p(parent) cd #切换目录 touch #创建空文件 echo #创建带有内容的文件。 cat #查看文件内容 cp #拷贝 mv #移动或重命名 rm #删除文件 -r #递归删除，可删除子目录及文件 -f #强制删除 find #在文件系统中搜索某文件 wc #统计文本中行数、字数、字符数 grep #在文本文件中查找某个字符串 rmdir #删除空目录 tree #树形结构显示目录，需要安装tree包 pwd #显示当前目录 ln #创建链接文件 more、less #分页显示文本文件内容 head、tail #显示文件头、尾内容 ctrl+alt+F1 #命令行全屏模式 下面只介绍一些常用的基本命令 安装和登录命令shutdown/halt/reboot/exit/logout命令说明 shutdown -r #关机重启 -h #关机不重启 now #立刻关机 halt #关机 reboot #重启 exit #退出当前shell logout #退出登录shell mount/unmount命令说明 fdisk -l #查看磁盘情况 fdisk /dev/sda #为/dev/sda设备分区 m #显示所有命令 n #添加分区 p/e #主分区/逻辑分区 +50G #指定分区大小为50G p #打印分区列表 w #保存 reboot #重启 cat /etc/fstab #查看文件系统 mke2fs -t ext4 /dev/sda4 #格式化文件系统 mount /dev/sda4 /home #挂载到指定目录/home umount #取消挂载 #开机自动挂载 echo &quot;/dev/sda4 /home ext4 defaults 1 1&quot; &gt;&gt; /etc/fstab 文件处理命令awk一种编程语言，用于在linux/unix下对文本和数据进行处理 数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk最大的优势。 awk [options] &#39;script&#39; var=value file(s) awk [options] -f scriptfile var=value file(s) -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 {} 要执行的脚本内容 eg: cat /etc/passwd |awk -F &#39;:&#39; &#39;{print $1&quot;\t&quot;$7}&#39; awk模式和操作 awk脚本是由模式和操作组成的。 模式 模式可以是以下任意一个： /正则表达式/：使用通配符的扩展集。 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试。 模式匹配表达式：用运算符~（匹配）和~!（不匹配）。 BEGIN语句块、pattern语句块、END语句块：参见awk的工作原理 操作 操作由一个或多个命令、函数、表达式组成，之间由换行符或分号隔开，并位于大括号内，主要部分是： 变量或数组赋值 输出命令 内置函数 控制流语句 awk脚本基本结构 awk &#39;BEGIN{ print &quot;start&quot; } pattern{ commands } END{ print &quot;end&quot; }&#39; file 一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如： awk &#39;BEGIN{ i=0 } { i++ } END{ print i }&#39; filename awk &quot;BEGIN{ i=0 } { i++ } END{ print i }&quot; filename awk的工作原理 awk &#39;BEGIN{ commands } pattern{ commands } END{ commands }&#39; 第一步：执行BEGIN{ commands }语句块中的语句； 第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ commands }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。 第三步：当读至输入流末尾时，执行END{ commands }语句块。 BEGIN语句块在awk开始从输入流中读取行之前被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。 END语句块在awk从输入流中读取完所有的行之后即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。 pattern语句块中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行{ print }，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。 eg： [jet@jet ~]$ echo -e &quot;A line 1\nA line 2&quot; | awk &#39;BEGIN{ print &quot;Start&quot; } { print } END{ print &quot;End&quot; }&#39; Start A line 1 A line 2 End 当使用不带参数的print时，它就打印当前行，当print的参数是以逗号进行分隔时，打印时则以空格作为定界符。在awk的print语句块中双引号是被当作拼接符使用，例如： echo | awk &#39;{ var1=&quot;v1&quot;; var2=&quot;v2&quot;; var3=&quot;v3&quot;; print var1,var2,var3; }&#39; v1 v2 v3 双引号拼接使用： echo | awk &#39;{ var1=&quot;v1&quot;; var2=&quot;v2&quot;; var3=&quot;v3&quot;; print var1&quot;=&quot;var2&quot;=&quot;var3; }&#39; v1=v2=v3 { }类似一个循环体，会对文件中的每一行进行迭代，通常变量初始化语句（如：i=0）以及打印文件头部的语句放入BEGIN语句块中，将打印的结果等语句放在END语句块中。 awk内置变量（预定义变量） 说明：[A][N][P][G]表示第一个支持变量的工具，[A]=awk、[N]=nawk、[P]=POSIXawk、[G]=gawk $n 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 $0 这个变量包含执行过程中当前行的文本内容。 [N] ARGC 命令行参数的数目。 [G] ARGIND 命令行中当前文件的位置（从0开始算）。 [N] ARGV 包含命令行参数的数组。 [G] CONVFMT 数字转换格式（默认值为%.6g）。 [P] ENVIRON 环境变量关联数组。 [N] ERRNO 最后一个系统错误的描述。 [G] FIELDWIDTHS 字段宽度列表（用空格键分隔）。 [A] FILENAME 当前输入文件的名。 [P] FNR 同NR，但相对于当前文件。 [A] FS 字段分隔符（默认是任何空格）。 [G] IGNORECASE 如果为真，则进行忽略大小写的匹配。 [A] NF 表示字段数，在执行过程中对应于当前的字段数。 [A] NR 表示记录数，在执行过程中对应于当前的行号。 [A] OFMT 数字的输出格式（默认值是%.6g）。 [A] OFS 输出字段分隔符（默认值是一个空格）。 [A] ORS 输出记录分隔符（默认值是一个换行符）。 [A] RS 记录分隔符（默认是一个换行符）。 [N] RSTART 由match函数所匹配的字符串的第一个位置。 [N] RLENGTH 由match函数所匹配的字符串的长度。 [N] SUBSEP 数组下标分隔符（默认值是34）。 eg: echo -e &quot;line1 f2 f3nline2 f4 f5nline3 f6 f7&quot; | awk &#39;{print &quot;Line No:&quot;NR&quot;, No of fields:&quot;NF, &quot;$0=&quot;$0, &quot;$1=&quot;$1, &quot;$2=&quot;$2, &quot;$3=&quot;$3}&#39; Line No:1, No of fields:3 $0=line1 f2 f3 $1=line1 $2=f2 $3=f3 Line No:2, No of fields:3 $0=line2 f4 f5 $1=line2 $2=f4 $3=f5 Line No:3, No of fields:3 $0=line3 f6 f7 $1=line3 $2=f6 $3=f7 使用print $NF可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段，其他以此类推： echo -e &quot;line1 f2 f3n line2 f4 f5&quot; | awk &#39;{print $NF}&#39; f3 f5 echo -e &quot;line1 f2 f3n line2 f4 f5&quot; | awk &#39;{print $(NF-1)}&#39; f2 f4 打印每一行的第二和第三个字段： awk &#39;{ print $2,$3 }&#39; filename 统计文件中的行数： awk &#39;END{ print NR }&#39; filename 以上命令只使用了END语句块，在读入每一行的时，awk会将NR更新为对应的行号，当到达最后一行NR的值就是最后一行的行号，所以END语句块中的NR就是文件的行数。 一个每一行中第一个字段值累加的例子： seq 5 | awk &#39;BEGIN{ sum=0; print &quot;总和：&quot; } { print $1&quot;+&quot;; sum+=$1 } END{ print &quot;等于&quot;; print sum }&#39; 总和： 1+ 2+ 3+ 4+ 5+ 等于 15 将外部变量值传递给awk 借助-v选项，可以将外部值（并非来自stdin）传递给awk： VAR=10000 echo | awk -v VARIABLE=$VAR &#39;{ print VARIABLE }&#39; 另一种传递外部变量方法： var1=”aaa”var2=”bbb”echo | awk ‘{ print v1,v2 }’ v1=$var1 v2=$var2 当输入来自于文件时使用： awk ‘{ print v1,v2 }’ v1=$var1 v2=$var2 filename 以上方法中，变量之间用空格分隔作为awk的命令行参数跟随在BEGIN、{}和END语句块之后。 awk运算与判断 作为一种程序设计语言所应具有的特点之一，awk支持多种运算，这些运算与C语言提供的基本相同。awk还提供了一系列内置的运算函数（如log、sqr、cos、sin等）和一些用于对字符串进行操作（运算）的函数（如length、substr等等）。这些函数的引用大大的提高了awk的运算功能。作为对条件转移指令的一部分，关系判断是每种程序设计语言都具备的功能，awk也不例外，awk中允许进行多种测试，作为样式匹配，还提供了模式匹配表达式~（匹配）和~!（不匹配）。作为对测试的一种扩充，awk也支持用逻辑运算符。 算术运算符 运算符 描述符 + - 加，减 * / &amp; 乘，除与求余 + - ！ 一元加 ，减和逻辑非 ^ *** 求冥 ++ — 自增,自减 作为前缀或后缀 eg： awk &#39;BEGIN{a=&quot;b&quot;;print a++,++a;}&#39; 0 2 注意：所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0 赋值运算符 运算符 描述符 = += -= *= /= %= ^= **= 赋值语句 eg： a+=5; 等价于：a=a+5; 逻辑运算符 运算符 描述 &#124;&#124; 逻辑或 &amp;&amp; 逻辑与 eg： awk &#39;BEGIN{a=1;b=2;print (a&gt;5 &amp;&amp; b&lt;=2),(a&gt;5 || b&lt;=2);}&#39; 0 1 正则运算符 运算符 描述 ~ ~! 匹配正则表达式和不匹配正则表达式 eg： awk &#39;BEGIN{a=&quot;100testa&quot;;if(a ~ /^100*/){print &quot;ok&quot;;}}&#39; ok 关系运算符 运算符 描述 &lt; &lt;= &gt; &gt;= != == 关系运算符 eg： awk &#39;BEGIN{a=11;if(a &gt;= 9){print &quot;ok&quot;;}}&#39; ok 注意：> < 可以作为字符串比较，也可以用作数值比较，关键看操作数如果是字符串就会转换为字符串比较。两个都为数字才转为数值比较。字符串比较：按照ASCII码顺序比较。 其它运算符 运算符 描述 $ 字段引用 空格 字符串连接符 ?: C条件表达式 in 数组中是否存在某键值 eg： awk &#39;BEGIN{a=&quot;b&quot;;print a==&quot;b&quot;?&quot;ok&quot;:&quot;err&quot;;}&#39; ok awk &#39;BEGIN{a=&quot;b&quot;;arr[0]=&quot;b&quot;;arr[1]=&quot;c&quot;;print (a in arr);}&#39; 0 awk &#39;BEGIN{a=&quot;b&quot;;arr[0]=&quot;b&quot;;arr[&quot;b&quot;]=&quot;c&quot;;print (a in arr);}&#39; 1 运算级优先级表 awk高级输入输出 读取下一条记录 awk中next语句使用：在循环逐行匹配，如果遇到next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。net语句一般用于多行合并： cat text.txt a b c d e awk &#39;NR%2==1{next}{print NR,$0;}&#39; text.txt 2 b 4 d 当记录行号除以2余1，就跳过当前行。下面的print NR,$0也不会执行。下一行开始，程序有开始判断NR%2值。这个时候记录行号是：2 ，就会执行下面语句块：&#39;print NR,$0&#39; 分析发现需要将包含有“web”行进行跳过，然后需要将内容与下面行合并为一行： [jet@jet oschina_hexo_server]$ cat test.txt web01[192.168.2.100] httpd ok tomcat ok sendmail ok web02[192.168.2.101] httpd ok postfix ok web03[192.168.2.102] mysqld ok httpd ok 0 [jet@jet oschina_hexo_server]$ awk &#39;/^web/{T=$0;next;}{print T&quot;:\t&quot;$0;}&#39; test.txt web01[192.168.2.100] : httpd ok web01[192.168.2.100] : tomcat ok web01[192.168.2.100] : sendmail ok web02[192.168.2.101] : httpd ok web02[192.168.2.101] : postfix ok web03[192.168.2.102] : mysqld ok web03[192.168.2.102] : httpd ok web03[192.168.2.102] : 0 web03[192.168.2.102] : 简单地读取一条记录 awk getline用法：输出重定向需用到getline函数。getline从标准输入、管道或者当前正在处理的文件之外的其他输入文件获得输入。它负责从输入获得下一行的内容，并给NF,NR和FNR等内建变量赋值。如果得到一条记录，getline函数返回1，如果到达文件的末尾就返回0，如果出现错误，例如打开文件失败，就返回-1。 getline语法：getline var，变量var包含了特定行的内容。 awk getline从整体上来说，用法说明： 当其左右无重定向符|或&lt;时：getline作用于当前文件，读入当前文件的第一行给其后跟的变量var或$0（无变量），应该注意到，由于awk在处理getline之前已经读入了一行，所以getline得到的返回结果是隔行的。 当其左右有重定向符|或&lt;时：getline则作用于定向输入文件，由于该文件是刚打开，并没有被awk读入一行，只是getline读入，那么getline返回的是该文件的第一行，而不是隔行。 eg： 执行linux的date命令，并通过管道输出给getline，然后再把输出赋值给自定义变量out，并打印它： awk &#39;BEGIN{ &quot;date&quot; | getline out; print out }&#39; test 执行shell的date命令，并通过管道输出给getline，然后getline从管道中读取并将输入赋值给out，split函数把变量out转化成数组mon，然后打印数组mon的第二个元素： awk &#39;BEGIN{ &quot;date&quot; | getline out; split(out,mon); print mon[2] }&#39; test 命令ls的输出传递给geline作为输入，循环使getline从ls的输出中读取一行，并把它打印到屏幕。这里没有输入文件，因为BEGIN块在打开输入文件前执行，所以可以忽略输入文件。 awk &#39;BEGIN{ while( &quot;ls&quot; | getline) print }&#39; 关闭文件 awk中允许在程序中关闭一个输入或输出文件，方法是使用awk的close语句。 close(&quot;filename&quot;) filename可以是getline打开的文件，也可以是stdin，包含文件名的变量或者getline使用的确切命令。或一个输出文件，可以是stdout，包含文件名的变量或使用管道的确切命令。 输出到一个文件 awk中允许用如下方式将结果输出到一个文件： echo | awk &#39;{printf(&quot;hello word!n&quot;) &gt; &quot;datafile&quot;}&#39; 或 echo | awk &#39;{printf(&quot;hello word!n&quot;) &gt;&gt; &quot;datafile&quot;}&#39; 设置字段定界符 默认的字段定界符是空格，可以使用-F &quot;定界符&quot;明确指定一个定界符： awk -F: &#39;{ print $NF }&#39; /etc/passwd 或 awk &#39;BEGIN{ FS=&quot;:&quot; } { print $NF }&#39; /etc/passwd 在BEGIN语句块中则可以用OFS=“定界符”设置输出字段的定界符。 流程控制语句 在linux awk的while、do-while和for语句中允许使用break,continue语句来控制流程走向，也允许使用exit这样的语句来退出。break中断当前正在执行的循环并跳到循环外执行下一条语句。if 是流程选择用法。awk中，流程控制语句，语法结构，与c语言类型。有了这些语句，其实很多shell程序都可以交给awk，而且性能是非常快的。下面是各个语句用法。 条件判断语句 if(表达式) 语句1 else 语句2 格式中语句1可以是多个语句，为了方便判断和阅读，最好将多个语句用{}括起来。awk分枝结构允许嵌套，其格式为： if(表达式) {语句1} else if(表达式) {语句2} else {语句3} eg： awk &#39;BEGIN{ test=100; if(test&amp;gt;90){ print &quot;very good&quot;; } else if(test&amp;gt;60){ print &quot;good&quot;; } else{ print &quot;no pass&quot;; } }&#39; very good 每条命令语句后面可以用;分号结尾。 循环语句 while语句 while(表达式) {语句} eg： awk &#39;BEGIN{ test=100; total=0; while(i&amp;lt;=test){ total+=i; i++; } print total; }&#39; 5050 for循环 for循环有两种格式： 格式1： for(变量 in 数组) {语句} eg： awk &#39;BEGIN{ for(k in ENVIRON){ print k&quot;=&quot;ENVIRON[k]; } }&#39; TERM=linux G_BROKEN_FILENAMES=1 SHLVL=1 pwd=/root/text ... logname=root HOME=/root SSH_CLIENT=192.168.1.21 53087 22 【注】ENVIRON是awk常量，是子典型数组。 格式2： for(变量;条件;表达式) {语句} eg： awk &#39;BEGIN{ total=0; for(i=0;i&amp;lt;=100;i++){ total+=i; } print total; }&#39; do循环 do {语句} while(条件) eg： awk &#39;BEGIN{ total=0; i=0; do {total+=i;i++;} while(i&amp;lt;=100) print total; }&#39; 5050 其他语句 break 当 break 语句用于 while 或 for 语句时，导致退出程序循环。 continue 当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代。 next 能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程。 exit 语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行。 数组应用 数组是awk的灵魂，处理文本中最不能少的就是它的数组处理。因为数组索引（下标）可以是数字和字符串在awk中数组叫做关联数组(associative arrays)。awk 中的数组不必提前声明，也不必声明大小。数组元素用0或空字符串来初始化，这根据上下文而定。 数组的定义 数字做数组索引（下标）： Array[1]=&quot;sun&quot; Array[2]=&quot;kai&quot; 字符串做数组索引（下标）： Array[&quot;first&quot;]=&quot;www&quot; Array[&quot;last&quot;]=&quot;name&quot; Array[&quot;birth&quot;]=&quot;1987&quot; 使用中print Array[1]会打印出sun；使用print Array[2]会打印出kai；使用print[&quot;birth&quot;]会得到1987。 读取数组的值 { for(item in array) {print array[item]}; } #输出的顺序是随机的 { for(i=1;i&lt;=len;i++) {print array[i]}; } #Len是数组的长度 数组相关函数 得到数组长度: awk &#39;BEGIN{info=&quot;it is a test&quot;;lens=split(info,tA,&quot; &quot;);print length(tA),lens;}&#39; 4 4 length返回字符串以及数组长度，split进行分割字符串为数组，也会返回分割得到数组长度。 awk &#39;BEGIN{info=&quot;it is a test&quot;;split(info,tA,&quot; &quot;);print asort(tA);}&#39; 4 asort对数组进行排序，返回数组长度。 输出数组内容（无序，有序输出）： awk ‘BEGIN{info=”it is a test”;split(info,tA,” “);for(k in tA){print k,tA[k];}}’4 test1 it2 is3 a for…in输出，因为数组是关联数组，默认是无序的。所以通过for…in得到是无序的数组。如果需要得到有序数组，需要通过下标获得。 awk &#39;BEGIN{info=&quot;it is a test&quot;;tlen=split(info,tA,&quot; &quot;);for(k=1;k&lt;=tlen;k++){print k,tA[k];}}&#39; 1 it 2 is 3 a 4 test 注意：数组下标是从1开始，与C数组不一样。 判断键值存在以及删除键值： #错误的判断方法： awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;if(tB[&quot;c&quot;]!=&quot;1&quot;){print &quot;no found&quot;;};for(k in tB){print k,tB[k];}}&#39; no found a a1 b b1 c 以上出现奇怪问题，tB[“c”]没有定义，但是循环时候，发现已经存在该键值，它的值为空，这里需要注意，awk数组是关联数组，只要通过数组引用它的key，就会自动创建改序列。 #正确判断方法： awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;if( &quot;c&quot; in tB){print &quot;ok&quot;;};for(k in tB){print k,tB[k];}}&#39; a a1 b b1 if(key in array)通过这种方法判断数组中是否包含key键值。 #删除键值： [chengmo@localhost ~]$ awk &#39;BEGIN{tB[&quot;a&quot;]=&quot;a1&quot;;tB[&quot;b&quot;]=&quot;b1&quot;;delete tB[&quot;a&quot;];for(k in tB){print k,tB[k];}}&#39; b b1 delete array[key]可以删除，对应数组key的，序列值。 二维、多维数组使用 awk的多维数组在本质上是一维数组，更确切一点，awk在存储上并不支持多维数组。awk提供了逻辑上模拟二维数组的访问方式。例如，array[2,4]=1这样的访问是允许的。awk使用一个特殊的字符串SUBSEP(�34)作为分割字段，在上面的例子中，关联数组array存储的键值实际上是2�344。 类似一维数组的成员测试，多维数组可以使用if ( (i,j) in array)这样的语法，但是下标必须放置在圆括号中。类似一维数组的循环访问，多维数组使用for ( item in array )这样的语法遍历数组。与一维数组不同的是，多维数组必须使用split()函数来访问单独的下标分量。 awk &#39;BEGIN{ for(i=1;i&amp;lt;=9;i++){ for(j=1;j&amp;lt;=9;j++){ tarr[i,j]=i*j; print i,&quot;*&quot;,j,&quot;=&quot;,tarr[i,j]; } } }&#39; 1 * 1 = 1 1 * 2 = 2 1 * 3 = 3 1 * 4 = 4 1 * 5 = 5 1 * 6 = 6 ... 9 * 6 = 54 9 * 7 = 63 9 * 8 = 72 9 * 9 = 81 可以通过array[k,k2]引用获得数组内容。 另一种方法： awk ‘BEGIN{ for(i=1;i&lt;=9;i++){ for(j=1;j&lt;=9;j++){ tarr[i,j]=ij; } } for(m in tarr){ split(m,tarr2,SUBSEP); print tarr2[1],”“,tarr2[2],”=”,tarr[m]; } }’ 内置函数 awk内置函数，主要分以下3种类似：算数函数、字符串函数、其它一般函数、时间函数 算术函数 格式 描述 atan2( y, x ) 返回 y/x 的反正切。 cos( x ) 返回 x 的余弦；x 是弧度。 sin( x ) 返回 x 的正弦；x 是弧度。 exp( x ) 返回 x 幂函数。 log( x ) 返回 x 的自然对数。 sqrt( x ) 返回 x 平方根。 int( x ) 返回 x 的截断至整数的值。 rand( ) 返回任意数字 n，其中 0 &lt;= n &lt; 1。 srand( [expr] ) 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。 举例说明： awk &#39;BEGIN{OFMT=&quot;%.3f&quot;;fs=sin(1);fe=exp(10);fl=log(10);fi=int(3.1415);print fs,fe,fl,fi;}&#39; 0.841 22026.466 2.303 3 OFMT 设置输出数据格式是保留3位小数。 获得随机数： awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 78 awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 31 awk &#39;BEGIN{srand();fr=int(100*rand());print fr;}&#39; 41 字符串函数 格式 描述 gsub( Ere, Repl, [ In ] ) 除了正则表达式所有具体值被替代这点，它和 sub 函数完全一样地执行。 sub( Ere, Repl, [ In ] ) 用 Repl 参数指定的字符串替换 In 参数指定的字符串中的由 Ere 参数指定的扩展正则表达式的第一个具体值。sub 函数返回替换的数量。出现在 Repl 参数指定的字符串中的 &amp;（和符号）由 In 参数指定的与 Ere 参数的指定的扩展正则表达式匹配的字符串替换。如果未指定 In 参数，缺省值是整个记录（$0 记录变量）。 index( String1, String2 ) 在由 String1 参数指定的字符串（其中有出现 String2 指定的参数）中，返回位置，从 1 开始编号。如果 String2 参数不在 String1 参数中出现，则返回 0（零）。 length [(String)] 返回 String 参数指定的字符串的长度（字符形式）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 blength [(String)] 返回 String 参数指定的字符串的长度（以字节为单位）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 substr( String, M, [ N ] ) 返回具有 N 参数指定的字符数量子串。子串从 String 参数指定的字符串取得，其字符以 M 参数指定的位置开始。M 参数指定为将 String 参数中的第一个字符作为编号 1。如果未指定 N 参数，则子串的长度将是 M 参数指定的位置到 String 参数的末尾 的长度。 match( String, Ere ) 在 String 参数指定的字符串（Ere 参数指定的扩展正则表达式出现在其中）中返回位置（字符形式），从 1 开始编号，或如果 Ere 参数不出现，则返回 0（零）。RSTART 特殊变量设置为返回值。RLENGTH 特殊变量设置为匹配的字符串的长度，或如果未找到任何匹配，则设置为 -1（负一）。 split( String, A, [Ere] ) 将 String 参数指定的参数分割为数组元素 A[1], A[2], . . ., A[n]，并返回 n 变量的值。此分隔可以通过 Ere 参数指定的扩展正则表达式进行，或用当前字段分隔符（FS 特殊变量）来进行（如果没有给出 Ere 参数）。除非上下文指明特定的元素还应具有一个数字值，否则 A 数组中的元素用字符串值来创建。 tolower( String ) 返回 String 参数指定的字符串，字符串中每个大写字符将更改为小写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 toupper( String ) 返回 String 参数指定的字符串，字符串中每个小写字符将更改为大写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 sprintf(Format, Expr, Expr, . . . ) 根据 Format 参数指定的 printf 子例程格式字符串来格式化 Expr 参数指定的表达式并返回最后生成的字符串。 注：Ere都可以是正则表达式。 gsub,sub使用 awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;gsub(/[0-9]+/,&quot;!&quot;,info);print info}&#39; this is a test!test! 在 info中查找满足正则表达式，/[0-9]+/用””替换，并且替换后的值，赋值给info 未给info值，默认是$0 查找字符串（index使用） awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print index(info,&quot;test&quot;)?&quot;ok&quot;:&quot;no found&quot;;}&#39; ok 未找到，返回0 正则表达式匹配查找(match使用） awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print match(info,/[0-9]+/)?&quot;ok&quot;:&quot;no found&quot;;}&#39; ok 截取字符串(substr使用） [wangsl@centos5 ~]$ awk &#39;BEGIN{info=&quot;this is a test2010test!&quot;;print substr(info,4,10);}&#39; s is a tes 从第 4个 字符开始，截取10个长度字符串 字符串分割（split使用） awk &#39;BEGIN{info=&quot;this is a test&quot;;split(info,tA,&quot; &quot;);print length(tA);for(k in tA){print k,tA[k];}}&#39; 4 4 test 1 this 2 is 3 a 分割info，动态创建数组tA，这里比较有意思，awk for …in循环，是一个无序的循环。 并不是从数组下标1…n ，因此使用时候需要注意。 格式化字符串输出（sprintf使用） 格式化字符串格式： 其中格式化字符串包括两部分内容：一部分是正常字符，这些字符将按原样输出; 另一部分是格式化规定字符，以”%”开始，后跟一个或几个规定字符,用来确定输出内容格式。 格式 描述 %d 十进制有符号整数 %u 十进制无符号整数 %f 浮点数 %s 字符串 %c 单个字符 %p 指针的值 %e 指数形式的浮点数 %x %X无符号以十六进制表示的整数 %o 无符号以八进制表示的整数 %g 自动选择合适的表示法 awk &#39;BEGIN{n1=124.113;n2=-1.224;n3=1.2345; printf(&quot;%.2f,%.2u,%.2g,%X,%on&quot;,n1,n2,n3,n1,n1);}&#39; 124.11,18446744073709551615,1.2,7C,174 一般函数 格式 描述 close( Expression ) 用同一个带字符串值的 Expression 参数来关闭由 print 或 printf 语句打开的或调用 getline 函数打开的文件或管道。如果文件或管道成功关闭，则返回 0；其它情况下返回非零值。如果打算写一个文件，并稍后在同一个程序中读取文件，则 close 语句是必需的。 system(command ) 执行 Command 参数指定的命令，并返回退出状态。等同于 system 子例程。 Expression&#124;getline [ Variable ] 从来自 Expression 参数指定的命令的输出中通过管道传送的流中读取一个输入记录，并将该记录的值指定给 Variable 参数指定的变量。如果当前未打开将 Expression 参数的值作为其命令名称的流，则创建流。创建的流等同于调用 popen 子例程，此时 Command 参数取 Expression 参数的值且 Mode 参数设置为一个是 r 的值。只要流保留打开且 Expression 参数求得同一个字符串，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。 getline [ Variable ] &lt; Expression 从 Expression 参数指定的文件读取输入的下一个记录，并将 Variable 参数指定的变量设置为该记录的值。只要流保留打开且 Expression 参数对同一个字符串求值，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。 getline [ Variable ] 将 Variable 参数指定的变量设置为从当前输入文件读取的下一个输入记录。如果未指定 Variable 参数，则 $0 记录变量设置为该记录的值，还将设置 NF、NR 和 FNR 特殊变量。 打开外部文件（close用法） awk &#39;BEGIN{while(&quot;cat /etc/passwd&quot;|getline){print $0;};close(&quot;/etc/passwd&quot;);}&#39; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin 逐行读取外部文件(getline使用方法） awk &#39;BEGIN{while(getline &lt; &quot;/etc/passwd&quot;){print $0;};close(&quot;/etc/passwd&quot;);}&#39; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin awk &#39;BEGIN{print &quot;Enter your name:&quot;;getline name;print name;}&#39; Enter your name: chengmo chengmo 调用外部应用程序(system使用方法） awk &#39;BEGIN{b=system(&quot;ls -al&quot;);print b;}&#39; total 42092 drwxr-xr-x 14 chengmo chengmo 4096 09-30 17:47 . drwxr-xr-x 95 root root 4096 10-08 14:01 .. b返回值，是执行结果。 时间函数 格式 描述 函数名 说明 mktime( YYYY MM dd HH MM ss[ DST]) 生成时间格式 strftime([format [, timestamp]]) 格式化时间输出，将时间戳转为时间字符串 具体格式，见下表. systime() 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数 建指定时间(mktime使用） awk &#39;BEGIN{tstamp=mktime(&quot;2001 01 01 12 12 12&quot;);print strftime(&quot;%c&quot;,tstamp);}&#39; 2001年01月01日 星期一 12时12分12秒 awk &#39;BEGIN{tstamp1=mktime(&quot;2001 01 01 12 12 12&quot;);tstamp2=mktime(&quot;2001 02 01 0 0 0&quot;);print tstamp2-tstamp1;}&#39; 2634468 求2个时间段中间时间差，介绍了strftime使用方法 awk &#39;BEGIN{tstamp1=mktime(&quot;2001 01 01 12 12 12&quot;);tstamp2=systime();print tstamp2-tstamp1;}&#39; 308201392 strftime日期和时间格式说明符 格式 描述 %a 星期几的缩写(Sun) %A 星期几的完整写法(Sunday) %b 月名的缩写(Oct) %B 月名的完整写法(October) %c 本地日期和时间 %d 十进制日期 %D 日期 08/20/99 %e 日期，如果只有一位会补上一个空格 %H 用十进制表示24小时格式的小时 %I 用十进制表示12小时格式的小时 %j 从1月1日起一年中的第几天 %m 十进制表示的月份 %M 十进制表示的分钟 %p 12小时表示法(AM/PM) %S 十进制表示的秒 %U 十进制表示的一年中的第几个星期(星期天作为一个星期的开始) %w 十进制表示的星期几(星期天是0) %W 十进制表示的一年中的第几个星期(星期一作为一个星期的开始) %x 重新设置本地日期(08/20/99) %X 重新设置本地时间(12：00：00) %y 两位数字表示的年(99) %Y 当前月份 %Z 时区(PDT) %% 百分号(%) sed 对数据行进行替换、删除、新增、选取等操作 a 新增，在新的下一行出现 c 取代，取代 n1,n2 之间的行 eg: sed &#39;1,2c Hi&#39; ab d 删除 i 插入，在新的上一行出现 eg: #指定时间段查看日志 sed -n &#39;/2016-10-21 14:18:29/,/2016-10-21 18:18:29/p&#39; catalina.out #替换当前目录下所有文件中的 /usr/local为/data/dshp sed -i &quot;s/\/usr\/local/\/data\/dshp/g&quot; . paste 合并文件，需确保合并的两文件行数相同 -d 指定不同于空格或tab键的域分隔符 -s 按行合并，单独一个文件为一行 rename 重命名文件 #批量重命名相同前缀的文件 #将当前目录下所有以central开头的文件中的central替换为distributed rename central distributed central* wc wc 计算数字 利用wc指令我们可以计算文件的Byte数、字数或是列数，若不指定文件名称，或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据 wc(选项)(参数) #(选项) -c或--bytes或——chars：只显示Bytes数； -l或——lines：只显示列数； -w或——words：只显示字数。 #(参数) 文件：需要统计的文件列表。 统计当前文件夹下文件的个数 ls -l |grep &quot;^-&quot;|wc -l 统计当前文件夹下目录的个数 ls -l |grep &quot;^d&quot;|wc -l 统计当前文件夹下文件的个数，包括子文件夹里的 ls -lR|grep &quot;^-&quot;|wc -l 统计文件夹下目录的个数，包括子文件夹里的 ls -lR|grep &quot;^d&quot;|wc -l ls -l #长列表输出当前文件夹下文件信息(注意这里的文件，不同于一般的文件，可能是目录、链接、设备文件等) grep &quot;^-&quot; #这里将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d wc -l #统计当前目录下指定文件后缀的行数，既可以统计项目的代码量 find . -name &quot;*.java&quot; -or -name &quot;*.jsp&quot; -or -name &quot;*.xml&quot; -or -name &quot;*.c&quot; |xargs grep -v &quot;^$&quot;|wc -l #统计输出信息的行数，因为已经过滤得只剩一般文件了，所以统计结果就是一般文件信息的行数，又由于一行信息对应一个文件，所以也就是文件的个数 uniq uniq 去除文件中相邻的重复行 清空/新建文件，将内容重定向输入进去 &amp;&gt; 正确、错误都重定向过去 后面追加 file 该命令用于判断接在file命令后的文件的基本数据，因为在Linux下文件的类型并不是以后缀为分的，所以这个命令对我们来说就很有用了，它的用法非常简单，基本语法如下： [plain] view plain copy print? file filename #例如： file ./test mkdir 创建新目录 mkdir [选项] 目录… -p #递归创建目录，若父目录不存在则依次创建 eg: mkdir -p ~/temp/test -m #自定义创建目录的权限 eg:mkdir -m 777 temp -v #显示创建目录的详细信息 eg:mkdir -m 662 -pv ~/temp/test grep 用正则表达式搜索文本，并把匹配的行打印出来 grep ‘正则表达式’ 文件名 | -c 只输出匹配行的计数。 -I 不区分大小写(只适用于单字符)。 -l 只显示文件名 -v 显示不包含匹配文本的所有行。 -n 显示匹配行数据及其行号 eg: # 取出文件urls.txt中包含mysql的行，并把找到的关键字加上颜色 grep --color=auto &#39;mysql&#39; urls.txt # 把ls -l的输出中包含字母file（不区分大小写）的内容输出 ls -l | grep -i file # 查找当前目录下所有包含mysql的文件并逐行显示,文件路径+行号+匹配内容 grep -rn &quot;mysql&quot; ./* find 在文件树种查找文件，并作出相应的处理 find [PATH] [option] [action] 选项与参数： 与时间有关的选项：共有 -atime, -ctime 与 -mtime 和-amin,-cmin与-mmin，以 -mtime 说明 -mtime n ：n 为数字，意义为在 n 天之前的『一天之内』被更动过内容的档案； -mtime +n ：列出在 n 天之前(不含 n 天本身)被更动过内容的档案档名； -mtime -n ：列出在 n 天之内(含 n 天本身)被更动过内容的档案档名。 -newer file ：file 为一个存在的档案，列出比 file 还要新的档案档名 eg： find ./ -mtime 0 # 在当前目录下查找今天之内有改动的文件 与使用者或组名有关的参数： -uid n ：n 为数字，这个数字是用户的账号 ID，亦即 UID -gid n ：n 为数字，这个数字是组名的 ID，亦即 GID -user name ：name 为使用者账号名称！例如 dmtsai -group name：name 为组名，例如 users ； -nouser ：寻找档案的拥有者不存在 /etc/passwd 的人！ -nogroup ：寻找档案的拥有群组不存在于 /etc/group 的档案！ eg： find /home/jet -user jet # 在目录/home/jet中找出所有者为jet的文件 与档案权限及名称有关的参数： -name filename #搜寻文件名为 filename 的档案（可使用通配符） -size [+-]SIZE #搜寻比 SIZE 还要大(+)或小(-)的档案。这个 SIZE 的规格有： c: 代表 byte k: 代表 1024bytes。所以，要找比 50KB还要大的档案，就是『 -size +50k 』 -type TYPE #搜寻档案的类型为 TYPE 的，类型主要有： 一般正规档案 (f) 装置档案 (b, c) 目录 (d) 连结档 (l) socket (s) FIFO (p) -perm mode #搜寻档案权限『刚好等于』 mode的档案，这个mode为类似chmod的属性值 举例来说，-rwsr-xr-x 的属性为4755！ -perm -mode #搜寻档案权限『必须要全部囊括 mode 的权限』的档案 举例来说，我们要搜寻-rwxr--r-- 亦即 0744 的档案，使用-perm -0744，当一个档案的权限为 -rwsr-xr-x ，亦即 4755 时，也会被列出来，因为 -rwsr-xr-x 的属性已经囊括了 -rwxr--r-- 的属性了。 -perm +mode #搜寻档案权限『包含任一 mode 的权限』的档案 举例来说，我们搜寻-rwxr-xr-x ，亦即 -perm +755 时，但一个文件属性为 -rw-------也会被列出来，因为他有 -rw.... 的属性存在！ eg： find / -name passwd # 查找文件名为passwd的文件 find . -perm 0755 # 查找当前目录中文件权限的0755的文件 find . -size +12k # 查找当前目录中大于12KB的文件，注意c表示byte 额外可进行的动作： -exec command #command 为其他指令，-exec 后面可再接额外的指令来处理搜寻到的结果。 -print ：将结果打印到屏幕上，这个动作是预设动作！ eg: find / -perm +7000 -exec ls -l {} \; #额外指令以-exec开头，以\;结尾{}代替前面找到的内容 | xargs -i 默认的前面输出用{}代替 eg: # 将当前目录下所有以.log结尾的文件移动到文件夹logs中，logs文件夹需要先建立好，不然会生成一个logs空文件 find . -name &quot;*.log&quot; | xargs -i mv {} logs # 查找当前目录下所有log结尾的文件并删除 find . -name *.log | xargs rm dd 用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换(convert and copy a file ) 语法：dd [选项] if =输入文件（或设备名称）。 of =输出文件（或设备名称）。 ibs = bytes 一次读取bytes字节，即读入缓冲区的字节数。 skip = blocks 跳过读入缓冲区开头的ibs*blocks块。 obs = bytes 一次写入bytes字节，即写入缓冲区的字节数。 bs = bytes 同时设置读/写缓冲区的字节数（等于设置ibs和obs）。 cbs = byte 一次转换bytes字节。 count=blocks 只拷贝输入的blocks块。 conv = ASCII 把EBCDIC码转换为ASCIl码。 conv = ebcdic 把ASCIl码转换为EBCDIC码。 conv = ibm 把ASCIl码转换为alternate EBCDIC码。 conv = block 把变动位转换成固定字符。 conv = ublock 把固定位转换成变动位。 conv = ucase 把字母由小写转换为大写。 conv = lcase 把字母由大写转换为小写。 conv = notrunc 不截短输出文件。 conv = swab 交换每一对输入字节。 conv = noerror 出错时不停止处理。 conv = sync 把每个输入记录的大小都调到ibs的大小（用NUL填充）。 例1：要把一张软盘的内容拷贝到另一张软盘上，利用/tmp作为临时存储区。把源盘插入驱动器中，输入下述命令： $ dd if =/dev/fd0 of = /tmp/tmpfile 拷贝完成后，将源盘从驱动器中取出，把目标盘插入，输入命令： $ dd if = /tmp/tmpfile of =/dev/fd0 软盘拷贝完成后，应该将临时文件删除： $ rm /tmp/tmpfile 例2：把net.i这个文件写入软盘中，并设定读/写缓冲区的数目。 （注意：软盘中的内容会被完全覆盖掉） $ dd if = net.i of = /dev/fd0 bs = 16384 例3：将文件sfile拷贝到文件 dfile中。 $ dd if=sfile of=dfile 例4：创建一个100M的空文件 $ dd if=/dev/zero of=hello.txt bs=100M count=1 # 创建一个大小为1k的空文件 $ dd if=/dev/zero of=./test.txt bs=1k count=1 $ ls -l total 4 -rw-rw-r--. 1 jet jet 1024 Jun 20 16:36 test.txt # 将access_log中错误信息丢弃 $ find / -name access_log 2&gt;/dev/null &gt;-&gt;&gt;-/dev/null-/dev/zero /dev/null: 它是空设备，也称为位桶（bit bucket），外号叫无底洞，任何写入它的输出都会被抛弃。如果不想让消息以标准输出显示或写入文件，那么可以将消息重定向到位桶。 /dev/zero: 是一个输入设备，该设备无穷尽地提供0，可以使用任何你需要的数目，用于向设备或文件写入字符串0，你可你用它来初始化文件。 &lt; ：由 &lt; 的右边读入参数档案； &gt; ：将原本由屏幕输出的正确数据输出到 &gt; 右边的 file ( 文件名称 ) 或 device ( 装置，如 printer )去； &gt;&gt; ：将原本由屏幕输出的正确数据输出到 &gt;&gt; 右边，与 &gt; 不同的是，该档案将不会被覆盖，而新的数据将以『增加的方式』增加到该档案的最后面； 2&gt; ：将原本应该由屏幕输出的错误数据输出到 2&gt; 的右边去。 说明 [jet @jet oschina_hexo_server]# ls -al &gt; test.txt # 将显示的结果输出到 test.txt 档案中，若该档案以存在则覆盖！ [jet @jet oschina_hexo_server]# ls -al &gt;&gt; test.txt # 将显示的结果追加到 test.txt 档案中，该档案为累加的，旧数据保留！ [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt; test.err # 将显示的数据，正确的输出到 test.txt 错误的数据输出到 test.err [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt;&amp;1 # 将显示的数据，不论正确或错误均输出到 test.txt 当中！ [jet @jet oschina_hexo_server]# ls -al 1&gt; test.txt 2&gt; /dev/null # 将显示的数据，正确的输出到 test.txt 错误的数据则予以丢弃！ 【注意】错误与正确档案输出到同一个档案中，则必须以上面的方法来写！ 不能写成其它格式！这个观念相当的重要，尤其是在 /etc/crontab 当中执行的时候，如果我们已经知道错误的讯息为何，又不想要让错误的讯息一直填满 root 的信箱，就必须以 2&gt; 搭配 /dev/null 这个垃圾桶黑洞装置，来将数据丢弃！这个相当的重要！ rm 删除文件 rm [选项] 文件 -r 【--recursive】删除文件夹即递归的删除目录下面文件以及子目录下文件。 -f 【--force】强制删除不提示，忽略不存在的文件。 -i 【--interactive】交互模式删除文件，删除文件前给出提示 -v 【-verbose】详细显示进行步骤 cd 切换工作目录 cd . #返回上层目录 cd .. #返回上层目录 cd 回车 #返回主目录同cd ~ cd / #根目录 cd ~/git/ #主目录下的git目录 cd - #回到之前的目录 ls 列出相关目录下的所有目录和文件 ls [选项] [目录名] -a 列出包括.a开头的隐藏文件的所有文件 -A 通-a，但不列出&quot;.&quot;和&quot;..&quot; -l 列出文件的详细信息 -c 根据ctime排序显示 -t 根据文件修改时间排序 ---color[=WHEN] 用色彩辨别文件类型 WHEN 可以是’never’、’always’或’auto’其中之一 白色：表示普通文件 蓝色：表示目录 绿色：表示可执行文件 红色：表示压缩文件 浅蓝色：链接文件 红色闪烁：表示链接的文件有问题 黄色：表示设备文件 灰色：表示其它文件 mv 移动或重命名文件 mv [选项] 源文件或目录 目录或多个源文件 -b 覆盖前做备份 -f 如存在不询问而强制覆盖 -i 如存在则询问是否覆盖 -u 较新才覆盖 -t 将多个源文件移动到统一目录下，目录参数在前，文件参数在后 eg: mv a /tmp/ 将文件a移动到 /tmp目录下 mv a b 将a命名为b mv /home/zenghao test1.txt test2.txt test3.txt cp 将源文件复制至目标文件，或将多个源文件复制至目标目录。 cp [选项] 源文件或目录 目录或多个源文件 -r -R #递归复制该目录及其子目录内容 -p #连同档案属性一起复制过去 -f #不询问而强制复制 -s #生成快捷方式 -a #将档案的所有特性都一起复制 eg: cp -a file1 file2 #连同文件的所有特性把文件file1复制成文件file2 cp file1 file2 file3 dir #把文件file1、file2、file3复制到目录dir中 touch 创建空文件或更新文件时间 touch [选项] 文件 -a #只修改存取时间 -m #值修改变动时间 -r #eg:touch -r a b ,使b的时间和a相同 -t #指定特定的时间 eg:touch -t 201211142234.50 log.log #-t time [[CC]YY]MMDDhhmm[.SS],C:年前两位 pwd 查看当前所在路径 rmdir 删除空目录 -v 显示执行过程 -p 若自父母删除后父目录为空则一并删除 rm 删除一个或多个文件或目录 rm [选项].. 文件 -f 忽略不存在的文件，不给出提示 -i 交互式删除 -r 将列出的目录及其子目录递归删除 -v 列出详细信息 echo 显示内容到屏幕 -n 输出后不换行 -e 遇到转义字符特殊处理 eg: echo &quot;hello\nworld&quot; 显示hello\nworld ehco -e &quot;hello\nworld&quot; 显示hello(换行了)world cat 一次显示整个文件或从键盘创建一个文件或将几个文件合并成一个文件 cat [选项] [文件].. -n 编号文件内容再输出 -E 在结束行提示$ tac cat的反向显示 more 按页查看文章内容，从前向后读取文件，因此在启动时就加载整个文件 +n 从第n行开始显示 -n 每次查看n行数据 +/String 搜寻String字符串位置，从其前两行开始查看 -c 清屏再显示 -p 换页时清屏 less 可前后移动地逐屏查看文章内容，在查看前不会加载整个文件 -m 显示类似于more命令的百分比 -N 显示行号 / 字符串：向下搜索“字符串”的功能 ? 字符串：向上搜索“字符串”的功能 n 重复前一个搜索（与 / 或 ? 有关） N 反向重复前一个搜索（与 / 或 ? 有关） b 向后翻一页 d 向后翻半页 nl 将输出内容自动加上行号 nl [选项]… [文件]… -b -b a 不论是否有空行，都列出行号（类似 cat -n) -b t 空行则不列行号（默认） -n 有ln rn rz三个参数，分别为再最左方显示，最右方显示不加0，最右方显示加0 head 显示档案开头，默认开头10行 head [参数]… [文件]… -v 显示文件名 -c number 显示前number个字符,若number为负数,则显示除最后number个字符的所有内容 -number/n (+)number 显示前number行内容， -n number 若number为负数，则显示除最后number行数据的所有内容 tail 显示文件结尾内容 tail [必要参数] [选择参数] [文件] -v #显示详细的处理信息 -q #不显示处理信息 -num/-n (-)num #显示最后num行内容 -n +num #从第num行开始显示后面的数据 -c #显示最后c个字符 -f #循环读取 # 实时查看日志，从文件最后50行开始 tail -fn 50 ExecuteConfig.log vi 编辑文件 :w filename #将文章以指定的文件名保存起来 :q #退出 :q! #强制退出 :wq #保存并退出 :set nu #显示行号 :set nonu #隐藏行号 /git #在文档中查找git 按n跳到下一个，shift+n上一个 yyp #复制光标所在行，并粘贴 h(左移一个字符←)、j(下一行↓)、k(上一行↑)、l(右移一个字符→) 命令行模式功能键 1. 插入模式 按「i」切换进入插入模式「insert mode」，按&quot;i&quot;进入插入模式后是从光标当前位置开始输入文件； 按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字； 按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。 2. 从插入模式切换为命令行模式 按「ESC」键。 3. 移动光标 vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。 按「ctrl」+「b」#屏幕往&quot;后&quot;移动一页。 按「ctrl」+「f」#屏幕往&quot;前&quot;移动一页。 按「ctrl」+「u」#屏幕往&quot;后&quot;移动半页。 按「ctrl」+「d」#屏幕往&quot;前&quot;移动半页。 按数字「0」#移到文章的开头。 按「G」#移动到文章的最后。 按「$」#移动到光标所在行的&quot;行尾&quot;。 按「^」#移动到光标所在行的&quot;行首&quot; 按「w」#光标跳到下个字的开头 按「e」#光标跳到下个字的字尾 按「b」#光标回到上个字的开头 按「#l」#光标移到该行的第#个位置，如：5l,56l。 4. 删除文字 「x」#每按一次，删除光标所在位置的&quot;后面&quot;一个字符。 「#x」#例如，「6x」表示删除光标所在位置的&quot;后面&quot;6个字符。 「X」#大写的X，每按一次，删除光标所在位置的&quot;前面&quot;一个字符。 「#X」#例如，「20X」表示删除光标所在位置的&quot;前面&quot;20个字符。 「dd」#删除光标所在行。 「#dd」#从光标所在行开始删除#行 5. 复制 「yw」#将光标所在之处到字尾的字符复制到缓冲区中。 「#yw」#复制#个字到缓冲区 「yy」#复制光标所在行到缓冲区。 「#yy」#例如，「6yy」表示拷贝从光标所在的该行&quot;往下数&quot;6行文字。 「p」#将缓冲区内的字符贴到光标所在位置。注意：所有与&quot;y&quot;有关的复制命令都必须与&quot;p&quot;配合才能完成复制与粘贴功能。 6. 替换 「r」#替换光标所在处的字符。 「R」#替换光标所到之处的字符，直到按下「ESC」键为止。 7. 恢复上一次操作 「u」#如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次&quot;u&quot;可以执行多次回复。 8. 更改 「cw」#更改光标所在处的字到字尾处 「c#w」#例如，「c3w」表示更改3个字 9. 跳至指定的行 「ctrl」+「g」 #列出光标所在行的行号。 「#G」：例如，「15G」，表示移动光标至文章的第15行行首。 10.视图模式 「ctrl」+「v」#进入视图模式，可以移动方向键选中多行，按d键可以删除 11.清空文件 :.,$d 回车 #命令模式输入 .,$d 后回车 12.替换文本内容 #可以替换当前行的所有/usr/local为/data/dshp，命令模式输入s/old\new/g :s/\/usr\/local/\/data\/dshp/g 管道 将一个命令的标准输出作为另一个命令的标准输入。也就是把几个命令组合起来使用，后一个命令除以前一个命令的结果。 grep -r “close” /home/* | more 在home目录下所有文件中查找，包括close的文件，并分页输出。 xargs 管道实现的是将前面的stdout作为后面的stdin，但是有些命令不接受管道的传递方式，最常见的就是ls命令。有些时候命令希望管道传递的是参数，但是直接用管道有时无法传递到命令的参数位，这时候需要xargs，xargs实现的是将管道传输过来的stdin进行处理然后传递到命令的参数位上。也就是说xargs完成了两个行为：处理管道传输过来的stdin；将处理后的传递到正确的位置上。 -0 当sdtin含有特殊字元时候，将其当成一般字符，如/、空格等 例如：root@localhost:~/test#echo &quot;//&quot;|xargs echo root@localhost:~/test#echo &quot;//&quot;|xargs -0 echo / -a file 从文件中读入作为sdtin，（看例一） -e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。（例二） -p 当每次执行一个argument的时候询问一次用户。（例三） -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。（例四） -t 表示先打印命令，然后再执行。（例五） -i 或者是-I，这得看Linux支持了，将xargs的每项名称，一般是一行一行赋值给{}，可以用{}代替。（例六） -r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。（例七） -s num 命令行的最好字符数，指的是xargs后面那个命令的最大命令行字符数。（例八） -L num Use at most max-lines nonblank input lines per command line.-s是含有空格的。 -l 同-L -d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符（例九） -x exit的意思，主要是配合-s使用。 -P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 示例例一： root@localhost:~/test#cat test #!/bin/sh echo &quot;hello world/n&quot; root@localhost:~/test#xargs -a test echo #!/bin/sh echo hello world/n 例二： root@localhost:~/test#cat txt /bin tao shou kun root@localhost:~/test#cat txt|xargs -E &#39;shou&#39; echo /bin tao 例三： root@localhost:~/test#cat txt|xargs -p echo echo /bin tao shou kun ff ?...y /bin tao shou kun ff 例四： root@localhost:~/test#cat txt|xargs -n1 echo /bin tao shou kun root@localhost:~/test3#cat txt|xargs echo /bin tao shou kun 例五： root@localhost:~/test#cat txt|xargs -t echo echo /bin tao shou kun /bin tao shou kun 例六： $ ls | xargs -t -i mv {} {}.bak 例七： root@localhost:~/test#echo &quot;&quot;|xargs -t mv mv mv: missing file operand Try `mv --help&#39; for more information. root@localhost:~/test#echo &quot;&quot;|xargs -t -r mv root@localhost:~/test# （直接退出） 例八： root@localhost:~/test#cat test |xargs -i -x -s 14 echo &quot;{}&quot; exp1 exp5 file xargs: argument line too long linux-2 例九： root@localhost:~/test#cat txt |xargs -i -p echo {} echo /bin tao shou kun ?...y root@localhost:~/test#cat txt |xargs -i -p -d &quot; &quot; echo {} echo /bin ?...y echo tao ?.../bin y echo shou ?...tao 再如： root@localhost:~/test#cat test |xargs -i -p -d &quot; &quot; echo {} echo exp1 exp5 file linux-2 ngis_post tao test txt xen-3 ?...y root@localhost:~/test#cat test |xargs -i -p echo {} echo exp1 ?...y echo exp5 ?...exp1 y echo file ?...exp5 y diff 以逐行的方式，比较文本文件的异同处。指定要比较目录，则diff会比较目录中相同文件名的文件，但不会比较其中子目录。 diff [参数] [文件1或目录1] [文件2或目录2] -&lt;行数&gt; 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或--text diff预设只会逐行比较文本文件。 -b或--ignore-space-change 不检查空格字符的不同。 -B或--ignore-blank-lines 不检查空白行。 -c 显示全部内文，并标出不同之处。 -C&lt;行数&gt;或--context&lt;行数&gt; 与执行&quot;-c-&lt;行数&gt;&quot;指令相同。 -d或--minimal 使用不同的演算法，以较小的单位来做比较。 -D&lt;巨集名称&gt;或ifdef&lt;巨集名称&gt; 此参数的输出格式可用于前置处理器巨集。 -e或--ed 此参数的输出格式可用于ed的script文件。 -f或-forward-ed 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。 -H或--speed-large-files 比较大文件时，可加快速度。 -l&lt;字符或字符串&gt;或--ignore-matching-lines&lt;字符或字符串&gt; 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。 -i或--ignore-case 不检查大小写的不同。 -l或--paginate 将结果交由pr程序来分页。 -n或--rcs 将比较结果以RCS的格式来显示。 -N或--new-file 在比较目录时，若文件A仅出现在某个目录中，预设会显示： Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。 -p 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。 -P或--unidirectional-new-file 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。 -q或--brief 仅显示有无差异，不显示详细的信息。 -r或--recursive 比较子目录中的文件。 -s或--report-identical-files 若没有发现任何差异，仍然显示信息。 -S&lt;文件&gt;或--starting-file&lt;文件&gt; 在比较目录时，从指定的文件开始比较。 -t或--expand-tabs 在输出时，将tab字符展开。 -T或--initial-tab 在每行前面加上tab字符以便对齐。 -u,-U&lt;列数&gt;或--unified=&lt;列数&gt; 以合并的方式来显示文件内容的不同。 -v或--version 显示版本信息。 -w或--ignore-all-space 忽略全部的空格字符。 -W&lt;宽度&gt;或--width&lt;宽度&gt; 在使用-y参数时，指定栏宽。 -x&lt;文件名或目录&gt;或--exclude&lt;文件名或目录&gt; 不比较选项中所指定的文件或目录。 -X&lt;文件&gt;或--exclude-from&lt;文件&gt; 您可以将文件或目录类型存成文本文件，然后在=&lt;文件&gt;中指定此文本文件。 -y或--side-by-side 以并列的方式显示文件的异同之处。 --help 显示帮助。 --left-column 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。 --suppress-common-lines 在使用-y参数时，仅显示不同之处 示例： 比较两个文件 [root@localhost test3]# diff log2014.log log2013.log 3c3 &lt; 2014-03 --- &gt; 2013-03 8c8 &lt; 2013-07 --- &gt; 2013-08 11,12d10 &lt; 2013-11 &lt; 2013-12 【注】 上面的”3c3”和”8c8”表示log2014.log和log20143log文件在3行和第8行内容有所不同；”11,12d10”表示第一个文件比第二个文件多了第11和12行。 并排格式输出 [root@localhost test3]# diff log2014.log log2013.log -y -W 50 2013-01 2013-01 2013-02 2013-02 2014-03 | 2013-03 2013-04 2013-04 2013-05 2013-05 2013-06 2013-06 2013-07 2013-07 2013-07 | 2013-08 2013-09 2013-09 2013-10 2013-10 2013-11 &lt; 2013-12 &lt; [root@localhost test3]# diff log2013.log log2014.log -y -W 50 2013-01 2013-01 2013-02 2013-02 2013-03 | 2014-03 2013-04 2013-04 2013-05 2013-05 2013-06 2013-06 2013-07 2013-07 2013-08 | 2013-07 2013-09 2013-09 2013-10 2013-10 &gt; 2013-11 &gt; 2013-12 【注】 “|”表示前后2个文件内容有不同 “&lt;”表示后面文件比前面文件少了1行内容 “&gt;”表示后面文件比前面文件多了1行内容 ln 某一个文件在另外一个位置建立一个同步的链接，通常给/usr/bin/下建立软连接，相当于给某个应用程序配置环境变量一样，可以不带路径直接运行命令 ln [参数] [源文件或目录] [目标文件或目录] -s 建立软连接 -v 显示详细的处理过程 which 查看可执行文件的位置，在PATH变量指定的路径中查看系统命令是否存在及其位置 which 可执行文件名称 whereis 定位可执行文件、源代码文件、帮助文件在文件系统中的位置 whereis [-bmsu] [BMS 目录名 -f ] 文件名 -b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 -B 指定搜索可执行文件的路径。 -M 指定搜索帮助文件的路径。 -S 指定搜索源代码文件的路径。 locate 通过搜寻数据库快速搜寻档案 -r 使用正规运算式做寻找的条件 系统管理相关命令ps 列出当前进程的快照 a 显示所有的进程 -a 显示同一终端下的所有程序 e 显示环境变量 f 显示进程间的关系 -H 显示树状结构 r 显示当前终端的程序 T 显示当前终端的所有程序 -au 显示更详细的信息 -aux 显示所有包含其他使用者的行程 -u 指定用户的所有进程 eg: ps aux # 查看系统所有的进程数据 ps ax # 查看不与terminal有关的所有进程 ps -lA # 查看系统所有的进程数据 ps axjf # 查看连同一部分进程树状态 ps aux | grep tomcat ps -ef | grep tomcat df 显示指定磁盘文件的可用空间,如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示 df [选项] [文件] -a 显示全部文件系统 -h 文件大小友好显示，即会显示单位 -l 只显示本地文件系统 -i 显示inode信息 -T 显示文件系统类型 du 显示每个文件和目录的磁盘使用空间 du [选项] [文件] -h 方便阅读的方式，会带单位 -s 只显示总和的大小 eg: #按照文件/文件夹从大到小降序排列，方便查找大文件 du -sh * | sort -nr | head top 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 top [参数] free 显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer free [参数] quota 显示用户或者工作组的磁盘配额信息。输出信息包括磁盘使用和配额限制 quota(选项)(参数) -g：列出群组的磁盘空间限制； -q：简明列表，只列出超过限制的部分； -u：列出用户的磁盘空间限制； -v：显示该用户或群组，在所有挂入系统的存储设备的空间限制； -V：显示版本信息。 我们可以限制某一群组所能使用的最大磁盘配额，而且可以再限制某一使用者的最大磁盘配额 ，好比做一个收费的应用，vip可以得到空间更大一些。另外，以 Link 的方式，来使邮件可以作为限制的配额（更改/var/spool/mail 这个路径），不2，需要重新再规划一个硬盘！直接使用 Link 的方式指向 /home （或者其它已经做好的 quota 磁盘）就可以！这通常是用在原本规划不好，但是却又不想要更动原有主机架构的情况中！ 要求：Linux 主机里面主要针对 quser1 及 quser2 两个使用者来进行磁盘配额， 且这两个使用者都是挂在 qgroup 组里面的。每个使用者总共有 50MB 的磁盘空间 (不考虑 inode) 限制！并且 soft limit 为 45 MB；而宽限时间设定为 1 天， 但是在一天之内必须要将多余的文件删除掉，否则将无法使用剩下的空间 ；gquota 这个组考虑最大限额，所以设定为 90 MB！（注意，这样设置的好处是富有弹性，好比现在的邮件服务，那么多用户，承诺给用户每人最大空间为数GB，然而人们不可能每人都会使用那么大的空间，所以邮件服务的总空间，实际上肯定不是注册客户数乘以数GB，否则这样得多大啊。） [root@jet ~]# groupadd qgroup [root@jet ~]# useradd -m -g qgroup quser1 [root@jet ~]# useradd -m -g qgroup quser2 [root@jet ~]# passwd quser1 [root@jet ~]# passwd quser2 [root@jet ~]# df #用/disk2测试 Filesystem 1K-blocks Used Available Use% Mounted on /dev/hda1 5952252 3193292 2451720 57% / /dev/hdb1 28267608 77904 26730604 1% /disk2 /dev/hda5 9492644 227252 8775412 3% /disk1 [root@jet ~]# vi /etc/fstab LABEL=/ / ext3 defaults 1 1 LABEL=/disk1 /disk1 ext3 defaults 1 2 LABEL=/disk2 /disk2 ext3 defaults,usrquota,grpquota 1 2 /dev/hda3 swap swap defaults 0 0 注意多了usrquota,grpquota，在defaults,usrquota,grpquota之间都没有空格，务必正确书写。这样就算加入了 quota 的磁盘格式了！不过，由于真正的 quota 在读取的时候是读取/etc/mtab这个文件的，而该文件需要重新开机之后才能够以/etc/fstab 的新数据进行改写！所以这个时候可以选择：重新开机 (reboot)。 重新remount filesystem来驱动设定值。 [root@jet ~]# umount /dev/hdb1 [root@jet ~]# mount -a [root@jet ~]# grep &#39;/disk2&#39; /etc/mtab /dev/hdb1 /disk2 ext3 rw,usrquota,grpquota 0 0 事实上，也可以利用 mount 的 remount 功能。 [root@jet ~]# mount -o remount /disk2 这样就已经成功的将 filesystem 的 quota 功能加入。 扫瞄磁盘的使用者使用状况，并产生重要的 aquota.group 与 aquota.user： [root@jet ~]# quotacheck -avug quotacheck: Scanning /dev/hdb1 [/disk2] done quotacheck: Checked 3 directories and 4 files [root@localhost ~]# ll /disk2 -rw------- 1 root root 6144 Sep 6 11:44 aquota.group -rw------- 1 root root 6144 Sep 6 11:44 aquota.user 使用 quotacheck 就可以轻易的将所需要的数据给他输出了！但奇怪的是，在某些 Linux 版本中，不能够以 aquota.user(group) 来启动quota ，可能是因为旧版 quota 的关系， 所以就另外做了一个 link 文件按来欺骗 quota，这个动作非必要。（主要是学习这个思维很重要） [root@localhost ~]# cd /disk2 [root@localhost ~]# ln -s aquota.user quota.user [root@localhost ~]# ln -s aquota.group quota.group 启动 quota 的限额： [root@localhost ~]# quotaon -avug /dev/hdb1 [/disk2]: group quotas turned on /dev/hdb1 [/disk2]: user quotas turned on ===&gt; 看到turned on，才是真的成功！ 编辑使用者的可使用空间： [root@localhost ~]# edquota -u quser1 Disk quotas for user quser1 (uid 502): Filesystem blocks soft hard inodes soft hard /dev/hdb1 0 45000 50000 0 0 0 [root@localhost ~]# edquota -p quser1 quser2 ===&gt; 直接复制给quser2 接下来要来设定宽限时间，还是使用 edquota [root@localhost ~]# edquota -t Grace period before enforcing soft limits for users: time units may be: days, hours, minutes, or seconds Filesystem Block grace period Inode grace period /dev/hdb1 1days 7days 使用quota -v来查询： [root@localhost ~]# quota -vu quser1 quser2 Disk quotas for user quser1 (uid 502): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 45000 50000 0 0 0 Disk quotas for user quser2 (uid 503): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 45000 50000 0 0 0 注意，由于使用者尚未超过45 MB，所以 grace ( 宽限时间 ) 就不会出现。 编辑群组可使用的空间： [root@localhost ~]# edquota -g qgroup Disk quotas for group qgroup (gid 502): Filesystem blocks soft hard inodes soft hard /dev/hdb1 0 80000 90000 0 0 0 [root@localhost ~]# quota -vg qgroup Disk quotas for group qgroup (gid 502): Filesystem blocks quota limit grace files quota limit grace /dev/hdb1 0 80000 90000 0 0 0 kill 用于向某个工作（%jobnumber）或者是某个PID（数字）传送一个信号，它通常与ps和jobs命令一起使用 kill [参数] [进程号] 1：SIGHUP，启动被终止的进程 2：SIGINT，相当于输入ctrl+c，中断一个程序的进行 9：SIGKILL，强制中断一个进程的进行 15：SIGTERM，以正常的结束进程方式来终止进程 17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行 eg: kill -9 19785 killall 杀死同一进程组内的所有进程。其允许指定要终止的进程的名称，而非PID -i ：交互式的意思，若需要删除时，会询问用户 -e ：表示后面接的command name要一致，但command name不能超过15个字符 -I ：命令名称忽略大小写 eg: killall -SIGHUP syslogd # 重新启动syslogd useradd 创建的新的系统用户 帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 在Slackware中，adduser指令是个script程序，利用交谈的方式取得输入的用户帐号资料，然后再交由真正建立帐号的useradd命令建立新用户，如此可方便管理员建立用户帐号。在Red Hat Linux中，adduser命令则是useradd命令的符号连接，两者实际上是同一个指令 useradd(选项)(参数) -c&lt;备注&gt;：加上备注文字。备注文字会保存在passwd的备注栏位中； -d&lt;登入目录&gt;：指定用户登入时的启始目录； -D：变更预设值； -e&lt;有效期限&gt;：指定帐号的有效期限； -f&lt;缓冲天数&gt;：指定在密码过期后多少天即关闭该帐号； -g&lt;群组&gt;：指定用户所属的群组； -G&lt;群组&gt;：指定用户所属的附加群组； -m：自动建立用户的登入目录； -M：不要自动建立用户的登入目录； -n：取消建立以用户名称为名的群组； -r：建立系统帐号； -s：指定用户登入后所使用的shell； -u：指定用户id。 新建用户加入组： useradd –g sales jack –G company,employees //-g：加入主要组、-G：加入次要组 建立一个新用户账户，并设置ID： useradd caojh -u 544 需要说明的是，设定ID值时尽量要大于500，以免冲突。因为Linux安装后会建立一些特殊用户，一般0到499之间的值留给bin、mail这样的系统账号 groupadd 创建一个新的工作组，新工作组的信息将被添加到系统文件中 groupadd(选项)(参数) -g：指定新建工作组的id； -r：创建系统工作组，系统工作组的组ID小于500； -K：覆盖配置文件“/ect/login.defs”； -o：允许添加组ID号不唯一的工作组。 实例 建立一个新组，并设置组ID加入系统： groupadd -g 344 linuxde 此时在/etc/passwd文件中产生一个组ID（GID）是344的项目 groupdel 删除指定的工作组 要修改的系统文件包括/ect/group和/ect/gshadow。若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组 groupdel(参数) eg: groupadd damon //创建damon工作组 groupdel damon //删除这个工作组 crontab 提交和管理用户的需要周期性执行的任务 与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 crontab(选项)(参数) -e：编辑该用户的计时器设置； -l：列出该用户的计时器设置； -r：删除该用户的计时器设置； -u&lt;用户名称&gt;：指定要设定计时器的用户名称。 Linux下的任务调度分为两类：系统任务调度和用户任务调度。 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=&quot;&quot;HOME=/ # run-parts 51 * * * * root run-parts /etc/cron.hourly 24 7 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly 【注】 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下： /etc/cron.deny 该文件中所列用户不允许使用crontab命令 /etc/cron.allow 该文件中所列用户允许使用crontab命令 /var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： minute hour day month week command 顺序：分 时 日 月 周 minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正 斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 service crond start #启动服务 service crond stop #关闭服务 service crond restart #重启服务 service crond reload #重新载入配置 service crond status #查看crontab服务状态 ntsysv #查看crontab服务是否已设置为开机启动 chkconfig –level 35 crond on #开机启动 eg: * * * * * command #每1分钟执行一次command 3,15 * * * * command #每小时的第3和第15分钟执行 3,15 8-11 * * * command #在上午8点到11点的第3和第15分钟执行 3,15 8-11 */2 * * command #每隔两天的上午8点到11点的第3和第15分钟执行 3,15 8-11 * * 1 command #每个星期一的上午8点到11点的第3和第15分钟执行 30 21 * * * /etc/init.d/smb restart #每晚的21:30重启smb 45 4 1,10,22 * * /etc/init.d/smb restart #每月1、10、22日的4 : 45重启smb 10 1 * * 6,0 /etc/init.d/smb restart #每周六、周日的1:10重启smb 0,30 18-23 * * * /etc/init.d/smb restart #每天18 : 00至23 : 00之间每隔30分钟重启smb 0 23 * * 6 /etc/init.d/smb restart #每星期六的晚上11:00 pm重启smb * */1 * * * /etc/init.d/smb restart #每一小时重启smb * 23-7/1 * * * /etc/init.d/smb restart #晚上11点到早上7点之间，每隔一小时重启smb 0 11 4 * mon-wed /etc/init.d/smb restart #每月的4号与每周一到周三的11点重启smb 0 4 1 jan * /etc/init.d/smb restart #一月一号的4点重启smb 01 * * * * root run-parts /etc/cron.hourly #每小时执行/etc/cron.hourly目录内的脚本 网络操作命令ifconfig 配置和显示Linux内核中网络接口的网络参数 用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 ifconfig(参数) add&lt;地址&gt;：设置网络设备IPv6的ip地址； del&amp;lt;地址&amp;gt;：删除网络设备IPv6的IP地址； down：关闭指定的网络设备； &amp;lt;hw&amp;lt;网络设备类型&amp;gt;&amp;lt;硬件地址&amp;gt;：设置网络设备的类型与硬件地址； io_addr&amp;lt;I/O地址&amp;gt;：设置网络设备的I/O地址； irq&amp;lt;IRQ地址&amp;gt;：设置网络设备的IRQ； media&amp;lt;网络媒介类型&amp;gt;：设置网络设备的媒介类型； mem_start&amp;lt;内存地址&amp;gt;：设置网络设备在主内存所占用的起始地址； metric&amp;lt;数目&amp;gt;：指定在计算数据包的转送次数时，所要加上的数目； mtu&amp;lt;字节&amp;gt;：设置网络设备的MTU； netmask&amp;lt;子网掩码&amp;gt;：设置网络设备的子网掩码； tunnel&amp;lt;地址&amp;gt;：建立IPv4与IPv6之间的隧道通信地址； up：启动指定的网络设备； -broadcast&amp;lt;地址&amp;gt;：将要送往指定地址的数据包当成广播数据包来处理； -pointopoint&amp;lt;地址&amp;gt;：与指定地址的网络设备建立直接连线，此模式具有保密功能； -promisc：关闭或启动指定网络设备的promiscuous模式； IP地址：指定网络设备的IP地址； 网络设备：指定网络设备的名称。 显示网络设备信息（激活状态的）： [root@jet ~]# ifconfig eth0 Link encap:Ethernet HWaddr 00:16:3E:00:1E:51 inet addr:10.160.7.81 Bcast:10.160.15.255 Mask:255.255.240.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:61430830 errors:0 dropped:0 overruns:0 frame:0 TX packets:88534 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:3607197869 (3.3 GiB) TX bytes:6115042 (5.8 MiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:56103 errors:0 dropped:0 overruns:0 frame:0 TX packets:56103 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5079451 (4.8 MiB) TX bytes:5079451 (4.8 MiB) 【注】eth0表示第一块网卡，其中HWaddr表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是00:16:3E:00:1E:51。 inet addr**用来表示网卡的IP地址，此网卡的IP地址是10.160.7.81，广播地址Bcast:10.160.15.255，掩码地址Mask:255.255.240.0。 lo是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址）。第二行：网卡的IP地址、子网、掩码。第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节。第四、五行：接收、发送数据包情况统计。第七行：接收、发送数据字节数统计信息。 启动关闭指定网卡： ifconfig eth0 up ifconfig eth0 down ifconfig eth0 up为启动网卡eth0，ifconfig eth0 down为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 为网卡配置和删除IPv6地址： ifconfig eth0 add 33ffe:3240:800:1005::2/64 #为网卡eth0配置IPv6地址 ifconfig eth0 del 33ffe:3240:800:1005::2/64 #为网卡eth0删除IPv6地址 用ifconfig修改MAC地址： ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE 配置IP地址： [root@localhost ~]# ifconfig eth0 192.168.2.10 [root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 [root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255 启用和关闭arp协议： ifconfig eth0 arp #开启网卡eth0 的arp协议 ifconfig eth0 -arp #关闭网卡eth0 的arp协议 设置最大传输单元： ifconfig eth0 mtu 1500 #设置能通过的最大数据包大小为 1500 bytes netstat 打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况 netstat(选项) -a或--all：显示所有连线中的Socket； -A&amp;lt;网络类型&amp;gt;或--&amp;lt;网络类型&amp;gt;：列出该网络类型连线中的相关地址； -c或--continuous：持续列出网络状态； -C或--cache：显示路由器配置的快取信息； -e或--extend：显示网络其他相关信息； -F或--fib：显示FIB； -g或--groups：显示多重广播功能群组组员名单； -h或--help：在线帮助； -i或--interfaces：显示网络界面信息表单； -l或--listening：显示监控中的服务器的Socket； -M或--masquerade：显示伪装的网络连线； -n或--numeric：直接使用ip地址，而不通过域名服务器； -N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称； -o或--timers：显示计时器； -p或--programs：显示正在使用Socket的程序识别码和程序名称； -r或--route：显示Routing Table； -s或--statistice：显示网络工作信息统计表； -t或--tcp：显示TCP传输协议的连线状况； -u或--udp：显示UDP传输协议的连线状况； -v或--verbose：显示指令执行过程； -V或--version：显示版本信息； -w或--raw：显示RAW传输协议的连线状况； -x或--unix：此参数的效果和指定&quot;-A unix&quot;参数相同； --ip或--inet：此参数的效果和指定&quot;-A inet&quot;参数相同。 eg: netstat -a #列出所有端口 netstat -at #列出所有tcp端口 netstat -au #列出所有udp端口 netstat -l #只显示监听端口 netstat -lt #只列出所有监听 tcp 端口 netstat -lu #只列出所有监听 udp 端口 netstat -lx #只列出所有监听 UNIX 端口 netstat -s #显示所有端口的统计信息 netstat -st #显示TCP端口的统计信息 netstat -su #显示UDP端口的统计信息 netstat -pt #在netstat输出中显示 PID 和进程名称 netstat -p可以与其它开关一起使用，就可以添加“PID/进程名称”到netstat输出中，这样debugging的时候可以很方便的发现特定端口运行的程序。 在netstat输出中不显示主机，端口和用户名(host, port or user) 当你不想让主机，端口和用户名显示，使用netstat -n。将会使用数字代替那些名称。同样可以加速输出，因为不用进行比对查询。 netstat -an 如果只是不想让这三个名称中的一个被显示，使用以下命令: netsat -a --numeric-ports netsat -a --numeric-hosts netsat -a --numeric-users 持续输出netstat信息 netstat -c #每隔一秒输出网络信息 显示系统不支持的地址族(Address Families) netstat --verbose 在输出的末尾，会有如下的信息： netstat: no support for `AF IPX&#39; on this system. netstat: no support for `AF AX25&#39; on this system. netstat: no support for `AF X25&#39; on this system. netstat: no support for `AF NETROM&#39; on this system. 显示核心路由信息 netstat -r 使用netstat -rn显示数字格式，不查询主机名称。 找出程序运行的端口 并不是所有的进程都能找到，没有权限的会不显示，使用 root 权限查看所有的信息。 netstat -ap | grep ssh 找出运行在指定端口的进程： netstat -an | grep &#39;:80&#39; 显示网络接口列表 netstat -i 显示详细信息，像是ifconfig使用 netstat -ie。 IP和TCP分析 查看连接某服务端口最多的的IP地址： netstat -ntu | grep :80 | awk &#39;{print $5}&#39; | cut -d: -f1 | awk &#39;{++ip[$1]} END {for(i in ip) print ip[i],&quot;\t&quot;,i}&#39; | sort -nr TCP各种状态列表： netstat -nt | grep -e 127.0.0.1 -e 0.0.0.0 -e ::: -v | awk &#39;/^tcp/ {++state[$NF]} END {for(i in state) print i,&quot;\t&quot;,state[i]}&#39; 查看phpcgi进程数，如果接近预设值，说明不够用，需要增加： netstat -anpo | grep &quot;php-cgi&quot; | wc -l telnet 登录远程主机，对远程主机进行管理 telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。 telnet(选项)(参数) -8：允许使用8位字符资料，包括输入与输出； -a：尝试自动登入远端系统； -b&lt;主机别名&gt;：使用别名指定远端主机名称； -c：不读取用户专属目录里的.telnetrc文件； -d：启动排错模式； -e&lt;脱离字符&gt;：设置脱离字符； -E：滤除脱离字符； -f：此参数的效果和指定&quot;-F&quot;参数相同； -F：使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机； -k&lt;域名&gt;：使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名； -K：不自动登入远端主机； -l&lt;用户名称&gt;：指定要登入远端主机的用户名称； -L：允许输出8位字符资料； -n&lt;记录文件&gt;：指定文件记录相关信息； -r：使用类似rlogin指令的用户界面； -S&lt;服务类型&gt;：设置telnet连线所需的ip TOS信息； -x：假设主机有支持数据加密的功能，就使用它； -X&lt;认证形态&gt;：关闭指定的认证形态。 eg: [root@jet oschina_hexo_server]# telnet 124.251.54.61 5001 Trying 124.251.54.61... Connected to 124.251.54.61. Escape character is &#39;^]&#39;. SSH-2.0-OpenSSH_6.7 ping 测试主机之间网络的连通性 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 ping(选项)(参数) -d：使用Socket的SO_DEBUG功能； -c&lt;完成次数&gt;：设置完成要求回应的次数； -f：极限检测； -i&lt;间隔秒数&gt;：指定收发信息的间隔时间； -I&lt;网络界面&gt;：使用指定的网络界面送出数据包； -l&lt;前置载入&gt;：设置在送出要求信息之前，先行发出的数据包； -n：只输出数值； -p&lt;范本样式&gt;：设置填满数据包的范本样式； -q：不显示指令执行过程，开头和结尾的相关信息除外； -r：忽略普通的Routing Table，直接将数据包送到远端主机上； -R：记录路由过程； -s&lt;数据包大小&gt;：设置数据包的大小； -t&lt;存活数值&gt;：设置存活数值TTL的大小； -v：详细显示指令的执行过程。 eg: [root@jet oschina_hexo_server]# ping www.baidu.com PING www.a.shifen.com (220.181.112.244) 56(84) bytes of data. 64 bytes from 220.181.112.244: icmp_seq=1 ttl=45 time=13.9 ms 64 bytes from 220.181.112.244: icmp_seq=2 ttl=45 time=3.27 ms 64 bytes from 220.181.112.244: icmp_seq=3 ttl=45 time=2.46 ms 64 bytes from 220.181.112.244: icmp_seq=4 ttl=45 time=3.39 ms 64 bytes from 220.181.112.244: icmp_seq=5 ttl=45 time=2.42 ms 64 bytes from 220.181.112.244: icmp_seq=6 ttl=45 time=2.70 ms 64 bytes from 220.181.112.244: icmp_seq=7 ttl=45 time=2.54 ms 64 bytes from 220.181.112.244: icmp_seq=8 ttl=45 time=3.78 ms 64 bytes from 220.181.112.244: icmp_seq=9 ttl=45 time=3.20 ms 64 bytes from 220.181.112.244: icmp_seq=10 ttl=45 time=2.46 ms 64 bytes from 220.181.112.244: icmp_seq=11 ttl=45 time=2.56 ms 64 bytes from 220.181.112.244: icmp_seq=12 ttl=45 time=2.74 ms 64 bytes from 220.181.112.244: icmp_seq=13 ttl=45 time=2.42 ms 64 bytes from 220.181.112.244: icmp_seq=14 ttl=45 time=2.46 ms #ctrl + c 结束 --- www.a.shifen.com ping statistics --- 14 packets transmitted, 14 received, 0% packet loss, time 13555ms rtt min/avg/max/mdev = 2.420/3.596/13.902/2.889 ms ftp 用来设置文件系统相关功能 ftp服务器在网上较为常见，Linux ftp命令的功能是用命令的方式来控制在本地机和远程机之间传送文件，这里详细介绍Linux ftp命令的一些经常使用的命令，相信掌握了这些使用Linux进行ftp操作将会非常容易。 ftp(选项)(参数) -d：详细显示指令执行过程，便于排错或分析程序执行的情况； -i：关闭互动模式，不询问任何问题； -g：关闭本地主机文件名称支持特殊字符的扩充特性； -n：不使用自动登录； -v：显示指令执行过程。 FTP&gt;ascii: 设定以ASCII方式传送文件(缺省值) FTP&gt;bell: 每完成一次文件传送,报警提示. FTP&gt;binary: 设定以二进制方式传送文件. FTP&gt;bye: 终止主机FTP进程,并退出FTP管理方式. FTP&gt;case: 当为ON时,用MGET命令拷贝的文件名到本地机器中,全部转换为小写字母. FTP&gt;cd: 同UNIX的CD命令. FTP&gt;cdup: 返回上一级目录. FTP&gt;chmod: 改变远端主机的文件权限. FTP&gt;close: 终止远端的FTP进程,返回到FTP命令状态, 所有的宏定义都被删除. FTP&gt;delete: 删除远端主机中的文件. FTP&gt;dir [remote-directory] [local-file] 列出当前远端主机目录中的文件.如果有本地文件,就将结果写至本地文件. FTP&gt;get [remote-file] [local-file] 从远端主机中传送至本地主机中. FTP&gt;help [command] 输出命令的解释. FTP&gt;lcd: 改变当前本地主机的工作目录,如果缺省,就转到当前用户的HOME目录. FTP&gt;ls [remote-directory] [local-file] 同DIR. FTP&gt;macdef: 定义宏命令. FTP&gt;mdelete [remote-files] 删除一批文件. FTP&gt;mget [remote-files] 从远端主机接收一批文件至本地主机. FTP&gt;mkdir directory-name 在远端主机中建立目录. FTP&gt;mput local-files 将本地主机中一批文件传送至远端主机. FTP&gt;open host [port] 重新建立一个新的连接. FTP&gt;prompt: 交互提示模式. FTP&gt;put local-file [remote-file] 将本地一个文件传送至远端主机中. FTP&gt;pwd: 列出当前远端主机目录. FTP&gt;quit: 同BYE. FTP&gt;recv remote-file [local-file] 同GET. FTP&gt;rename [from] [to] 改变远端主机中的文件名. FTP&gt;rmdir directory-name 删除远端主机中的目录. FTP&gt;send local-file [remote-file] 同PUT. FTP&gt;status: 显示当前FTP的状态. FTP&gt;system: 显示远端主机系统类型. FTP&gt;user user-name [password] [account] 重新以别的用户名登录远端主机. FTP&gt;? [command]: 同HELP. [command]指定需要帮助的命令名称。如果没有指定 command，ftp 将显示全部命令的列表。 FTP&gt;! 从 ftp 子系统退出到外壳。 sftp 交互式的文件传输程序 命令的运行和使用方式与ftp命令相似，但是，sftp命令对传输的所有信息使用ssh加密，它还支持公钥认证和压缩等功能 -B：指定传输文件时缓冲区的大小； -l：使用ssh协议版本1； -b：指定批处理文件； -C：使用压缩； -o：指定ssh选项； -F：指定ssh配置文件； -R：指定一次可以容忍多少请求数； -v：升高日志等级。 推荐一款sftp连接工具 FlashFXP iptables Linux上常用的防火墙软件 是netfilter项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。 iptables(选项)(参数) -t&lt;表&gt;：指定要操纵的表； -A：向规则链中添加条目； -D：从规则链中删除条目； -i：向规则链中插入条目； -R：替换规则链中的条目； -L：显示规则链中已有的条目； -F：清楚规则链中已有的条目； -Z：清空规则链中的数据包计算器和字节计数器； -N：创建新的用户自定义规则链； -P：定义规则链中的默认目标； -h：显示帮助信息； -p：指定要匹配的数据包协议类型； -s：指定要匹配的数据包源ip地址； -j&lt;目标&gt;：指定要跳转的目标； -i&lt;网络接口&gt;：指定数据包进入本机的网络接口； -o&lt;网络接口&gt;：指定数据包要离开本机所使用的网络接口。 iptables命令选项输入顺序： iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作 表名包括： raw：高级功能，如：网址过滤。 mangle：数据包修改（QOS），用于实现服务质量。 net：地址转换，用于网关路由器。 filter：包过滤，用于防火墙规则。 规则链名包括： I NPUT链：处理输入数据包。 OUTPUT链：处理输出数据包。 PORWARD链：处理转发数据包。 PREROUTING链：用于目标地址转换（DNAT）。 POSTOUTING链：用于源地址转换（SNAT）。 动作包括： accept：接收数据包。 DROP：丢弃数据包。 REDIRECT：重定向、映射、透明代理。 SNAT：源地址转换。 DNAT：目标地址转换。 MASQUERADE：IP伪装（NAT），用于ADSL。 LOG：日志记录。 清除已有iptables规则 iptables -F iptables -X iptables -Z 开放指定的端口 iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机) iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #允许已建立的或相关连的通行 iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问 iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口 iptables -A INPUT -p tcp --dport 80 -j ACCEPT #允许访问80端口 iptables -A INPUT -p tcp --dport 21 -j ACCEPT #允许ftp服务的21端口 iptables -A INPUT -p tcp --dport 20 -j ACCEPT #允许FTP服务的20端口 iptables -A INPUT -j reject #禁止其他未允许的规则访问 iptables -A FORWARD -j REJECT #禁止其他未允许的规则访问 屏蔽IP iptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽单个IP的命令 iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令 iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令 iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是 查看已添加的 iptables规则 iptables -L -n -v Chain INPUT (policy DROP 48106 packets, 2690K bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 191K 90M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 1499K 133M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 4364K 6351M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 6256 327K ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- * lo 0.0.0.0/0 0.0.0.0/0 删除已添加的iptables规则 将所有iptables以序号标记显示，执行： iptables -L -n --line-numbers 比如要删除INPUT里序号为8的规则，执行： iptables -D INPUT 8 mail 命令行的电子邮件发送和接收工具 操作的界面不像elm或pine那么容易使用，但功能非常完整。 mail(选项)(参数) -b&lt;地址&gt;：指定密件副本的收信人地址； -c&lt;地址&gt;：指定副本的收信人地址； -f&lt;邮件文件&gt;：读取指定邮件文件中的邮件； -i：不显示终端发出的信息； -I：使用互动模式； -n：程序使用时，不使用mail.rc文件中的设置； -N：阅读邮件时，不显示邮件的标题； -s&lt;邮件主题&gt;：指定邮件的主题； -u&lt;用户帐号&gt;：读取指定用户的邮件； -v：执行时，显示详细的信息。 mail -s &quot;Hello from jet-han.oschina.io by shell&quot; admin@oschina.io hello,this is the content of mail. welcome to jet-han.oschina.io 【注】 第一行是输入的命令，-s表示邮件的主题，后面的admin@oschina.io则是邮件的接收人，输入完这行命令后回车，会进入邮件正文的编写，我们可以输入任何文字，比如上面的两行。当邮件正文输入完成后，需要按CTRL+D结束输入，此时会提示你输入Cc地址，即邮件抄送地址，没有直接回车就完成了邮件的发送 使用管道进行邮件发送 echo &quot;hello,this is the content of mail.welcome to jet-han.oschina.io&quot; | mail -s &quot;Hello from jet-han.oschina.io by pipe&quot; admin@oschina.io 【注】 使用管道直接敲入这行命令即可完成邮件的发送，其中echo后的是邮件正文。 使用文件进行邮件发送 mail -s &quot;Hello from jet-han.oschina.io by file&quot; admin@oschina.io &lt; mail.txt 使用上面的命令后，我们就可以把mail.txt文件的内容作为邮件的内容发送给admin@oschina.io了。 使用上述三种方式都可以给外部邮箱进行邮件发送，但因为前面2种都是直接在shell中敲入邮件内容，因此无法输入中文，即使我们使用粘贴的方式输入了中文，那么收到的邮件也是乱码的。但第3种方式，我们可以在window下编辑好邮件内容后，放到linux下，再进行发送，这样就可以正常发送中文了。不过目前邮件的中文标题暂时没有找到解决办法。 因为mail程序本身就是调用sendmail来进行邮件发送的，因此我们可以在mail命令中使用sendmail的参数进行配置，比如我想使用特定的发件人发送邮件，可以使用如下命令： mail -s &quot;Hello from linuxde.net with sender&quot; admin@oschina.io -- -f jet@oschina.io&lt; mail.txt 【注】 上面的命令中，我们使用了– -f jet@oschina.io这样的参数，这是sendmail的选项，其中-f表示邮件的发送人邮件地址 很多情况下，我们也需要使用邮件来发送附件，在linux下使用mail命令发送附件也很简单，不过首先需要安装uuencode软件包，这个程序是对二进制文件进行编码使其适合通过邮件进行发送，在CentOS上安装该软件包如下： yum install sharutils 安装完成后我们就可以来进行附件的发送了，使用如下命令： uuencode test.txt test | mail -s &quot;hello,see the attachement&quot; admin@oschina.io 完成后就可以把text.txt文件作为邮件的附件发送出去了。uuencode有两个参数，第一个是要发送的文件，第二个是显示的文件名称。 这里我主要介绍的是在CentOS下使用mail发送电子邮件的一些使用方法，需要的要求是你的linux必须安装了sendmail并开启了，同时保证可以连接外网。另外，文章中提到的命令本人都经过亲自测试，保证完全可用，不过你需要将命令中的电子邮件地址换成自己的电子邮件地址 如果出现错误[Postfix] – warning: mail_queue_enter: create file maildrop Permission denied [jet@jet oschina_hexo_server]$ lpostdrop: warning: mail_queue_enter: create file maildrop/820792.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/821453.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/821762.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/822488.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/822928.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/823425.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/823907.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/824427.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/824928.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/825368.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/825899.3848: Permission denied postdrop: warning: mail_queue_enter: create file maildrop/826355.3848: Permission denied root@jet:/var/spool/postfix# postfix check postfix/postfix-script: warning: not owned by group postdrop: /var/spool/postfix/public postfix/postfix-script: warning: not owned by group postdrop: /var/spool/postfix/maildrop root@jet:/var/spool/postfix# /etc/init.d/postfix stop root@jet:/var/spool/postfix# killall -9 postdrop root@jet:/var/spool/postfix# chgrp -R postdrop /var/spool/postfix/public root@jet:/var/spool/postfix# chgrp -R postdrop /var/spool/postfix/maildrop/ root@jet:/var/spool/postfix# postfix check root@jet:/var/spool/postfix# postfix start root@jet:/var/spool/postfix# postfix reload chmod g+s /usr/sbin/postqueue chmod g+s /usr/sbin/postdrop root@gandalf:/var/spool/postfix# postfix check #此时没有警告了 nslookup 命令是常用域名查询工具，就是查DNS信息用的命令 nslookup4有两种工作模式，即“交互模式”和“非交互模式”。在“交互模式”下，用户可以向域名服务器查询各类主机、域名的信息，或者输出域名中的主机列表。而在“非交互模式”下，用户可以针对一个主机或域名仅仅获取特定的名称或所需信息。 进入交互模式，直接输入nslookup命令，不加任何参数，则直接进入交互模式，此时nslookup会连接到默认的域名服务器（即/etc/resolv.conf 的第一个dns地址）。或者输入nslookup -nameserver/ip。进入非交互模式，就直接输入nslookup 域名就可以了。 nslookup(选项)(参数) -sil：不显示任何警告信息。 eg: [jet@jet oschina_hexo_server]$ nslookup jet-han.oschina.io Server: 114.114.114.114 Address: 114.114.114.114#53 Non-authoritative answer: Name: jet-han.oschina.io Address: 103.21.119.115 ip 显示或操纵Linux主机的路由、网络设备、策略路由和隧道，是Linux下较新的功能强大的网络配置工具 ip(选项)(参数) #(参数) 网络对象：指定要管理的网络对象； 具体操作：对指定的网络对象完成具体操作； help：显示网络对象支持的操作命令的帮助信息。 #(选项) -V：显示指令版本信息； -s：输出更详细的信息； -f：强制使用指定的协议族； -4：指定使用的网络层协议是IPv4协议； -6：指定使用的网络层协议是IPv6协议； -0：输出信息每条记录输出一行，即使内容较多也不换行显示； -r：显示主机时，不使用IP地址，而使用主机的域名。 用ip命令显示网络设备的运行状态 [jet@jet oschina_hexo_server]$ ip -s link list 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 RX: bytes packets errors dropped overrun mcast 830 14 0 0 0 0 TX: bytes packets errors dropped carrier collsns 830 14 0 0 0 0 2: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:8b:22:5e brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 121209665 198774 0 0 0 0 TX: bytes packets errors dropped carrier collsns 227857521 157867 0 0 0 0 显示核心路由表 [jet@jet oschina_hexo_server]$ ip route list 10.111.24.0/24 dev eth3 proto kernel scope link src 10.111.24.222 metric 1 default via 10.111.24.1 dev eth3 proto static 显示邻居表 [jet@jet oschina_hexo_server]$ ip neigh list fe80::ac9e:6493:cb07:4add dev eth3 lladdr 54:ee:75:03:60:e7 STALE fe80::1870:3ef3:ba4f:e55b dev eth3 lladdr bc:9f:ef:8e:d3:13 STALE fe80::a28d:16ff:fe84:ac43 dev eth3 lladdr a0:8d:16:84:ac:43 STALE fe80::ca6:b92b:4736:a95a dev eth3 lladdr 24:a2:e1:39:ba:21 STALE fe80::83c:b2a6:cc95:fae7 dev eth3 lladdr 54:4e:90:7a:6b:e3 STALE fe80::8486:b8c:5150:15c1 dev eth3 lladdr 28:d2:44:74:a0:d4 STALE fe80::1c2f:e7e6:82ee:17f0 dev eth3 lladdr 20:3c:ae:b0:87:88 STALE fe80::78d2:a70e:cba3:5373 dev eth3 lladdr 28:d2:44:68:36:3c STALE fe80::9c19:a5e5:918b:b069 dev eth3 lladdr 50:7b:9d:e6:7c:ac STALE fe80::1c12:cb63:3897:fcb0 dev eth3 lladdr fc:d8:48:92:44:c9 STALE fe80::184c:1bc1:a201:2fa7 dev eth3 lladdr 90:b0:ed:77:94:4a STALE fe80::792a:9e54:8679:cb84 dev eth3 lladdr b8:ee:65:04:81:89 STALE fe80::1098:1ed2:b23a:4ef dev eth3 lladdr 5c:ad:cf:6e:11:6a STALE fe80::28bc:6b21:4fce:f51a dev eth3 lladdr 28:d2:44:c9:c0:ed STALE fe80::1895:8cec:208d:5eec dev eth3 lladdr f4:5c:89:8e:ad:a5 STALE fe80::1c44:bede:96a5:499a dev eth3 lladdr 20:ab:37:93:1f:43 STALE fe80::5a:d7d:e3f6:6f9f dev eth3 lladdr 14:2d:27:f8:4b:79 STALE fe80::61e9:b002:68c1:7ba dev eth3 lladdr 28:d2:44:6e:11:d7 STALE fe80::1cbe:22d4:b466:d564 dev eth3 lladdr 40:33:1a:ad:1b:8c STALE fe80::1863:6370:356f:c6ec dev eth3 lladdr 24:24:0e:de:6c:86 STALE fe80::426c:8fff:fe3f:304e dev eth3 lladdr 40:6c:8f:3f:30:4e STALE fe80::1824:b631:d44a:5bf3 dev eth3 lladdr 9c:fc:01:e7:c8:21 STALE fe80::1038:72ee:9f9e:51ea dev eth3 lladdr 70:48:0f:60:e7:0d STALE fe80::a069:d8a9:5fc0:219d dev eth3 lladdr ac:22:0b:c9:bb:6c STALE fe80::8e4:27f0:1ac7:97cc dev eth3 lladdr 28:d2:44:6d:fb:6f STALE fe80::59a0:8d02:8cb7:430c dev eth3 lladdr 28:d2:44:68:74:80 STALE fe80::3884:7711:39ce:2b96 dev eth3 lladdr 48:5a:b6:df:8f:8d STALE fe80::f8c1:52f5:5286:9bae dev eth3 lladdr 28:d2:44:75:5d:ec STALE fe80::c9c:69:70b5:2796 dev eth3 lladdr 2c:20:0b:bf:2d:16 STALE fe80::ee01:eeff:fe0a:fe55 dev eth3 lladdr ec:01:ee:0a:fe:55 STALE fe80::3ea3:48ff:fe99:5bf6 dev eth3 lladdr 3c:a3:48:99:5b:f6 STALE fe80::1092:6d00:3bfa:ad5f dev eth3 lladdr 7c:04:d0:32:44:24 STALE fe80::c3e:8ab7:92f8:d0f6 dev eth3 lladdr 64:b0:a6:26:a5:f0 STALE fe80::ca6:3dd9:1fe:f6a0 dev eth3 lladdr 24:24:0e:be:1a:57 STALE fe80::d265:caff:fece:62c9 dev eth3 lladdr d0:65:ca:ce:62:c9 STALE fe80::163e:bfff:fefc:ac78 dev eth3 lladdr 14:3e:bf:fc:ac:78 STALE fe80::9ef3:87ff:fec0:c6fa dev eth3 lladdr 9c:f3:87:c0:c6:fa STALE fe80::1415:65e3:71b0:4c9e dev eth3 lladdr d8:1d:72:52:6f:84 STALE 10.111.24.248 dev eth3 lladdr bc:75:74:5e:fb:2e STALE 10.111.24.220 dev eth3 lladdr 14:3e:bf:fc:ac:78 STALE 10.111.24.1 dev eth3 lladdr 10:47:80:28:02:e0 STALE 10.111.24.227 dev eth3 lladdr f4:8e:38:ba:47:3f REACHABLE scp 远程拷贝文件的命令 和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 scp(选项)(参数) #(选项) -1：使用ssh协议版本1； -2：使用ssh协议版本2； -4：使用ipv4； -6：使用ipv6； -B：以批处理模式运行； -C：使用压缩； -F：指定ssh配置文件； -l：指定宽带限制； -o：指定使用的ssh选项； -P：指定远程主机的端口号； -p：保留文件的最后修改时间，最后访问时间和权限模式； -q：不显示复制进度； -r：以递归方式复制； -v 详细显示输出的具体情况。 #(参数) 源文件：指定要复制的源文件。 目标文件：目标文件。格式为user@host：filename（文件名为目标文件的名称）。 (1) 复制文件： 命令格式： scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file 或者 scp local_file remote_ip:remote_folder 或者 scp local_file remote_ip:remote_file 第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名 第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名 (2) 复制目录： 命令格式： scp -r local_folder remote_username@remote_ip:remote_folder 或者 scp -r local_folder remote_ip:remote_folder 第1个指定了用户名，命令执行后需要输入用户密码； 第2个没有指定用户名，命令执行后需要输入用户名和密码； eg: #从 本地 上传到 远程 scp /home/daisy/full.tar.gz root@172.19.2.75:/home/root #从 远程 下载到 本地 scp root@172.19.2.75:/home/root/full.tar.gz /home/daisy/ wget 直接从网络上下载文件 wget [参数] [URL地址] -o FILE 把记录写到FILE文件中 eg : wget -O a.txt URL wget --limit-rate=300k URL 限速下载 系统安全相关命令vmstat 对操作系统的虚拟内存、进程、CPU活动进行监控 iostat 对系统的磁盘操作活动进行监视,汇报磁盘活动统计情况，同时也会汇报出CPU使用情况 -p[磁盘] 显示磁盘和分区的情况 watch 重复执行某一命令以观察变化 watch [参数] [命令] -n 时隔多少秒刷新 -d 高亮显示动态变化 at 在一个指定的时间执行一个指定任务，只能执行一次 at [参数] [时间] HH:MM[am|pm] + number [minutes|hours|days|weeks] 强制在某年某月某日的某时刻进行该项任务 atq 查看系统未执行的任务 atrm n 删除编号为n的任务 at -c n 显示编号为n的任务的内容 passwd 用于设置用户的认证信息，包括用户密码、密码过期时间等 系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 passwd(选项)(参数) #(选项) -l 使密码失效 -u 与-l相对，用户解锁 -S 列出登陆用户passwd文件内的相关参数 -n 后面接天数，shadow 的第 4 字段，多久不可修改密码天数 -x 后面接天数，shadow 的第 5 字段，多久内必须要更动密码 -w 后面接天数，shadow 的第 6 字段，密码过期前的警告天数 -i 后面接『日期』，shadow 的第 7 字段，密码失效日期 使用管道刘设置密码：echo &quot;zeng&quot; | passwd --stdin zenghao #(参数) 用户名：需要设置密码的用户名。 与用户、组账户信息相关的文件 存放用户信息： /etc/passwd /etc/shadow 存放组信息： /etc/group /etc/gshadow 用户信息文件分析（每项用:隔开） 例如：jack:X:503:504:::/home/jack/:/bin/bash jack //用户名 X //口令、密码 503 //用户 （0代表root、普通新建用户从500开始） 504 //所在组 : //描述 /home/jack/ //用户主目录 /bin/bash //用户缺省Shell 组信息文件分析 例如：jack:$!$:???:13801:0:99999:7:*:*: jack //组名 $!$ //被加密的口令 13801 //创建日期与今天相隔的天数 0 //口令最短位数 99999 //用户口令 7 //到7天时提醒 * //禁用天数 * //过期天数 如果是普通用户执行passwd只能修改自己的密码。如果新建用户后，要为新用户创建密码，则用passwd用户名，注意要以root用户的权限来创建 [root@jet ~]# passwd linuxde //更改或创建linuxde用户的密码； Changing password for user linuxde. New UNIX password: //请输入新密码； Retype new UNIX password: //再输入一次； passwd: all authentication tokens updated successfully. //成功； 普通用户如果想更改自己的密码，直接运行passwd即可，比如当前操作的用户是jet [jet@jet ~]$ passwd Changing password for user linuxde. //更改jet用户的密码； (current) UNIX password: //请输入当前密码； New UNIX password: //请输入新密码； Retype new UNIX password: //确认新密码； passwd: all authentication tokens updated successfully. //更改成功； 比如我们让某个用户不能修改密码，可以用-l选项来锁定： [root@localhost ~]# passwd -l linuxde //锁定用户jet不能更改密码； Locking password for user linuxde. passwd: Success //锁定成功； [jet@jet ~]# su linuxde //通过su切换到jet用户； [jet@jet ~]$ passwd //jet来更改密码； Changing password for user jet. Changing password for linuxde (current) UNIX password: //输入jet的当前密码； passwd: Authentication token manipulation error //失败，不能更改密码； 清除密码 [root@jet ~]# passwd -d jet //清除jet用户密码； Removing password for user jet. passwd: Success //清除成功； [root@jet ~]# passwd -S jet //查询jet用户密码状态； Empty password. //空密码，也就是没有密码； 【注】 当我们清除一个用户的密码时，登录时就无需密码，这一点要加以注意。 su 切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码 su [参数] user -c&lt;指令&gt;或--command=&lt;指令&gt;：执行完指定的指令后，即恢复原来的身份； -f或——fast：适用于csh与tsch，使shell不用去读取启动文件； -l或——login：改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,logname。此外，也会变更PATH变量； -m,-p或--preserve-environment：变更身份时，不要变更环境变量； -s或--shell=：指定要执行的shell； --help：显示帮助； --version；显示版本信息。 eg: #变更帐号为root并在执行ls指令后退出变回原使用者： su -c ls root #变更帐号为root并传入-f选项给新执行的shell： su root -f #变更帐号为test并改变工作目录至test的家目录： su -test sudo 以其他身份来执行命令，预设的身份为root 在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。 sudo(选项)(参数) -b：在后台执行指令； -h：显示帮助； -H：将HOME环境变量设为新身份的HOME环境变量； -k：结束密码的有效期限，也就是下次再执行sudo时便需要输入密码；。 -l：列出目前用户可执行与无法执行的指令； -p：改变询问密码的提示符号； -s：执行指定的shell； -u&lt;用户&gt;：以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份； -v：延长密码有效期限5分钟； -V ：显示版本信息。 配置sudo必须通过编辑/etc/sudoers文件，而且只有超级用户才可以修改它，还必须使用visudo编辑。之所以使用visudo有两个原因，一是它能够防止两个用户同时修改它；二是它也能进行有限的语法检查。所以，即使只有你一个超级用户，你也最好用visudo来检查一下语法。 visudo默认的是在vi里打开配置文件，用vi来修改文件。我们可以在编译时修改这个默认项。visudo不会擅自保存带有语法错误的配置文件，它会提示你出现的问题，并询问该如何处理，就像： &gt;&gt;&gt; sudoers file: syntax error, line 22 &lt;&lt; 此时我们有三种选择：键入“e”是重新编辑，键入“x”是不保存退出，键入“Q”是退出并保存。如果真选择Q，那么sudo将不会再运行，直到错误被纠正。 现在，我们一起来看一下神秘的配置文件，学一下如何编写它。让我们从一个简单的例子开始：让用户Foobar可以通过sudo执行所有root可执行的命令。以root身份用visudo打开配置文件，可以看到类似下面几行： # Runas alias specification # User privilege specificationroot ALL=(ALL)ALL 我们一看就明白个差不多了，root有所有权限，只要仿照现有root的例子就行，我们在下面加一行（最好用tab作为空白）： foobar ALL=(ALL) ALL 保存退出后，切换到foobar用户，我们用它的身份执行命令： [foobar@localhost ~]$ ls /root ls: /root: 权限不够 [foobar@localhost ~]$ sudo ls /root PassWord: anaconda-ks.cfg Desktop install.log install.log.syslog 好了，我们限制一下foobar的权利，不让他为所欲为。比如我们只想让他像root那样使用ls和ifconfig，把那一行改为： foobar localhost= /sbin/ifconfig, /bin/ls 再来执行命令： [foobar@localhost ~]$ sudo head -5 /etc/shadow Password: Sorry, user foobar is not allowed to execute &#39;/usr/bin/head -5 /etc/shadow&#39; as root on localhost.localdomain. [foobar@localhost ~]$ sudo /sbin/ifconfigeth0 Linkencap:Ethernet HWaddr 00:14:85:EC:E9:9B... 现在让我们来看一下那三个ALL到底是什么意思。第一个ALL是指网络中的主机，我们后面把它改成了主机名，它指明foobar可以在此主机上执行后面的命令。第二个括号里的ALL是指目标用户，也就是以谁的身份去执行命令。最后一个ALL当然就是指命令名了。例如，我们想让foobar用户在linux主机上以jimmy或rene的身份执行kill命令，这样编写配置文件： foobar linux=(jimmy,rene) /bin/kill 但这还有个问题，foobar到底以jimmy还是rene的身份执行？这时我们应该想到了sudo -u了，它正是用在这种时候。 foobar可以使用sudo -u jimmy kill PID或者sudo -u rene kill PID，但这样挺麻烦，其实我们可以不必每次加-u，把rene或jimmy设为默认的目标用户即可。再在上面加一行： Defaults:foobar runas_default=rene Defaults后面如果有冒号，是对后面用户的默认，如果没有，则是对所有用户的默认。就像配置文件中自带的一行： Defaults env_reset 另一个问题是，很多时候，我们本来就登录了，每次使用sudo还要输入密码就显得烦琐了。我们可不可以不再输入密码呢？当然可以，我们这样修改配置文件： foobar localhost=NOPASSWD: /bin/cat, /bin/ls 再来sudo一下： [foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg Desktop install.log install.log.syslog 当然，你也可以说“某些命令用户foobar不可以运行”，通过使用!操作符，但这不是一个好主意。因为，用!操作符来从ALL中“剔出”一些命令一般是没什么效果的，一个用户完全可以把那个命令拷贝到别的地方，换一个名字后再来运行。 日志与安全 sudo为安全考虑得很周到，不仅可以记录日志，还能在有必要时向系统管理员报告。但是，sudo的日志功能不是自动的，必须由管理员开启。这样来做： touch /var/log/sudo vi /etc/syslog.conf 在syslog.conf最后面加一行（必须用tab分割开）并保存： local2.debug /var/log/sudo 重启日志守候进程， ps aux grep syslogd 把得到的syslogd进程的PID（输出的第二列是PID）填入下面： kill –HUP PID 这样，sudo就可以写日志了： [foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg Desktop install.log install.log.syslog $cat /var/log/sudoJul 28 22:52:54 localhost sudo: foobar : TTY=pts/1 ; pwd=/home/foobar ; USER=root ; command=/bin/ls /root 不过，有一个小小的“缺陷”，sudo记录日志并不是很忠实： [foobar@localhost ~]$ sudo cat /etc/shadow &gt; /dev/null cat /var/log/sudo...Jul 28 23:10:24 localhost sudo: foobar : TTY=pts/1 ; PWD=/home/foobar ; USER=root ; COMMAND=/bin/cat /etc/shadow 重定向没有被记录在案！为什么？因为在命令运行之前，shell把重定向的工作做完了，sudo根本就没看到重定向。这也有个好处，下面的手段不会得逞： [foobar@localhost ~]$ sudo ls /root &gt; /etc/shadowbash: /etc/shadow: 权限不够 sudo 有自己的方式来保护安全。以root的身份执行sudo-V，查看一下sudo的设置。因为考虑到安全问题，一部分环境变量并没有传递给sudo后面的命令，或者被检查后再传递的，比如：PATH，HOME，SHELL等。当然，你也可以通过sudoers来配置这些环境变量。 chgrp 改变文件或目录所属的用户组 该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组 chgrp(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quiet或——silent：不显示错误信息； -h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件； -R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理； -v或——verbose：显示指令执行过程； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； #(参数) 组：指定新工作名称； 文件：指定要改变所属组的文件列表。多个文件或者目录之间使用空格隔开。 eg： chgrp users -R ./dir #递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users chown 改变某个文件或目录的所有者和所属的组 该命令可以向某个用户授权，使该用户变成指定文件的所有者或者改变文件所属的组。用户可以是用户或者是用户D，用户组可以是组名或组id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。 chown(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quite或——silent：不显示错误信息； -h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件； -R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -v或——version：显示指令执行过程； --dereference：效果和“-h”参数相同； --help：在线帮助； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同； --version：显示版本信息。 #(参数) 用户：组：指定所有者和所属工作组。当省略“：组”，仅改变文件所有者； 文件：指定要改变所有者和工作组的文件列表。支持多个文件和目标，支持shell通配符。 #将目录/usr/meng及其下面的所有文件、子目录的文件主改成 liu： chown -R liu /usr/meng #文件的属主和属组属性设置 chown user:market f01 //把文件f01给uesr，添加到market组 ll -d f1 查看目录f1的属性 chmod 变更文件或目录的权限 在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 权限范围的表示法如下：u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户 ；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”； - 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 chmod(选项)(参数) #(选项) -c或——changes：效果类似“-v”参数，但仅回报更改的部分； -f或--quiet或——silent：不显示错误信息； -R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理； -v或——verbose：显示指令执行过程； --reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； &lt;权限范围&gt;+&lt;权限设置&gt;：开启权限范围的文件或目录的该选项权限设置； &lt;权限范围&gt;-&lt;权限设置&gt;：关闭权限范围的文件或目录的该选项权限设置； &lt;权限范围&gt;=&lt;权限设置&gt;：指定权限范围的文件或目录的该选项权限设置； #(参数) 权限模式：指定文件的权限模式； 文件：要改变权限的文件。 eg： chmod 0755 file # 把file的文件权限改变为-rxwr-xr-x chmod g+w file # 向file的文件权限中加入用户组可写权限 Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ linux文件的用户权限的分析图 文件权限管理 三种基本权限 R 读 数值表示为4 W 写 数值表示为2 X 可执行 数值表示为1 如图所示，copyright.html文件的权限为-rw-rw-r— -rw-rw-r-- 一共十个字符，分成四段。 第一个字符“-”表示普通文件；这个位置还可能会出现“l”链接；“d”表示目录 第二三四个字符“rw-”表示当前所属用户的权限。 所以用数值表示为4+2=6 第五六七个字符“rw-”表示当前所属组的权限。 所以用数值表示为4+2=6 第八九十个字符“r-–”表示其他用户权限。 所以用数值表示为4 所以操作此文件的权限用数值表示为664 用户及用户组管理 /etc/passwd 存储用户账号 /etc/group 存储组账号 /etc/shadow 存储用户账号的密码 /etc/gshadow 存储用户组账号的密码 useradd 添加用户名 userdel 删除用户名 adduser 添加用户名 groupadd 添加组名 groupdel 删除组名 passwd root 给root设置密码 su root su – root /etc/profile 系统环境变量 bash_profile 用户环境变量 .bashrc 用户环境变量 su user 切换用户，加载配置文件.bashrc su – user 切换用户，加载配置文件/etc/profile ，加载bash_profile who 显示目前登录系统的用户信息 执行who命令可得知目前有那些用户登入系统，单独执行who命令会列出登入帐号，使用的终端机，登入时间以及从何处登入或正在使用哪个X显示器。 who(选项)(参数) #(选项) -H或--heading：显示各栏位的标题信息列； -i或-u或--idle：显示闲置时间，若该用户在前一分钟之内有进行任何动作，将标示成&quot;.&quot;号，如果该用户已超过24小时没有任何动作，则标示出&quot;old&quot;字符串； -m：此参数的效果和指定&quot;am i&quot;字符串相同； -q或--count：只显示登入系统的帐号名称和总人数； -s：此参数将忽略不予处理，仅负责解决who指令其他版本的兼容性问题； -w或-T或--mesg或--message或--writable：显示用户的信息状态栏； --help：在线帮助； --version：显示版本信息。 #(参数) 文件：指定查询文件。 whoami 打印当前有效的用户名称，相当于执行id -un命令 whoami(选项) (选项) --help：在线帮助； --version：显示版本信息。 which 查找并显示给定命令的绝对路径 环境变量PATH中保存了查找命令时需要遍历的目录。which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 which(选项)(参数) #(选项) -n&lt;文件名长度&gt;：制定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名； -p&lt;文件名长度&gt;：与-n参数相同，但此处的&lt;文件名长度&gt;包含了文件的路径； -w：指定输出时栏位的宽度； -V：显示版本信息。 #(参数) 指令名：指令名列表。 查找文件、显示命令路径： [root@jet ~]# which pwd /bin/pwd [root@jet ~]# which adduser /usr/sbin/adduser 【注】 which是根据使用者所配置的 PATH 变量内的目录去搜寻可运行档的！所以，不同的 PATH 配置内容所找到的命令当然不一样的！ ntpdate设置本地日期和时间 服务器的时间不对的时候，可以使用ntpdate工具来校正时间。 ntpdate ip/site eg: /usr/sbin/ntpdate time.windows.com 以下是一些可用的NTP服务器地址： Name IP Location 210.72.145.44 210.72.145.44 中国（国家授时中心） 133.100.11.8 133.100.11.8 日本（福冈大学） time-a.nist.gov 129.6.15.28 NIST,Gaithersburg,Maryland time-b.nist.gov 129.6.15.29 NIST,Gaithersburg,Maryland time-a.timefreq.bldrdoc.gov 132.163.4.101 NIST,Boulder,Colorado time-b.timefreq.bldrdoc.gov 132.163.4.102 NIST,Boulder,Colorado time-c.timefreq.bldrdoc.gov 132.163.4.103 NIST,Boulder,Colorado utcnist.colorado.edu 128.138.140.44 UniversityofColorado,Boulder time.nist.gov 192.43.244.18 NCAR,Boulder,Colorado time-nw.nist.gov 131.107.1.10 Microsoft,Redmond,Washington nist1.symmetricom.com 69.25.96.13 Symmetricom,SanJose,California nist1-dc.glassey.com 216.200.93.8 Abovenet,Virginia nist1-ny.glassey.com 208.184.49.9 Abovenet,NewYorkCity nist1-sj.glassey.com 207.126.98.204 Abovenet,SanJose,California nist1.aol-ca.truetime.com 207.200.81.113 TrueTime,AOLfacility,Sunnyvale,California nist1.aol-va.truetime.com 64.236.96.53 TrueTime,AOLfacility,Virginia 其它命令gzip 压缩文件 gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，文件经它压缩过后，其名称后面会多处“.gz”扩展名，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。 gzip(选项)(参数) #(选项) -a或——ascii：使用ASCII文字模式； -d或--decompress或----uncompress：解开压缩文件； -f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接； -h或——help：在线帮助； -l或——list：列出压缩文件的相关信息； -L或——license：显示版本与版权信息； -n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记； -N或——name：压缩文件时，保存原来的文件名称及时间戳记； -q或——quiet：不显示警告信息； -r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串； -t或——test：测试压缩文件是否正确无误； -v或——verbose：显示指令执行过程； -V或——version：显示版本信息； -&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高； --best：此参数的效果和指定“-9”参数相同； --fast：此参数的效果和指定“-1”参数相同。 #(参数) 文件列表：指定要压缩的文件列表。 把test6目录下的每个文件压缩成.gz文件 gzip * 把上例中每个压缩的文件解压，并列出详细的信息 gzip -dv * 详细显示例1中每个压缩的文件的信息，并不解压 gzip -l * 压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz gzip -r log.tar 递归的压缩目录 gzip -rv test6 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。 递归地解压目录 gzip -dr test6 gunzip 解压缩文件 gunzip是个使用广泛的解压缩程序，它用于解开被gzip压缩过的文件，这些压缩文件预设最后的扩展名为.gz。事实上gunzip就是gzip的硬连接，因此不论是压缩或解压缩，都可通过gzip指令单独完成。 gunzip(选项)(参数) #(选项) -a或——ascii：使用ASCII文字模式； -c或--stdout或--to-stdout：把解压后的文件输出到标准输出设备； -f或-force：强行解开压缩文件，不理会文件名称或硬连接是否存在以及该文件是否为符号连接； -h或——help：在线帮助； -l或——list：列出压缩文件的相关信息； -L或——license：显示版本与版权信息； -n或--no-name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其忽略不予处理； -N或——name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其回存到解开的文件上； -q或——quiet：不显示警告信息； -r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理； -S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串； -t或——test：测试压缩文件是否正确无误； -v或——verbose：显示指令执行过程； -V或——version：显示版本信息； #(参数) 文件列表：指定要解压缩的压缩包。 首先将/etc目录下的所有文件以及子目录进行压缩，备份压缩包etc.zip到/opt目录，然后对etc.zip文件进行gzip压缩，设置gzip的压缩级别为9。 zip –r /opt/etc.zip /etc gzip -9v /opt/etc.zip 查看上述etc.zip.gz文件的压缩信息。 gzip -l /opt/etc.zip.gz compressed uncompressed ratio uncompressed_name 11938745 12767265 6.5% /opt/etc.zip 解压上述etc.zip.gz文件到当前目录。 [root@mylinux ~]#gzip –d /opt/etc.zip.gz 或者执行 [root@mylinux ~]#gunzip /opt/etc.zip.gz 通过上面的示例可以知道gzip –d等价于gunzip命令。 bzip2 创建和管理（包括解压缩）“.bz2”格式的压缩包 我们遇见Linux压缩打包方法有很多种，以下讲解了Linux压缩打包方法中的Linux bzip2命令的多种范例供大家查看，相信大家看完后会有很多收获。 bzip2(选项)(参数) #(选项) -c或——stdout：将压缩与解压缩的结果送到标准输出； -d或——decompress：执行解压缩； -f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数； -h或——help：在线帮助； -k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数； -s或——small：降低程序执行时内存的使用量； -t或——test：测试.bz2压缩文件的完整性； -v或——verbose：压缩或解压缩文件时，显示详细的信息； -z或——compress：强制执行压缩； -V或——version：显示版本信息； --repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果； --repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。 #(参数) 文件：指定要压缩的文件。 压缩指定文件filename: bzip2 filename 或 bzip2 -z filename 这里，压缩的时候不会输出，会将原来的文件filename给删除，替换成filename.bz2.如果以前有filename.bz2则不会替换并提示错误（如果想要替换则指定-f选项，例如bzip2 -f filename；如果filename是目录则也提醒错误不做任何操作；如果filename已经是压过的了有bz2后缀就提醒一下，不再压缩，没有bz2后缀会再次压缩。 解压指定的文件filename.bz2: bzip2 -d filename.bz2 或 bunzip2 filename.bz2 这里，解压的时候没标准输出，会将原来的文件filename.bz2给替换成filename。如果以前有filename则不会替换并提示错误（如果想要替换则指定-f选项，例如bzip2 -df filename.bz2。 压缩解压的时候将结果也输出： $bzip2 -v filename 输入之后，输出如下： filename: 0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out. 这里，加上-v选项就会输出了,只用压缩举例了，解压的时候同理bzip2 -dv filename.bz2不再举例了。 模拟解压实际并不解压： bzip2 -tv filename.bz2 输入之后，输出如下： filename.bz2: ok 这里，-t指定要进行模拟解压，不实际生成结果，也就是说类似检查文件,当然就算目录下面有filename也不会有什么错误输出了，因为它根本不会真的解压文件。为了在屏幕上输出，这里加上-v选项了,如果是真的解压bzip2 -dv filename.bz2则输出的是把”ok”替换成了”done”。 压缩解压的时候，除了生成结果文件，将原来的文件也保存: bzip2 -k filename 这里，加上-k就保存原始的文件了，否则原始文件会被结果文件替代。只用压缩举例了，解压的时候同理$bzip2 -dk filename.bz2不再举例了。 解压到标准输出： bzip2 -dc filename.bz2 输入之后，输出如下： hahahhaahahha 这里，使用-c指定到标准输出，输出的是文件filename的内容，不会将filename.bz2删除。 压缩到标准输出： bzip2 -c filename bzip2: I won&#39;t write compressed data to a terminal. bzip2: For help, type: `bzip2 --help&#39;. 这里，使用-c指定压缩到标准输出不删除原有文件，不同的是，压缩后的文件无法输出到标准输出。 使用bzip2的时候将所有后面的看作文件(即使文件名以’-‘开头)： bzip2 -- -myfilename 这里主要是为了防止文件名中-产生以为是选项的歧义。 bzcat 读取数据而无需解压 解压缩指定的.bz2文件，并显示解压缩后的文件内容。保留原压缩文件，并且不生成解压缩后的文件 bzcat(参数) #(参数) .bz2压缩文件：指定要显示内容的.bz2压缩文件。 将/tmp/man.config以bzip2格式压缩： bzip2 -z man.config 此时man.config会变成man.config.bz2 将上面的压缩文件内容读出来： bzcat man.config.bz2 此时屏幕上会显示 man.config.bz2 解压缩之后的文件内容。 tar 为linux的文件和目录创建档案 利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 tar(选项)(参数) #(选项) -A或--catenate：新增文件到以存在的备份文件； -B：设置区块大小； -c或--create：建立新的备份文件； -C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 -d：记录文件的差别； -x或--extract或--get：从备份文件中还原文件； -t或--list：列出备份文件的内容； -z或--gzip或--ungzip：通过gzip指令处理备份文件； -Z或--compress或--uncompress：通过compress指令处理备份文件； -f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件； -v或--verbose：显示指令执行过程； -r：添加文件到已经压缩的文件； -u：添加改变了和现有的文件到已经存在的压缩文件； -j：支持bzip2解压文件； -v：显示操作过程； -l：文件系统边界设置； -k：保留原有文件不覆盖； -m：保留文件不被覆盖； -w：确认压缩文件的正确性； -p或--same-permissions：用原来的文件权限还原文件； -P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号； -N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里； --exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 #(参数) 文件或目录：指定要打包的文件或目录列表。 将文件全部打包成tar包： tar -jcvf filename.tar.bz2 要被压缩的档案或目录名称 #压 缩 tar -jtvf filename.tar.bz2 #查 询 tar -jxvf filename.tar.bz2 -C 欲解压缩的目录 #解压缩 tar -cvf log.tar log2012.log #仅打包，不压缩！ tar -zcvf log.tar.gz log2012.log #打包后，以 gzip 压缩 tar -jcvf log.tar.bz2 log2012.log #打包后，以 bzip2 压缩 在选项f之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加z选项，则以.tar.gz或.tgz来代表gzip压缩过的tar包；如果加j选项，则以.tar.bz2来作为tar包名。 查阅上述tar包内有哪些文件： tar -ztvf log.tar.gz 由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上z这个选项了。 将tar包解压缩： tar -zxvf /opt/soft/test/log.tar.gz 在预设的情况下，我们可以将压缩档在任何地方解开的 只将tar内的部分文件解压出来： tar -zxvf /opt/soft/test/log30.tar.gz log2013.log 我可以透过tar -ztvf来查阅 tar 包内的文件名称，如果单只要一个文件，就可以透过这个方式来解压部分文件！ 文件备份下来，并且保存其权限： tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log 这个-p的属性是很重要的，尤其是当您要保留原本文件的属性时。 在文件夹当中，比某个日期新的文件才备份： tar -N “2012/11/13” -zcvf log17.tar.gz test 备份文件夹内容是排除部分文件： tar —exclude scf/service -zcvf scf.tar.gz scf/* 其实最简单的使用 tar 就只要记忆底下的方式即可： 压 缩：tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查 询：tar -jtv -f filename.tar.bz2 解压缩：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 users 显示当前登录系统地用户 who 登录在本机的用户与来源 -H或--heading 显示各栏位的标题信息列。 w 登录在本机的用户及其运行的程序 -s 使用简洁格式列表，不显示用户登入时间，终端机阶段作业和程序所耗费的CPU时间。 -h 不显示各栏位的标题信息列。 write 给当前联机的用户发消息 wall 给所有登录再本机的用户发消息 last 查看用户的登陆日志 lastlog 查看每个用户最后的登陆时间 finger 查看用户信息 -s 显示用户的注册名、实际姓名、终端名称、写状态、停滞时间、登录时间等信息 -l 除了用-s选项显示的信息外，还显示用户主目录、登录shell、邮件状态等信息，以及用户主目录下的.plan、.project和.forward文件的内容。 -p 除了不显示.plan文件和.project文件以外，与-l选项相同 hostname 查看主机名 alias 添加别名 unalias 清除别名 chage **修改用户密码的相关属性 -l 列出该账号的详细密码参数； -d 后面接日期，修改 shadow 第三字段(最近一次更改密码的日期)，格式YYYY-MM-DD -E 后面接日期，修改 shadow 第八字段(账号失效日)，格式 YYYY-MM-DD -I 后面接天数，修改 shadow 第七字段(密码失效日期) -m 后面接天数，修改 shadow 第四字段(密码最短保留天数) -M 后面接天数，修改 shadow 第五字段(密码多久需要进行变更) -W 后面接天数，修改 shadow 第六字段(密码过期前警告日期) usermod 修改用户的相关属性 -c 后面接账号的说明，即 /etc/passwd 第五栏的说明栏，可以加入一些账号的说明。 -d 后面接账号的家目录，即修改 /etc/passwd 的第六栏； -e 后面接日期，格式是 YYYY-MM-DD 也就是在 /etc/shadow 内的第八个字段数据啦！ -f 后面接天数为 shadow 的第七字段。 -g 后面接初始群组，修改 /etc/passwd 的第四个字段，亦即是GID的字段！ -G 后面接次要群组，修改这个使用者能够支持的群组 -l 后面接账号名称。亦即是修改账号名称， /etc/passwd 的第一栏！ -s 后面接 Shell 的实际档案，例如 /bin/bash 或 /bin/csh 等等。 -u 后面接 UID 数字啦！即 /etc/passwd 第三栏的资料； -L 冻结密码 -U 解冻密码 id 查看用户相关的id信息，还可以用来判断用户是否存在 groups 查看登陆用户支持的群组， 第一个输出的群组为有效群组 newgrp 切换有效群组 groupmod 修改组信息 -g 修改既有的 GID 数字 -n 修改既有的组名 groupdel 删除群组 gpasswd 群组管理员功能 root管理员动作： -gpasswd groupname 设定密码 -gpasswd [-A user1,...] [-M user3,...] groupname -A 将 groupname 的主控权交由后面的使用者管理(该群组的管理员) -M 将某些账号加入这个群组当中 -gpasswd [-r] groupname -r 将 groupname 的密码移除 群组管理员动作： - gpasswd [-ad] user groupname -a 将某位使用者加入到 groupname 这个群组当中 -d 将某位使用者移除出 groupname 这个群组当中 chfn 修改个人信息 cut Print selected parts of lines from each FILE to standard output -b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。 -c ：以字符为单位进行分割。 -d ：自定义分隔符，默认为制表符。 -f ：与-d一起使用，指定显示哪个区域。 sort sort -n 依照数值的大小排序。 -o&lt;输出文件&gt; 将排序后的结果存入指定的文件。 -r 以相反的顺序来排序。 -t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。 -k 选择以哪个区间进行排序。 set 显示环境变量和普通变量 env 显示环境变量 export 把普通变量变成环境变量 unset 删除一个环境变量 aaa(){} 定义函数 read read -p 接提示字符 -t 接等待的秒数 declare/typeset declare、typeset -i 声明为整数 -a 声明为数组 -f 声明为函数 -r 声明为只读 ulimit 限制使用者的某些系统资源 -f 此 shell 可以建立的最大档案容量 (一般可能设定为 2GB)单位为 Kbytes eg: ulimit -f 1024 限制使用者仅能建立 1MBytes 以下的容量的档案 date 显示或设定系统的日期与时间 date [参数]… [+格式] %H 小时(以00-23来表示)。 %M 分钟(以00-59来表示)。 %P AM或PM。 %D 日期(含年月日) %U 该年中的周数。 date -s “2015-10-17 01:01:01″ //时间设定 date +%Y%m%d //显示前天年月日 date +%Y%m%d --date=&quot;+1 day/month/year&quot; //显示前一天/月/年的日期 date +%Y%m%d --date=&quot;-1 day/month/year&quot; //显示后一天/月/年的日期 date -d &#39;2 weeks&#39; 2周后的日期 cal 查看日历 -1 显示当月的月历 -3 显示前、当、后一个月的日历 -m 显示星期一为一个星期的第一天 -s （默认）星期天为第一天 -j 显示当月是一年中的第几天的日历 -y 显示当前年份的日历 gcc 命令 对于一个用Linux开发C程序的人来说，这个命令就非常重要了，它用于把C语言的源程序文件，编译成可执行程序，由于g++的很多参数跟它非常相似，所以这里只介绍gcc的参数，它的常用参数如下： [plain] view plain copy print? -o ：output之意，用于指定生成一个可执行文件的文件名 -c ：用于把源文件生成目标文件（.o)，并阻止编译器创建一个完整的程序 -I ：增加编译时搜索头文件的路径 -L ：增加编译时搜索静态连接库的路径 -S ：把源文件生成汇编代码文件 -lm：表示标准库的目录中名为libm.a的函数库 -lpthread ：连接NPTL实现的线程库 -std= ：用于指定把使用的C语言的版本 # 例如： # 把源文件test.c按照c99标准编译成可执行程序test gcc -o test test.c -lm -std=c99 #把源文件test.c转换为相应的汇编程序源文件test.s gcc -S test.c time 测算一个命令（即程序）的执行时间 它的使用非常简单，就像平时输入命令一样，不过在命令的前面加入一个time即可，例如： [plain] view plain copy print? time ./process time ps aux 在程序或命令运行结束后，在最后输出了三个时间，它们分别是：user：用户CPU时间，命令执行完成花费的用户CPU时间，即命令在用户态中执行时间总和；system：系统CPU时间，命令执行完成花费的系统CPU时间，即命令在核心态中执行时间总和；real：实际时间，从command命令行开始执行到运行终止的消逝时间； 注：用户CPU时间和系统CPU时间之和为CPU时间，即命令占用CPU执行的时间总和。实际时间要大于CPU时间，因为Linux是多任务操作系统，往往在执行一条命令时，系统还要处理其它任务。另一个需要注意的问题是即使每次执行相同命令，但所花费的时间也是不一样，其花费时间是与系统运行相关的。 查看内存溢出 查看内存溢出 jmap -heap pid #打印heap的概要信息 jmap -histo pid #打印每个class的实例数目，内存占用，类全名信息 jmap -dump:format=b,file=heap.bin pid #输出heap信息到heap.bin文件 jhat -J-mx768m heap.bin #分析heap.bin文件 jstack -l pid &gt; deadlock.jstack #输出stack信息到deadlock.jstack vi deadlock.jstack #使用vi查看]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis安装及使用]]></title>
    <url>%2F2017%2F06%2F17%2Fredis%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[序言最近公司一台主要的redis服务器出现了异常，无限制的启动很多进程，导致系统资源耗尽，ssh很卡，redis及其他定时异常，想办法杀死了僵尸进程，但是最终还有一千六百多个crond进程和python进程处于D状态无法kill掉，庆幸的是进程数没有暴增了，可以继续使用，但是特别卡机。所以只能决定重启服务器，不过担心重启后起不来的情况，即使起来了所有实时数据都丢失，因为redis没开启持久化和主从读写分离备份，而且公司业务数据实时性要求非常高，最终办法是重新开启一台服务器多实例运行redis，两个原有端口不变先进行程序写入测试，另两个端口则设置从库从原有服务器对应端口实例同步数据，并且将现有redis和其他crontab定时迁移过去，并且开启数据持久化，让其他所有往这台服务器写数据的程序都同时重写一份进新服务器，读取还是从原有服务器，因为数据是按当天实时的，所以等待一天后，新服务器的数据就同步了，再将读也切换到新服务器，重启旧服务器，问题解决，吸取教训，重新配置主从和开启从库持久化，下面将整个过程介绍一下。 安装一、在线源安装 12345678910111213[op@bogon yum.repos.d]$ yum install redisLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile* base: mirrors.btte.net* extras: mirrors.btte.net* updates: mirrors.btte.netbase | 3.7 kB 00:00extras | 3.5 kB 00:00updates | 3.5 kB 00:00updates/primary_db | 4.6 MB 00:25Setting up Install ProcessNo package redis available.Error: Nothing to do 实际上redia位于第三方的yum源里面，不在centos官方yum源里面，如何解决呢？ 1、去下面的网站下载EPEL对应的版本：（epel是fedora维护的yum源，里面软件众多） 1http://fedoraproject.org/wiki/EPEL 2、我下载的是这个： 1wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm 3、安装epel： 1234rpm -ivh epel-release-6-8.noarch.rpmwarning: epel-release-6-8.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID 0608b895: NOKEYPreparing... ########################################### [100%] 1:epel-release ########################################### [100%] 4、安装redis： 123456789101112131415161718192021222324252627[op@bogon yum.repos.d]$ yum install redisLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfileepel/metalink | 4.1 kB 00:00* base: mirrors.btte.net* epel: mirrors.sohu.com* extras: mirrors.btte.net* updates: mirrors.btte.netepel | 4.3 kB 00:00epel/primary_db | 5.0 MB 00:43Setting up Install ProcessResolving Dependencies--&gt; Running transaction check---&gt; Package redis.x86_64 0:2.4.10-1.el6 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved================================================================================Package Arch Version Repository Size================================================================================Installing:redis x86_64 2.4.10-1.el6 epel 213 kTransaction Summary================================================================================Install 1 Package(s)Total download size: 213 kInstalled size: 668 kIs this ok [y/N]:y 查看版本 12[op@bogon yum.repos.d]$ redis-server -vRedis server version 2.4.10 (00000000:0) 二、软件包安装 1) 下载redis安装包 可去官网 下载，也可通过wget命令， 1wget http://download.redis.io/redis-stable.tar.gz 2) 解压 1tar –zxvf redis-stable.tar.gz 3) 编译、安装 12cd redis-stablemake&amp;&amp;make install 如果提示gcc command不识别，请自行安装gcc: 12yum install gcc -yyum install gcc-g++ -y 如果提示couldn’t execute tcl : no such file or dicrectory，请自行安装tcl;如果提示: erroo:jmalloc/jemalloc.h:no such file or dicrectory请执行make distclean，然后再make 注意：若此时执行redis-server –v (查看版本命令)，若提示redis-server command not found，查看环境变量：echo $PATH，使用whereis redis-server查看redis-server命令目录，使用ln -s redis-server命令目录/redis-server 环境变量bin目录/redis-server创建软链接，客户端命令相同； 如 12sudo ln -s /usr/local/bin/redis-server /usr/sbin/redis-serversudo ln -s /usr/local/bin/redis-cli /usr/sbin/redis-cli /usr/sbin/redis-server环境变量中的命令，不存在/usr/local/bin/redis-server是make install后创建的 结果 12[op@bogon ~]$ ll /usr/sbin/redis-serverlrwxrwxrwx. 1 root root 27 Aug 18 15:17 /usr/sbin/redis-server -&gt; /usr/local/bin/redis-server 或者直接将/usr/local/bin目录加到环境变量，如何添加，此处不做详细介绍，可查看修改/etc/profile。 二．配置redis多实例环境 创建配置文件目录，dump file 目录，进程pid目录，log目录等 这里配置多实例环境 12cd /datamkdir -p redis/conf redis/log redis/pid redis/dump 注意各个目录的权限和所属用户及用户组,否则启动会报错dump和pid两个目录必须属于redis 12345678[op@bogon redis]$ lltotal 20drwxrwxr-x. 2 op op 4096 Aug 18 11:52 confdrwxr-xr-x. 2 redis root 4096 Aug 18 11:26 dumpdrwxr-xr-x. 2 op op 4096 Aug 18 11:25 logs-rw-------. 1 root root 0 Aug 18 11:52 nohup.outdrwxr-xr-x. 2 redis root 4096 Aug 18 11:52 pid-rwxr-xr-x. 1 op op 171 Aug 18 11:50 start.sh 修改配置文件，配置参数 软件源安装配置文件在/etc/redis.conf软件包安装配置文件在redis-stable/redis.conf 12cp /etc/redis.conf /data/redis/conf/redis-6380.confcp /etc/redis.conf /data/redis/conf/redis-6381.conf 建立日志文件 12vi redis/log/redis-6380.log //建立日志空文件并保存vi redis/log/redis-6381.log 这里以端口6380为例进行配置 打开redis-6380.conf文件修改端口(默认6379) 123# Accept connections on the specified port, default is 6379.# If port 0 is specified Redis will not listen on a TCP socket.port 6380 修改pid目录为新建目录 123# When running daemonized, Redis writes a pid file in /var/run/redis.pid by# default. You can specify a custom pid file location here.pidfile /data/redis/pid/redis-6380.pid 修改dump目录为新建目录及文件名 123456789# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## Also the Append Only File will be created inside this directory.## Note that you must specify a directory here, not a file name.dir /data/redis/dump/ 12# The filename where to dump the DBdbfilename dump-6380.rdb 修改log存储目录为新建目录 1234# Specify the log file name. Also &apos;stdout&apos; can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile /data/redis/logs/redis-6380.log 修改redis后台运行 123# By default Redis does not run as a daemon. Use &apos;yes&apos; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize yes 持久化 默认rdb，可选择是否开启aof，若开启，修改配置文件appendonly 编写启动脚本，一次性启动 1vi start.sh 脚本内容 123#!/bin/bashnohup redis-server /data/redis/conf/redis-6380.conf &amp;nohup redis-server /data/redis/conf/redis-6381.conf &amp; 启动redis，查看各目录下文件 查看进程 1234[root@bogon conf]# ps aux | grep redisroot 22900 0.0 0.0 39944 2632 ? Ssl Aug14 0:24 redis-server /data/redis/conf/redis-6381.confroot 22901 0.0 0.0 39944 3748 ? Ssl Aug14 0:25 redis-server /data/redis/conf/redis-6380.confroot 39320 0.0 0.0 103332 876 pts/0 S+ 15:41 0:00 grep redis 关闭redis服务 1[root@bogon conf]# redis-cli shutdown 查看dump信息 1ls -al /data/redis/dump 若配置了aof持久化方式，data目录下还会有aof的相关文件，后面会详细介绍 客户端连接redis 本机连接，不指定则连接默认端口6379 1redis-cli 其他客户机连接，需要指定ip和要连接的端口 1redis-cli -h 192.168.132.25 -p 6380 如若是拒绝连接，则是因为redis配置默认是只允许本机连接的，所以要开启其他客户机访问，就将所有bind信息注释掉 12345# If you want you can bind a single interface, if the bind option is not# specified all the interfaces will listen for incoming connections.##bind 127.0.0.1#bind 192.168.132.25 若是没有路由，则是防火墙问题，防火墙添加路由，或者直接关闭防火墙 12sudo service iptables status #查看防火墙状态sudo service iptables stop #关闭防火墙服务 数据备份与恢复前面的配置提到了两种持久化方式，接下来详细介绍 RDB方式(默认)RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的所有数据进行快照并存储在硬盘上。进行快照的条件可以由用户在配置文件中自定义，由两个参数构成：时间和改动的键的个数。当在指定的时间内被更改的键的个数大于指定的数值时就会进行快照。RDB是redis默认采用的持久化方式，在配置文件中已经预置了3个条件： 123save 900 1 # 900秒内有至少1个键被更改则进行快照save 300 10 # 300秒内有至少10个键被更改则进行快照save 60 10000 # 60秒内有至少10000个键被更改则进行快照 可以存在多个条件，条件之间是“或”的关系，只要满足其中一个条件，就会进行快照。 如果想要禁用自动快照，只需要将所有的save参数删除即可。 Redis默认会将快照文件存储在/var/lib/redis/目录(可CONFIG GET dir来查看)的dump.rdb文件中，可以通过配置dir和dbfilename两个参数分别指定快照文件的存储路径和文件名。 Redis实现快照的过程 Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。在执行fork的时候操作系统（类Unix操作系统）会使用写时复制（copy-on-write）策略，即fork函数发生的一刻父子进程共享同一内存数据，当父进程要更改其中某片数据时（如执行一个写命令 ），操作系统会将该片数据复制一份以保证子进程的数据不受影响，所以新的RDB文件存储的是执行fork一刻的内存数据。 Redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。这使得我们可以通过定时备份RDB文件来实 现Redis数据库备份。RDB文件是经过压缩（可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。 除了自动快照，还可以手动发送SAVE或BGSAVE命令让Redis执行快照，两个命令的区别在于，前者是由主进程进行快照操作，会阻塞住其他请求，后者会通过fork子进程进行快照操作。 Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。通常将一个记录一千万个字符串类型键、大小为1GB的快照文件载入到内 存中需要花费20～30秒钟。 通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。 手动备份与恢复 12345redis 127.0.0.1:6379&gt; SAVE #redis 备份目录中创建dump.rdb文件redis 127.0.0.1:6379&gt; CONFIG GET dir #获取rdb存放目录1) &quot;dir&quot;2) &quot;/data/redis/dump/&quot; 恢复，将备份文件dump.rdb放到对应的dir目录，重启redis服务即可恢复rdb数据 AOF方式默认情况下Redis没有开启AOF(append only file)方式的持久化，可以在redis.conf中通过appendonly参数开启： 1appendonly yes 在启动时Redis会逐个执行AOF文件中的命令来将硬盘中的数据载入到内存中，载入的速度相较RDB会慢一些 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof，可以通过appendfilename参数修改： 1appendfilename appendonly.aof 配置redis自动重写AOF文件的条件 123456auto-aof-rewrite-percentage 100 # 当目前的AOF文件大小超过上一次重写时的AOF文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时的AOF文件大小为依据auto-aof-rewrite-min-size 64mb # 允许重写的最小AOF文件大小配置写入AOF文件后，要求系统刷新硬盘缓存的机制# appendfsync always # 每次执行写入都会执行同步，最安全也最慢appendfsync everysec # 每秒执行一次同步操作# appendfsync no # 不主动进行同步操作，而是完全交由操作系统来做（即每30秒一次），最快也最不 Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动Redis后Redis会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少 主从同步(复制)通过持久化功能，Redis保证了即使在服务器重启的情况下也不会损失（或少量损失）数据。但是由于数据是存储在一台服务器上的，如果这台服务器的硬盘出现故障，也会导致数据丢失。为了避免单点故障，我们希望将数据库复制多个副本以部署在不同的服务器上，即使有一台服务器出现故障其他服务器依然可以继续提供服务。这就要求当一台服务器上的数据库更新后，可以自动将更新的数据同步到其他服务器上，Redis提供了复制（replication）功能可以自动实现同步的过程。 配置方法 通过配置文件 从数据库的配置文件中加入slaveof master-ip master-port，主数据库无需配置 通过命令行参数 启动redis-server的时候，使用命令行参数--slaveof master-ip master port 1redis-server --port 6380 --slaveof 192.168.133.25 6379 从数据库启动以后再设置 1slaveof master-ip master-port 如 12192.168.133.24:6380&gt; slaveof 192.168.133.25 6379OK 查看从库状态 1info 从库info 1234567891011121314151617181920212223242526192.168.133.24:6380&gt; info# Serverredis_version:2.8.19redis_git_sha1:00000000redis_git_dirty:0redis_build_id:4a607fa14f74d354redis_mode:standaloneos:Linux 2.6.32-358.el6.x86_64 x86_64arch_bits:64...# Replicationrole:slave #角色是从master_host:192.168.133.25master_port:6379master_link_status:up #状态是upmaster_last_io_seconds_ago:0master_sync_in_progress:0slave_repl_offset:1323763642slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 查看主库状态 12345678910111213141516171819192.168.133.25:6379&gt; info# Serverredis_version:2.8.19redis_git_sha1:00000000redis_git_dirty:0redis_build_id:4a607fa14f74d354redis_mode:standaloneos:Linux 2.6.32-358.el6.x86_64 x86_64arch_bits:64...# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.133.25,port=6380,state=online,offset=1426459508,lag=0master_repl_offset:1426478361repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1425429786repl_backlog_histlen:1048576 查看数据同步情况 12345192.168.133.24:6380&gt; dbsize(integer) 58562192.168.133.25:6379&gt; dbsize(integer) 58562 从数据库停止接收其他数据库的同步转主数据库 1SLAVEOF NO ONE 如 12192.168.133.24:6380&gt; slaveof no oneOK 结果如下 12345678# Replicationrole:masterconnected_slaves:0master_repl_offset:1722534164repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 优点及应用场景 读写分离 通过复制可以实现读写分离以提高服务器的负载能力。在常见的场景中，读的频率大于写，当单机的Redis无法应付大量的读请求时（尤其是较耗资源的请求，比如SORT命令等）可以通过复制功能建立多个从数据库，主数据库只进行写操作，而从数据库负责读操作。 从数据库持久化 持久化通常相对比较耗时，为了提高性能，可以通过复制功能建立一个（或若干个）从数据库，并在从数据库中启用持久化，同时在主数据库禁用持久化。当从数据库崩溃时重启后主数据库会自动将数据同步过来，所以无需担心数据丢失。而当主数据库崩溃时，需要在从数据库中使用SLAVEOF NO ONE命令将从数据库提升成主数据库继续服务，并在原来的主数据库启动后使用SLAVEOF命令将其设置成新的主数据库的从数据库，即可将数据同步回来。 集群配置更多资料查看]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>nosql</tag>
        <tag>redis</tag>
        <tag>持久化</tag>
        <tag>RDB</tag>
        <tag>AOF</tag>
        <tag>主从</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql主从备份]]></title>
    <url>%2F2017%2F06%2F15%2Fmysql%E4%B8%BB%E4%BB%8E%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[yum配置MySQL源并安装MySQL年前公司机房迁移，自然需要迁移数据库，定在凌晨4点左右停止服务拷贝数据，这之前需要搭好所有环境，数据拷贝到新机房服务器就启动所有生产服务，服务器版本时centos6.4 运维做的事全都落在了我的身上，在网上查了很多资料对比实践，最终考虑了两种方案，直接将数据通过mysqldump导出打包，由于数据量庞大，不锁表导出数个小时都没完事，而且锁表导出生产没法正常写，所以不停服务直接迁移思路放弃了。改用第二种直接迁移数据库文件目录，本地测试也行得通，但是遇到有数据库版本不一致遇到很多异常，所以最后保持与原生产服务器数据库版本一致迁移。 这段时间突然好些服务器磁盘出现问题，包括一台从库，害怕生产主库服务器挂掉，所以决定多做几台从库备份，以及生产环境应用程序环境备份。问题来了，从库已经挂掉了，当时没有停止主从同步，拷贝出来的数据库有问题，不能用，那么只能建空库，再执行同步，但是生产数据不能停止写，所以不能锁表进行同步。最后决定用当天的全量备份导入以后再进行同步，这样会丢失一部分数据，总体来说数据相差不会太大。 CentOS7默认数据库是mariadb,配置等用着不习惯,因此决定改成mysql,但是CentOS7的yum源中默认好像是没有mysql的。为了解决这个问题，我们要先下载mysql的repo源。 1.由于CentOS 的yum源中没有mysql，需要到mysql的官网下载yum repo配置文件 1wget http://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm 2.安装yum repo文件 1rpm -ivh mysql57-community-release-el7-9.noarch.rpm 执行完成后会在/etc/yum.repos.d/目录下生成两个repo文件mysql-community.repo mysql-community-source.repo 3.然后更新yum缓存 12yum clean all yum makecache 4.安装mysql 确认mysql是否已安装： 12yum list installed mysql* rpm -qa | grep mysql* 查看是否有安装包： 1yum list mysql* 安装客户端和服务器端 1yum install mysql-community-client.x86_64 mysql-community-common.x86_64 mysql-community-devel.x86_64 mysql-community-libs.x86_64 mysql-community-server.x86_64 或者 1rpm install mysql-server 指定版本安装(迁移mysql数据库时，为了避免不必要的错误，最好mysql版本一致，所以可以指定生产版本安装) 1yum install mysql-community-server-5.6.23-2.el6.x86_64 如果签名报错则到mysql官网下载校验文件将key复制进mysql签名文件及目录/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 或者修改/etc/yum.repos.d/mysql-community.repo文件 1修改gpgcheck=0即可跳过检查 5.启动mysql 1service mysqld start 6.查看初始密码（忘记修改root密码） 1grep &apos;temporary password&apos; /var/log/mysqld.log 得到如下内容： 12016-10-28T10:36:32.369073Z 1 [Note] A temporary password is generated for root@localhost: 5Oazqgpiat!p 如果没有找到，可以自行修改root密码 123456service mysqld stop #停止mysql服务，注意权限mysqld_safe --skip-grant-tables&amp; #也可以在配置文件中添加--skip-grant-tablesmysql -u root mysql #这里就不用指定-p了，直接登录use mysql #选择使用mysql数据库UPDATE user SET Password = PASSWORD(&apos;new password&apos;) WHERE user = &apos;root&apos;; #修改密码FLUSH PRIVILEGES; #刷新，生效 7.使用初始密码登录 1mysql -u root -p //回车，然后输入上一步查到的初始密码 8.更改初始密码 1ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;MyNewPass4!&apos;; 现在的mysql对密码强度要求较高，需要包含大小写字母、数字和特殊字符至此，mysql-server安装完成 9.允许远程访问设置 开放防火墙的端口号mysql增加权限：mysql库中的user表新增一条记录host为“%”，user为“root”。 12345use mysql;UPDATE user SET `Host` = &apos;%&apos; WHERE `User` = &apos;root&apos; LIMIT 1;# %表示允许所有的ip访问# 远程连接赋予权限grant all PRIVILEGES on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;password&apos; WITH GRANT OPTION; 创建用户，并赋予相应数据库的操作权限 1234CREATE USER &apos;ebook&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;ebook123&apos;;CREATE USER &apos;ebook&apos;@&apos;%&apos; IDENTIFIED BY &apos;ebook123&apos;;GRANT ALL ON ebook.* TO &apos;ebook&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 查看数据库端口 1show global variables like &apos;port&apos;; 读写分离，主从配置主库IP：192.168.1.10 从库IP：192.168.1.11 1、主库配置编辑my.cnf： 12345678910111213141516171819log_bin = mysql-bin//开启二进制日志server-id = 10 //服务器id必须唯一，这里以ip最后一位表示log-bin-index=mysql-bin.indexsync_binlog=1binlog_format=mixedbinlog-do-db = testdb //需要进行同步数据库binlog-ignore-db = mysql //不需要同步的数据库，也可以不设置binlog-ignore-db = performance_schema //不需要同步的数据库，也可以不设置binlog-ignore-db = information_schema //不需要同步的数据库，也可以不设置binlog_checksum=NONE 2、创建同步账号 12mysql&gt; grant replication slave on *.* to slave@192.168.1.11 identified by &apos;123456&apos;# slave/123456为从库的用户名/密码,可以读写操作testdb库的 3、主库状态 1234567mysql&gt; flush privileges;mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000008 | 337 |testdb | mysql,performance_schema,information_schema |+------------------+----------+--------------+------------------+ 记录下二进制日志文件名(File)和位置(Position)对应的值 4、从库配置文件配置 1234567891011121314#[必须]启用二进制日志log-bin=mysql-bin#[必须]服务器唯一ID，默认是1，一般取IP最后一段server-id=11relay-log-index = slave-relay-bin.indexrelay-log = slave-relay-binsync_master_info = 1sync_relay_log = 1sync_relay_log_info = 1 5、配置连接主库 12mysql&gt; stop slave;mysql&gt; change master to master_host=&apos;192.168.1.10&apos;,master_user=&apos;slave&apos;,master_password=&apos;123456&apos;, master_log_file=&apos;mysql-bin.000008&apos;,master_log_pos=337; 6、开始同步 12mysql&gt; start slave;mysql&gt; show slave status\G; 7、正常状态 12Slave_IO_Running: YesSlave_SQL_Running: Yes 8、解决主从不同步 先上Master库： 123456789mysql&gt; show processlist; //查看下进程是否Sleep太多。发现很正常。mysql&gt; show master status; //也正常。+-------------------+----------+--------------+-------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------------------+----------+--------------+-------------------------------+| mysqld-bin.000001 | 3260 |testdb | mysql,performance_schema,information_schema |+-------------------+----------+--------------+-------------------------------+1 row in set (0.00 sec) 再到Slave上查看 12345mysql&gt; show slave status\GSlave_IO_Running: YesSlave_SQL_Running: No 可见是Slave不同步 下面介绍两种解决方法： 方法一：忽略错误后，继续同步 该方法适用于主从库数据相差不大，或者要求数据可以不完全统一的情况，数据要求不严格的情况 解决： 1234567891011121314stop slave;#表示跳过一步错误，后面的数字可变set global sql_slave_skip_counter =1;start slave;#之后再查看：mysql&gt; show slave status\G Slave_IO_Running: YesSlave_SQL_Running: Yes ok，现在主从同步状态正常 方式二：重新做主从，完全同步 该方法适用于主从库数据相差较大，或者要求数据完全统一的情况 解决步骤如下： 1.先进入主库，进行锁表，防止数据写入 使用命令： 1mysql&gt; flush tables with read lock; 注意：该处是锁定为只读状态，语句不区分大小写 2.进行数据备份 把数据备份到mysql.bak.sql文件 1[root@server01 mysql]#mysqldump -uroot -p -hlocalhost &gt; mysql.bak.sql 这里注意一点：数据库备份一定要定期进行，确保数据万无一失 3.查看master 状态 1234567mysql&gt; show master status;+-------------------+----------+--------------+-------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------------------+----------+--------------+-------------------------------+| mysqld-bin.000001 | 3260 |testdb | mysql,performance_schema,information_schema |+-------------------+----------+--------------+-------------------------------+1 row in set (0.00 sec) 4.把mysql备份文件传到从库机器，进行数据恢复 使用scp命令 1[root@server01 mysql]# scp mysql.bak.sql root@192.168.128.11:/tmp/ 5.停止从库的状态 1mysql&gt; stop slave; 6.然后到从库执行mysql命令，导入数据备份 10.创建数据库 123456789# localhost本地ipmysql&gt; create user &apos;slave&apos;@&apos;localhost&apos; identified by &apos;123456&apos;;# %代表外网任意ipmysql&gt; create user &apos;slave&apos;@&apos;%&apos; identified by &apos;123456&apos;;mysql&gt; grant all privileges on `testdb`.* to &apos;slave&apos;@&apos;%&apos; identified by &apos;123456&apos;;mysql&gt; flush privileges;mysql&gt; create database testdb DEFAULT CHARSET utf8 COLLATE utf8_general_ci;mysql&gt; use testdb;mysql&gt; source /tmp/mysql.bak.sql 7.设置从库同步，注意该处的同步点，就是主库show master status信息里的| File| Position两项 1change master to master_host = &apos;192.168.128.10&apos;, master_user = &apos;slave&apos;, master_port=3306, master_password=&apos;123456&apos;, master_log_file = &apos;mysqld-bin.000001&apos;, master_log_pos=3260; 8.重新开启从同步 1mysql&gt; start slave; 9.查看同步状态 12345mysql&gt; show slave status\G;Slave_IO_Running: YesSlave_SQL_Running: Yes 主从复制跳过错误mysql主从复制，经常会遇到错误而导致slave端复制中断，这个时候一般就需要人工干预，跳过错误才能继续跳过错误有两种方式： 1.跳过指定数量的事务： 123mysql&gt;slave stop;mysql&gt;SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1 #跳过一个事务mysql&gt;slave start 2.修改mysql的配置文件，通过slave_skip_errors参数来跳所有错误或指定类型的错误 1234vi /etc/my.cnf[mysqld]#slave-skip-errors=1062,1053,1146 #跳过指定error no类型的错误#slave-skip-errors=all #跳过所有错误 修改datadir位置，启动失败Mysql修改datadir导致无法启动问题解决方法,本文原因是SELINUX导致,用关闭SELINUX的方法解决 停止mysqld然后修改/etc/my.cnf datadir的位置，启动mysqld提示FAILED，查看日志 12345678120609 11:31:31 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended120609 11:35:12 mysqld_safe Starting mysqld daemon with databases from /data/mysql120609 11:35:13 [Warning] Can&apos;t create test file /data/mysql/data.lower-test120609 11:35:13 [Warning] Can&apos;t create test file /data/mysql/data.lower-test/usr/sbin/mysqld: Can&apos;t change dir to &apos;/data/mysql&apos; (Errcode: 13)120609 11:35:13 [ERROR] Aborting120609 11:35:13 [Note] /usr/libexec/mysqld: Shutdown complete120609 11:35:13 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended 新的datadir路径确实没问题，而且目录和目录下所有文件都是777权限，上层目录也有rx权限，只不过datadir和下属文件owner都是root,年前迁移也是这样的，不影响。后来查到资料说是selinux问题，设置为permissive模式之后正常启动mysqld。 还有一种情况是目录迁移到了非root的目录下，也会报错。所以一定要确保mysql的上层目录所属用户的目录拥有者和权限问题。 1234567[root@data selinux]# getenforceEnforcing[root@data selinux]# setenforce 0[root@data selinux]# getenforcePermissivesetenforce 1 设置SELinux 成为enforcing模式setenforce 0 设置SELinux 成为permissive模式 或者彻底关闭，vi /etc/selinux/config 修改 SELINUX=disabled 12345678910# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 最后如果启动成功，登录失败，并报错： 123[jet@jet mysql]$ mysql -u root -pEnter password: ERROR 2002 (HY000): Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2) 则需要建立软链接即可：1sudo ln -s /home/mysql/mysql.sock /var/lib/mysql/ 定期删除二进制文件对MySQL数据库的所有更新(增加、删除、修改)会被保存到MySQL的二进制日志文件里。有了这个二进制文件的话，我们可以对数据库进行回滚处理和复旧等处理。 一般就保留一份二进制文件即可，即保留主库的，从库不需要开启二进制文件，其作用如下： 1.数据恢复 如果你的数据库出问题了，而你之前有过备份，那么可以看日志文件，找出是哪个命令导致你的数据库出问题了，想办法挽回损失。 2.主从服务器之间同步数据 主服务器上所有的操作都在记录日志中，从服务器可以根据该日志来进行，以确保两个同步。 A：在每个从属服务器上，使用SHOW SLAVE STATUS来检查它正在读取哪个日志。B：使用SHOW MASTER LOGS获得主服务器上的一系列日志。C：在所有的从属服务器中判定最早的日志，这个是目标日志，如果所有的从属服务器是最新的，就是清单上的最后一个日志。D：清理所有的日志，但是不包括目标日志，因为从服务器还要跟它同步。 开启二进制日志 编辑文件： 1vi /etc/my.cnf 添加以下代码： 12log-bin=mysql-binbinlog_format=mixed 清理日志方法为： 手动清理，操作需谨慎有的时候不想让mysql服务停止，那我们可以用下面的方法来删除binary文件。我们可以看到产生了二进制文件 12345678910111213141516171819mysql&gt; show binary logs;+------------------+------------+| Log_name | File_size |+------------------+------------+| mysql-bin.000001 | 15056 | | mysql-bin.000002 | 628368 | | mysql-bin.000003 | 377 | | mysql-bin.000004 | 141 | | mysql-bin.000005 | 1073742287 | | mysql-bin.000006 | 1073742035 | | mysql-bin.000007 | 823654620 | | mysql-bin.000008 | 2265 | | mysql-bin.000009 | 628368 | | mysql-bin.000010 | 117 | | mysql-bin.000011 | 4525 | | mysql-bin.000012 | 117 | | mysql-bin.000013 | 3147 | | mysql-bin.000014 | 85468109 | +------------------+------------+ 二进制文件一般用来做replication同步，当查看slave上同步正确，或者是同步已经完成了，这时如果硬盘空间又不是很大的话，那我们可以手动去清理这些binary文件。 1mysql&gt; purge binary logs to &apos;mysql-bin.000013&apos;; 就是删除二进制文件到mysql-bin.000013，最后一个mysql-bin.000014 保留着。 重启mysql服务器 1service mysql restart 定期自动清理二进制文件的容量是非常庞大的，所以要配置日志滚动。 expire_logs_days 在MySQL数据库的my.cnf文件里添加expire_logs_days，7是保存二进制日志文件的天数。 修改my.cnf文件以后别忘了重启MySQL。 1/etc/my.cnf 添加一下代码 12345[mysqld]...expire_logs_days = 7 修改my.cnf的配置以后，不想重启数据库的可以使用SET GLOBAL命令。 1234567891011mysql&gt; SET GLOBAL expire_logs_days = 7;Query OK, 0 rows affected (0.00 sec)mysql&gt; SHOW GLOBAL VARIABLES like &apos;expire_logs_days&apos;;+------------------+-------+| Variable_name | Value |+------------------+-------+| expire_logs_days | 7 |+------------------+-------+1 row in set (0.00 sec) 全量备份和恢复 mysqldump默认是锁表进行备份的 注意，如果你运行mysqldump没有—quick或—opt选项，mysqldump将在导出结果前装载整个结果集到内存中，如果你正在导出一个大的数据库，这将可能是一个问题。 锁表进行备份时可能会影响业务操作，根据自身情况选择即可 打包成gz到当前目录 1mysqldump -h 192.168.167.55 -utest1 -ptest2 --opt --compress --single-transaction test3 | gzip &gt; test3.sql.gz 恢复备份数据库 12gunzip test3.sql.gz #解压mysql&gt;source test3.sql #恢复 直接备份并恢复到本地数据库 1mysqldump -h 192.168.167.55 -utest1 -ptest2 --opt --compress --skip-lock-tables | mysql -h localhost -uroot -proot database 解释： 192.168.167.55 远程服务器名称 test1 远程数据库登录名 test2 远程数据库登录密码 test3 远程数据库名（即：复制的源） localhost 本地数据库名称（一般情况下都是这个） root 本地数据库登录名（一般情况下都是这个） root 本地数据库登录密码（一般情况下都是这个） database 本地（即：复制的目标数据库） sql解释： mysqldump 是mysql的一个专门用于拷贝操作的命令 —single-transaction 不锁表进行备份 —opt 操作的意思 —compress 压缩要传输的数据 —skip-lock 忽略锁住的表（加上这句能防止当表有外键时的报错） -tables 某数据库所有表 -h 服务器名称 -u 用户名（后面无空格，直接加用户名） -p 密码（后面无空格，直接加密码） 注意： -u、-p的后面没有空格，直接加用户名和密码！！！ 如何将一个mysql数据库中的一个表导入到另一个mysql数据库中 db1为原数据库，db2为要导出到的数据库，fromtable 是要导出的表名 1.方法一： 登录导出到的数据库，执行 create table fromtable select * from db1.fromtable; 跨服务器 2.方法二： mysqldump -h 192.168.135.12 -u root -p db1 fromtable &gt;&gt; ~/fromtable.sql; 输入秘密，root为用户名 登录db2 执行 source ~/fromtable.sql; 3.方法三： 登录db1 执行 select * from fromtable into outfile “~/fromtable .txt”; 导出纯数据格式 登录db2 执行 load data infile ~/fromtable .txt into table fromtable; 需要先建一张和原表结构一样的空表。]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>主从备份</tag>
        <tag>binlog</tag>
        <tag>mysqldump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库之mysql（一）]]></title>
    <url>%2F2017%2F06%2F15%2Fcentos%E5%9C%A8%E7%BA%BF%E6%BA%90yum%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[yum配置MySQL源并安装MySQL年前公司机房迁移，自然需要迁移数据库，定在凌晨4点左右停止服务拷贝数据，这之前需要搭好所有环境，数据拷贝到新机房服务器就启动所有生产服务，服务器版本时centos6.4 运维做的事全都落在了我的身上，在网上查了很多资料对比实践，最终考虑了两种方案，直接将数据通过mysqldump导出打包，由于数据量庞大，不锁表导出数个小时都没完事，而且锁表导出生产没法正常写，所以不停服务直接迁移思路放弃了。改用第二种直接迁移数据库文件目录，本地测试也行得通，但是遇到有数据库版本不一致遇到很多异常，所以最后保持与原生产服务器数据库版本一致迁移。 这段时间突然好些服务器磁盘出现问题，包括一台从库，害怕生产主库服务器挂掉，所以决定多做几台从库备份，以及生产环境应用程序环境备份。问题来了，从库已经挂掉了，当时没有停止主从同步，拷贝出来的数据库有问题，不能用，那么只能建空库，再执行同步，但是生产数据不能停止写，所以不能锁表进行同步。最后决定用当天的全量备份导入以后再进行同步，这样会丢失一部分数据，总体来说数据相差不会太大。 CentOS7默认数据库是mariadb,配置等用着不习惯,因此决定改成mysql,但是CentOS7的yum源中默认好像是没有mysql的。为了解决这个问题，我们要先下载mysql的repo源。 1.由于CentOS 的yum源中没有mysql，需要到mysql的官网下载yum repo配置文件 1wget http://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm 2.安装yum repo文件 1rpm -ivh mysql57-community-release-el7-9.noarch.rpm 执行完成后会在/etc/yum.repos.d/目录下生成两个repo文件mysql-community.repo mysql-community-source.repo 3.然后更新yum缓存 12yum clean all yum makecache 4.安装mysql 确认mysql是否已安装： 12yum list installed mysql* rpm -qa | grep mysql* 查看是否有安装包： 1yum list mysql* 安装客户端和服务器端 1yum install mysql-community-client.x86_64 mysql-community-common.x86_64 mysql-community-devel.x86_64 mysql-community-libs.x86_64 mysql-community-server.x86_64 或者 1rpm install mysql-server 指定版本安装(迁移mysql数据库时，为了避免不必要的错误，最好mysql版本一致，所以可以指定生产版本安装) 1yum install mysql-community-server-5.6.23-2.el6.x86_64 如果签名报错则到mysql官网下载校验文件将key复制进mysql签名文件及目录/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 或者修改/etc/yum.repos.d/mysql-community.repo文件 1修改gpgcheck=0即可跳过检查 5.启动mysql 1service mysqld start 6.查看初始密码（忘记修改root密码） 1grep &apos;temporary password&apos; /var/log/mysqld.log 得到如下内容： 12016-10-28T10:36:32.369073Z 1 [Note] A temporary password is generated for root@localhost: 5Oazqgpiat!p 如果没有找到，可以自行修改root密码 123456service mysqld stop #停止mysql服务，注意权限mysqld_safe --skip-grant-tables&amp; #也可以在配置文件中添加--skip-grant-tablesmysql -u root mysql #这里就不用指定-p了，直接登录use mysql #选择使用mysql数据库UPDATE user SET Password = PASSWORD(&apos;new password&apos;) WHERE user = &apos;root&apos;; #修改密码FLUSH PRIVILEGES; #刷新，生效 7.使用初始密码登录 1mysql -u root -p //回车，然后输入上一步查到的初始密码 8.更改初始密码 1ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;MyNewPass4!&apos;; 现在的mysql对密码强度要求较高，需要包含大小写字母、数字和特殊字符至此，mysql-server安装完成 9.允许远程访问设置 开放防火墙的端口号mysql增加权限：mysql库中的user表新增一条记录host为“%”，user为“root”。 12345use mysql;UPDATE user SET `Host` = &apos;%&apos; WHERE `User` = &apos;root&apos; LIMIT 1;# %表示允许所有的ip访问# 远程连接赋予权限grant all PRIVILEGES on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;password&apos; WITH GRANT OPTION; 创建用户，并赋予相应数据库的操作权限 1234CREATE USER &apos;ebook&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;ebook123&apos;;CREATE USER &apos;ebook&apos;@&apos;%&apos; IDENTIFIED BY &apos;ebook123&apos;;GRANT ALL ON ebook.* TO &apos;ebook&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 查看数据库端口 1show global variables like &apos;port&apos;; 读写分离，主从配置主库IP：192.168.1.10 从库IP：192.168.1.11 1、主库配置编辑my.cnf： 12345678910111213141516171819log_bin = mysql-bin//开启二进制日志server-id = 10 //服务器id必须唯一，这里以ip最后一位表示log-bin-index=mysql-bin.indexsync_binlog=1binlog_format=mixedbinlog-do-db = testdb //需要进行同步数据库binlog-ignore-db = mysql //不需要同步的数据库，也可以不设置binlog-ignore-db = performance_schema //不需要同步的数据库，也可以不设置binlog-ignore-db = information_schema //不需要同步的数据库，也可以不设置binlog_checksum=NONE 2、创建同步账号 12mysql&gt; grant replication slave on *.* to slave@192.168.1.11 identified by &apos;123456&apos;# slave/123456为从库的用户名/密码,可以读写操作testdb库的 3、主库状态 1234567mysql&gt; flush privileges;mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000008 | 337 |testdb | mysql,performance_schema,information_schema |+------------------+----------+--------------+------------------+ 记录下二进制日志文件名(File)和位置(Position)对应的值 4、从库配置文件配置 1234567891011121314#[必须]启用二进制日志log-bin=mysql-bin#[必须]服务器唯一ID，默认是1，一般取IP最后一段server-id=11relay-log-index = slave-relay-bin.indexrelay-log = slave-relay-binsync_master_info = 1sync_relay_log = 1sync_relay_log_info = 1 5、配置连接主库 12mysql&gt; stop slave;mysql&gt; change master to master_host=&apos;192.168.1.10&apos;,master_user=&apos;slave&apos;,master_password=&apos;123456&apos;, master_log_file=&apos;mysql-bin.000008&apos;,master_log_pos=337; 6、开始同步 12mysql&gt; start slave;mysql&gt; show slave status\G; 7、正常状态 12Slave_IO_Running: YesSlave_SQL_Running: Yes 8、解决主从不同步 先上Master库： 123456789mysql&gt; show processlist; //查看下进程是否Sleep太多。发现很正常。mysql&gt; show master status; //也正常。+-------------------+----------+--------------+-------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------------------+----------+--------------+-------------------------------+| mysqld-bin.000001 | 3260 |testdb | mysql,performance_schema,information_schema |+-------------------+----------+--------------+-------------------------------+1 row in set (0.00 sec) 再到Slave上查看 12345mysql&gt; show slave status\GSlave_IO_Running: YesSlave_SQL_Running: No 可见是Slave不同步 下面介绍两种解决方法： 方法一：忽略错误后，继续同步 该方法适用于主从库数据相差不大，或者要求数据可以不完全统一的情况，数据要求不严格的情况 解决： 1234567891011121314stop slave;#表示跳过一步错误，后面的数字可变set global sql_slave_skip_counter =1;start slave;#之后再查看：mysql&gt; show slave status\G Slave_IO_Running: YesSlave_SQL_Running: Yes ok，现在主从同步状态正常 方式二：重新做主从，完全同步 该方法适用于主从库数据相差较大，或者要求数据完全统一的情况 解决步骤如下： 1.先进入主库，进行锁表，防止数据写入 使用命令： 1mysql&gt; flush tables with read lock; 注意：该处是锁定为只读状态，语句不区分大小写 2.进行数据备份 把数据备份到mysql.bak.sql文件 1[root@server01 mysql]#mysqldump -uroot -p -hlocalhost &gt; mysql.bak.sql 这里注意一点：数据库备份一定要定期进行，确保数据万无一失 3.查看master 状态 1234567mysql&gt; show master status;+-------------------+----------+--------------+-------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------------------+----------+--------------+-------------------------------+| mysqld-bin.000001 | 3260 |testdb | mysql,performance_schema,information_schema |+-------------------+----------+--------------+-------------------------------+1 row in set (0.00 sec) 4.把mysql备份文件传到从库机器，进行数据恢复 使用scp命令 1[root@server01 mysql]# scp mysql.bak.sql root@192.168.128.11:/tmp/ 5.停止从库的状态 1mysql&gt; stop slave; 6.然后到从库执行mysql命令，导入数据备份 10.创建数据库 123456789# localhost本地ipmysql&gt; create user &apos;slave&apos;@&apos;localhost&apos; identified by &apos;123456&apos;;# %代表外网任意ipmysql&gt; create user &apos;slave&apos;@&apos;%&apos; identified by &apos;123456&apos;;mysql&gt; grant all privileges on `testdb`.* to &apos;slave&apos;@&apos;%&apos; identified by &apos;123456&apos;;mysql&gt; flush privileges;mysql&gt; create database testdb DEFAULT CHARSET utf8 COLLATE utf8_general_ci;mysql&gt; use testdb;mysql&gt; source /tmp/mysql.bak.sql 7.设置从库同步，注意该处的同步点，就是主库show master status信息里的| File| Position两项 1change master to master_host = &apos;192.168.128.10&apos;, master_user = &apos;slave&apos;, master_port=3306, master_password=&apos;123456&apos;, master_log_file = &apos;mysqld-bin.000001&apos;, master_log_pos=3260; 8.重新开启从同步 1mysql&gt; start slave; 9.查看同步状态 12345mysql&gt; show slave status\G;Slave_IO_Running: YesSlave_SQL_Running: Yes 主从复制跳过错误mysql主从复制，经常会遇到错误而导致slave端复制中断，这个时候一般就需要人工干预，跳过错误才能继续跳过错误有两种方式： 1.跳过指定数量的事务： 123mysql&gt;slave stop;mysql&gt;SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1 #跳过一个事务mysql&gt;slave start 2.修改mysql的配置文件，通过slave_skip_errors参数来跳所有错误或指定类型的错误 1234vi /etc/my.cnf[mysqld]#slave-skip-errors=1062,1053,1146 #跳过指定error no类型的错误#slave-skip-errors=all #跳过所有错误 修改datadir位置，启动失败Mysql修改datadir导致无法启动问题解决方法,本文原因是SELINUX导致,用关闭SELINUX的方法解决 停止mysqld然后修改/etc/my.cnf datadir的位置，启动mysqld提示FAILED，查看日志 12345678120609 11:31:31 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended120609 11:35:12 mysqld_safe Starting mysqld daemon with databases from /data/mysql120609 11:35:13 [Warning] Can&apos;t create test file /data/mysql/data.lower-test120609 11:35:13 [Warning] Can&apos;t create test file /data/mysql/data.lower-test/usr/sbin/mysqld: Can&apos;t change dir to &apos;/data/mysql&apos; (Errcode: 13)120609 11:35:13 [ERROR] Aborting120609 11:35:13 [Note] /usr/libexec/mysqld: Shutdown complete120609 11:35:13 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended 新的datadir路径确实没问题，而且目录和目录下所有文件都是777权限，上层目录也有rx权限，只不过datadir和下属文件owner都是root,年前迁移也是这样的，不影响。后来查到资料说是selinux问题，设置为permissive模式之后正常启动mysqld。 1234567[root@data selinux]# getenforceEnforcing[root@data selinux]# setenforce 0[root@data selinux]# getenforcePermissivesetenforce 1 设置SELinux 成为enforcing模式setenforce 0 设置SELinux 成为permissive模式 或者彻底关闭，vi /etc/selinux/config 修改 SELINUX=disabled 12345678910# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 定期删除二进制文件对MySQL数据库的所有更新(增加、删除、修改)会被保存到MySQL的二进制日志文件里。有了这个二进制文件的话，我们可以对数据库进行回滚处理和复旧等处理。 一般就保留一份二进制文件即可，即保留主库的，从库不需要开启二进制文件，其作用如下： 1.数据恢复 如果你的数据库出问题了，而你之前有过备份，那么可以看日志文件，找出是哪个命令导致你的数据库出问题了，想办法挽回损失。 2.主从服务器之间同步数据 主服务器上所有的操作都在记录日志中，从服务器可以根据该日志来进行，以确保两个同步。 A：在每个从属服务器上，使用SHOW SLAVE STATUS来检查它正在读取哪个日志。B：使用SHOW MASTER LOGS获得主服务器上的一系列日志。C：在所有的从属服务器中判定最早的日志，这个是目标日志，如果所有的从属服务器是最新的，就是清单上的最后一个日志。D：清理所有的日志，但是不包括目标日志，因为从服务器还要跟它同步。 开启二进制日志 编辑文件： 1vi /etc/my.cnf 添加以下代码： 12log-bin=mysql-binbinlog_format=mixed 清理日志方法为： 手动清理，操作需谨慎有的时候不想让mysql服务停止，那我们可以用下面的方法来删除binary文件。我们可以看到产生了二进制文件 12345678910111213141516171819mysql&gt; show binary logs;+------------------+------------+| Log_name | File_size |+------------------+------------+| mysql-bin.000001 | 15056 | | mysql-bin.000002 | 628368 | | mysql-bin.000003 | 377 | | mysql-bin.000004 | 141 | | mysql-bin.000005 | 1073742287 | | mysql-bin.000006 | 1073742035 | | mysql-bin.000007 | 823654620 | | mysql-bin.000008 | 2265 | | mysql-bin.000009 | 628368 | | mysql-bin.000010 | 117 | | mysql-bin.000011 | 4525 | | mysql-bin.000012 | 117 | | mysql-bin.000013 | 3147 | | mysql-bin.000014 | 85468109 | +------------------+------------+ 二进制文件一般用来做replication同步，当查看slave上同步正确，或者是同步已经完成了，这时如果硬盘空间又不是很大的话，那我们可以手动去清理这些binary文件。 1mysql&gt; purge binary logs to &apos;mysql-bin.000013&apos;; 就是删除二进制文件到mysql-bin.000013，最后一个mysql-bin.000014 保留着。 重启mysql服务器 1service mysql restart 定期自动清理二进制文件的容量是非常庞大的，所以要配置日志滚动。 expire_logs_days 在MySQL数据库的my.cnf文件里添加expire_logs_days，7是保存二进制日志文件的天数。 修改my.cnf文件以后别忘了重启MySQL。 1/etc/my.cnf 添加一下代码 12345[mysqld]...expire_logs_days = 7 修改my.cnf的配置以后，不想重启数据库的可以使用SET GLOBAL命令。 1234567891011mysql&gt; SET GLOBAL expire_logs_days = 7;Query OK, 0 rows affected (0.00 sec)mysql&gt; SHOW GLOBAL VARIABLES like &apos;expire_logs_days&apos;;+------------------+-------+| Variable_name | Value |+------------------+-------+| expire_logs_days | 7 |+------------------+-------+1 row in set (0.00 sec) 全量备份和恢复 mysqldump默认是锁表进行备份的 注意，如果你运行mysqldump没有—quick或—opt选项，mysqldump将在导出结果前装载整个结果集到内存中，如果你正在导出一个大的数据库，这将可能是一个问题。 锁表进行备份时可能会影响业务操作，根据自身情况选择即可 打包成gz到当前目录 1mysqldump -h 192.168.167.55 -utest1 -ptest2 --opt --compress --single-transaction test3 | gzip &gt; test3.sql.gz 恢复备份数据库 12gunzip test3.sql.gz #解压mysql&gt;source test3.sql #恢复 直接备份并恢复到本地数据库 1mysqldump -h 192.168.167.55 -utest1 -ptest2 --opt --compress --skip-lock-tables | mysql -h localhost -uroot -proot database 解释： 192.168.167.55 远程服务器名称 test1 远程数据库登录名 test2 远程数据库登录密码 test3 远程数据库名（即：复制的源） localhost 本地数据库名称（一般情况下都是这个） root 本地数据库登录名（一般情况下都是这个） root 本地数据库登录密码（一般情况下都是这个） database 本地（即：复制的目标数据库） sql解释： mysqldump 是mysql的一个专门用于拷贝操作的命令 —single-transaction 不锁表进行备份 —opt 操作的意思 —compress 压缩要传输的数据 —skip-lock 忽略锁住的表（加上这句能防止当表有外键时的报错） -tables 某数据库所有表 -h 服务器名称 -u 用户名（后面无空格，直接加用户名） -p 密码（后面无空格，直接加密码） 注意： -u、-p的后面没有空格，直接加用户名和密码！！！ 如何将一个mysql数据库中的一个表导入到另一个mysql数据库中 db1为原数据库，db2为要导出到的数据库，fromtable 是要导出的表名 1.方法一： 登录导出到的数据库，执行 create table fromtable select * from db1.fromtable; 跨服务器 2.方法二： mysqldump -h 192.168.135.12 -u root -p db1 fromtable &gt;&gt; ~/fromtable.sql; 输入秘密，root为用户名 登录db2 执行 source ~/fromtable.sql; 3.方法三： 登录db1 执行 select * from fromtable into outfile “~/fromtable .txt”; 导出纯数据格式 登录db2 执行 load data infile ~/fromtable .txt into table fromtable; 需要先建一张和原表结构一样的空表。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Generics]]></title>
    <url>%2F2017%2F06%2F12%2FJava%20Generics%2F</url>
    <content type="text"><![CDATA[今天看了下Java的官方教程中关于泛型的部分。泛型引起我的注意是因为微博上一篇比较List&lt;?&gt;和List&lt;Object&gt;的文章。最近看Lucene的代码，其中Util部分大量的使用了泛型，今天刚好浮生修得一日闲便全面的学习一下Java的泛型。其中很多内容都与直觉相符，所以我觉得不值得通篇翻译，这里仅就我觉得有趣也反直觉的地方做一个总结。 The Java™ Tutorials Lesson: Generics 命名规则C++和Java中都学习过泛型，可是从来觉得写T就跟写for循环中的i一样，也就是约定俗成，谁知这里还是有一些规则的。 E - Element (used extensively by the Java Collections Framework) K - Key N - Number T - Type V - Value S, U, V etc. - 2nd, 3rd, 4th types The Diamond在Java 7及以后的版本中，当调用泛型类的构造函数时，可以省去类型参数，只使用一组空的尖括号，只要在编译期类型可以被编译器确定或者推导出来。这对尖括号，就叫作diamond。比如我们可以像如下这样声明一个链表： 1List&lt;String&gt; list = new ArrayList&lt;&gt;(); diamond在英文中可以指扑克牌中的方片，两个尖括号放在一起&lt;&gt;，的确像一个方片。 Multiple BoundsList&lt;? extends A&gt; 和 List&lt;? super A&gt;代表类型为A的子类和A的父类的链表类型。这里的语法称为upper bound和lower bound。这些都是非常基础的知识了。但是类型参数可以有多个bounds，这样的写法并不常见。如: 1&lt;T extends A &amp; B &amp; C&gt; 需要注意的是，如果A、B、C中有一个为类，其余为接口的话，类必须写到第一个的位置，否则会在编译时报错。 Unbounded Wildcardunbounded wildcard在两种情形下很有用： 当你在写一个只需要类Object提供的功能就能够完成的方法时 当使用的泛型类中的方法并不依赖于泛型参数时。比如List.clear或List.size。实际上，我们经常使用Class&lt;?&gt;，因为Class&lt;T&gt;中的大部分方法都和T无关。 考虑下面的方法： 12345public static void printList(List&lt;Object&gt; list) &#123; for (Object elem : list) System.out.println(elem + &quot; &quot;); System.out.println();&#125; printList的目的是打印任意类型的列表，但是上面的函数却做不到。它只能打印Object的List，无法打印List&lt;Integer&gt;，List&lt;String&gt;或List&lt;Double&gt;，因为它们都不是List&lt;Object&gt;的子类。为了写一个泛型的printList，需要使用List&lt;?&gt;： 12345public static void printList(List&lt;?&gt; list) &#123; for (Object elem: list) System.out.print(elem + &quot; &quot;); System.out.println();&#125; 因为对任何具体类型A，List&lt;A&gt;是List&lt;?&gt;的子类。 需要特别注意的是，List&lt;Object&gt;和List&lt;?&gt;是不一样的。你可以往List&lt;Object&gt;插入Object和其他Object的子类。但是你只能往List&lt;?&gt;中插入null。 Lower Bounded Wildcard你可以为一个Wildcard指定Upper Bound或者Lower Bound，但是不能同时指定。 Wildcards and Subtyping The common parent is List&lt;?&gt; A hierarchy of several generic List class declarations. Guidelines for Wildcard Use为了讨论的方便，我们假设一个变量提供下面的两种功能： “In”变量，为函数提供输入数据 “Out”变量，为函数提供输出 Wildcard Guidelines： “In”变量用Upper Bounded Wildcard定义，使用extends关键字 “Out”变量用Lower Bounded Wildcard定义，使用super关键字 当”In”变量可以用Object中定义的方法访问时，使用Unbounded Wildcard 当变量同时作为”In”和”Out”时，不要使用Wildcard 这份Guideline并不适用于函数的返回值类型，应该避免使用wildcard作为函数的返回值类型。 泛型的限制 不能用基本类型实例化泛型 不能创建类型参数的实例 你不能创建类型参数的实例，但是可以使用反射创建。 不能创建包含类型参数的static成员 因为所有类的实例都共同拥有static成员，但是可以创建和类的类型参数不同的泛型static函数 不能创建参数类型的数组 1234Object[] stringLists = new List&lt;String&gt;[]; // compiler error, but pretend it&apos;s allowedstringLists[0] = new ArrayList&lt;String&gt;(); // OKstringLists[1] = new ArrayList&lt;Integer&gt;(); // An ArrayStoreException should be thrown, // but the runtime can&apos;t detect it. 如果允许创建类型参数的数组，上面的代码将会抛出ArrayStoreException 不能创建，catch或throw参数类型的对象 泛型类不能直接或间接继承Throwable。 12345// Extends Throwable indirectlyclass MathException&lt;T&gt; extends Exception &#123; /** ... **/ &#125; // compile-time error// Extends Throwable directlyclass QueueFullException&lt;T&gt; extends Throwable &#123; /** ... **/ // compile-time error 一个方法不能catch类型参数的实例 12345678public static &lt;T extends Exception, J&gt; void execute(List&lt;J&gt; jobs) &#123; try &#123; for (J job : jobs) // ... &#125; catch (T e) &#123; // compile-time error // ... &#125;&#125; 然而，可以在throws子句中使用类型参数 12345class Parser&lt;T extends Exception&gt; &#123; public void parse(File file) throws T &#123; // OK // ... &#125;&#125; 不能拥有在Type Erasure之后签名一样的重载函数 1234public class Example &#123; public void print(Set&lt;String&gt; strSet) &#123; &#125; public void print(Set&lt;Integer&gt; intSet) &#123; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>泛型</tag>
        <tag>Generics</tag>
        <tag>Wildcard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown-语法说明]]></title>
    <url>%2F2017%2F05%2F30%2FMarkdown-%E8%AF%AD%E6%B3%95%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[斜体和粗体代码: 斜体 或者 斜体 粗体 加粗斜体 删除线 显示效果: 斜体 或者 斜体 粗体 加粗斜体 删除线 分级标题写法1，代码: # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 效果如下： 一级标题二级标题三级标题四级标题五级标题六级标题写法2，代码: 一级标题 ============================ 二级标题 ---------------------------- 效果如下： 一级标题 二级标题超链接 Markdown 支持两种形式的链接语法： 行内式和参考式两种形式，行内式一般使用较多。 行内式 语法说明： []里写链接文字，()里写链接地址, ()中的”“中可以为链接指定title属性，title属性可加可不加。title属性的效果是鼠标悬停在链接上会出现指定的 title文字。链接文字’这样的形式。链接地址与链接标题前有一个空格。 代码： 欢迎来到[jet&#39;s blog](http://jet-han.oschina.io/) 欢迎来到[jet&#39;s blog](http://jet-han.oschina.io/ &quot;jet&#39;s blog&quot;) 显示效果： 欢迎来到jet’s blog 欢迎来到jet’s blog 参考式 参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 语法说明：参考式链接分为两部分，文中的写法 [链接文字][链接标记]，在文本的任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格。 如果链接文字本身可以做为链接标记，你也可以写成[链接文字][][链接文字]：链接地址的形式，见代码的最后一行。 代码： 1.我经常去的几个网站[Google][1]、[印象笔记][2]以及[自己的博客][3] 2.[印象 笔记][2]是一个不错的[网站][]。 3.[1]:http://www.google.com &quot;Google&quot; 4.[2]:https://app.yinxiang.com &quot;印象笔记&quot; 5.[3]:http://jet-han.oschina.io &quot;jet&#39;s blog&quot; 6.[网站]:https://app.yinxiang.com 显示效果： 我经常去的几个网站Google、印象笔记以及自己的博客 印象 笔记是一个不错的网站。 自动链接 语法说明：Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如： 代码： https://www.baidu.com/ &lt;fajie_han@foxmail.com&gt; 显示效果： https://www.baidu.com/ &#102;&#97;&#x6a;&#x69;&#101;&#x5f;&#x68;&#97;&#110;&#64;&#x66;&#x6f;&#x78;&#109;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d; 列表无序列表 使用 *，+，- 表示无序列表 代码： - 无序列表项 一 + 无序列表项 二 * 无序列表项 三 显示效果： 无序列表项 一 无序列表项 二 无序列表项 三 有序列表 有序列表则使用数字接着一个英文句点。 代码： 1. 有序列表项 一 2. 有序列表项 二 3. 有序列表项 三 显示效果： 1.有序列表项 一 2.有序列表项 二 3.有序列表项 三 定义型列表 语法说明： 定义型列表由名词和解释组成。一行写上定义，紧跟一行写上解释。解释的写法:紧跟一个缩进(Tab) 代码： Markdown : 轻量级文本标记语言，可以转换成html，pdf等格式（左侧有一个可见的冒号和四个不可见的空格） 代码块 2 : 这是代码块的定义（左侧有一个可见的冒号和四个不可见的空格） 代码块（左侧有八个不可见的空格） 显示效果： Markdown 轻量级文本标记语言，可以转换成html，pdf等格式 代码块 2 这是代码块的定义 代码块（左侧有八个不可见的空格） 列表缩进 语法说明： 列表项目标记通常是放在最左边，但是其实也可以缩进，最多 3 个空格，项目标记后面则一定要接着至少一个空格或制表符。要让列表看起来更漂亮，你可以把内容用固定的缩进整理好（显示效果与代码一致）： 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。软泥上的青荇， 油油的在水底招摇； 在康河的柔波里， 我甘心做一条水草！ 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。寻梦？撑一支长篙， 向青草更青处漫溯； 满载一船星辉， 在星辉斑斓里放歌。但我不能放歌， 悄悄是别离的笙箫； 夏虫也为我沉默， 沉默是今晚的康桥！悄悄的我走了， 正如我悄悄的来； 我挥一挥衣袖， 不带走一片云彩。 但是如果你懒，那也行： 代码： * 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。 那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。 软泥上的青荇， 油油的在水底招摇； 在康河的柔波里， 我甘心做一条水草！ * 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 寻梦？撑一支长篙， 向青草更青处漫溯； 满载一船星辉， 在星辉斑斓里放歌。 但我不能放歌， 悄悄是别离的笙箫； 夏虫也为我沉默， 沉默是今晚的康桥！ 悄悄的我走了， 正如我悄悄的来； 我挥一挥衣袖， 不带走一片云彩。 显示效果： 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。软泥上的青荇， 油油的在水底招摇； 在康河的柔波里， 我甘心做一条水草！ 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。寻梦？撑一支长篙， 向青草更青处漫溯； 满载一船星辉， 在星辉斑斓里放歌。但我不能放歌， 悄悄是别离的笙箫； 夏虫也为我沉默， 沉默是今晚的康桥！悄悄的我走了， 正如我悄悄的来； 我挥一挥衣袖， 不带走一片云彩。 包含引用的列表 语法说明： 如果要在列表项目内放进引用，那 &gt; 就需要缩进： 代码： * 阅读的方法: &gt; 打开书本。 &gt; 打开电灯。 显示效果： 阅读的方法 打开书本。 打开电灯。 包含代码区块的引用 语法说明： 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符： 一列表项包含一个列表区块： &lt;代码写在这&gt; 一个特殊情况 在特殊情况下，项目列表很可能会不小心产生，像是下面这样的写法： 1986. What a great season. 会显示成： What a great season. 换句话说，也就是在行首出现数字-句点-空白，要避免这样的状况，你可以在句点前面加上反斜杠： 1986\. What a great season. 会显示成： 1986. What a great season. 引用 语法说明： 引用需要在被引用的文本前加上&gt;符号。 代码： &gt; 这是一个有两段文字的引用, &gt; 无意义的占行文字1. &gt; 无意义的占行文字2. &gt; 无意义的占行文字3. &gt; 无意义的占行文字4. 显示效果： 这是一个有两段文字的引用, 无意义的占行文字1. 无意义的占行文字2. 无意义的占行文字3. 无意义的占行文字4. 引用的多层嵌套 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &gt; ： 代码： &gt;&gt;&gt; 请问 Markdwon 怎么用？ - 小白 &gt;&gt; 自己看教程！ - 愤青 &gt; 教程在哪？ - 小白 显示效果： 请问 Markdwon 怎么用？ - 小白 自己看教程！ - 愤青 教程在哪？ - 小白 引用其它要素 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等： 代码： &gt; 1. 这是第一行列表项。 &gt; 2. 这是第二行列表项。 &gt; &gt; 给出一些例子代码： &gt; &gt; return shell_exec(&quot;echo $input | $markdown_script&quot;); 显示效果： 这是第一行列表项。 这是第二行列表项。 给出一些例子代码： return shell_exec(“echo $input | $markdown_script”); 插入图像 图片的创建方式与超链接相似，而且和超链接一样也有两种写法，行内式和参考式写法。语法中图片Alt的意思是如果图片因为某些原因不能显示，就用定义的图片Alt文字来代替图片。 图片Title则和 链接中的Title一样，表示鼠标悬停与图片上时出现的文字。 Alt 和 Title 都不是必须的，可以省略，但建议写上。 行内式 语法说明： ![图片Alt](图片地址 “图片Title”) 代码： 阿狸： ![阿狸](http://jethan.bid/img/ali.jpg &quot;阿狸&quot;) 显示效果： 阿狸： 参考式 语法说明： 在文档要插入图片的地方写![图片Alt][标记] 在文档的最后写上[标记]:图片地址 “Title” 代码： 1.阿狸： 2.![阿狸][Ali] 3.[Ali]:http://jet-han.oschina.io/img/ali.jpg &quot;阿狸&quot; 显示效果： 阿狸： 注脚 语法说明： 在需要添加注脚的文字后加上脚注名字注脚名字,称为加注。 然后在文本的任意位置(一般在最后)添加脚注， 脚注前必须有对应的脚注名字。 注意：经测试注脚与注脚之间必须空一行，不然会失效。成功后会发现，即使你没有把注脚写在文末，经Markdown转换后，也会自动归类到文章的最后。 代码： 1.使用 Markdown[^1]可以效率的书写文档, 直接转换成 HTML[^2], 你可以使用 Leanote[^Le] 编辑器进行书写。 2. 3.[^1]:Markdown是一种纯文本标记语言 4. 5.[^2]:HyperText Markup Language 超文本标记语言 #此处有空格会不起作用 6. 7.[^Le]:开源笔记平台，支持Markdown和笔记直接发为博文 显示效果： 使用 Markdown1可以效率的书写文档, 直接转换成 HTML2, 你可以使用 LeanoteLe 编辑器进行书写。 LaTeX 公式渲染MathJax数学公式 在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 原因 Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线’_’代表斜体，会被渲染引擎处理为&lt;\em&gt;标签。 因为类Latex格式书写的数学公式下划线 ‘_’ 表示下标，有特殊的含义，如果被强制转换为&lt;\em&gt;标签，那么 MathJax引擎在渲染数学公式的时候就会出错。例如，$x_i$在开始被渲染的时候，处理为$x&lt;\em&gt;i&lt;\/em&gt;$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。 类似的语义冲突的符号还包括’*’, ‘{‘, ‘}’, ‘\’等。 解决方法 更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。 npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： // escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ 这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 // em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。 在主题中开启mathjax开关 如何使用了主题了，别忘了在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： # MathJax Support mathjax: enable: true per_page: true 别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下： --- title: index.html date: 2016-12-28 21:01:30 tags: mathjax: true -- 不要嫌麻烦，之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。 $ 表示行内公式： 代码： 质能守恒方程可以用一个很简洁的方程式 $E=mc^2$ 来表达。 显示效果： 质能守恒方程可以用一个很简洁的方程式 $E=mc^2$ 来表达。 $$ 表示整行公式： 代码： $$\sum_{i=1}^n a_i=0$$ $$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$ $$\sum^{j-1}_{k=0}{\widehat{\gamma}_{kj} z_k}$$ 显示效果： \sum_{i=1}^n a_i=0f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2\sum^{j-1}_{k=0}{\widehat{\gamma}_{kj} z_k}访问 MathJax 参考更多使用方法。 流程图 代码： &lt;div id=&quot;flowchart-0&quot; class=&quot;flow-chart&quot;&gt;&lt;/div&gt; 显示效果： Install Generate flowchart diagrams for Hexo. npm install —save hexo-filter-flowchart config In your site’s _config.yml: flowchart: raphael: http://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js flowchart: https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js 表格 语法说明： 不管是哪种方式，第一行为表头，第二行分隔表头和主体部分，第三行开始每一行为一个表格行。 列于列之间用管道符|隔开。原生方式的表格每一行的两边也要有管道符。 第二行还可以为不同的列指定对齐方向。默认为左对齐，在-右边加上:就右对齐。 代码： 为列指定方向写表格： | Item | Value | Qty | | :- | -: | :-: | | Computer | 1600 USD | 5 | | Phone | 12 USD| 12 | | Pipe | 1 USD| 234 | 原生方式写表格： |Item|Value|Qty| |-|-|-| |Computer|1600 USD|5| |Phone|12 USD|12| |Pipe|1 USD|234| 显示效果： 为列指定方向写表格： Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 原生方式写表格： Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 使用Echarts动态图表 在博客页面中引用js文件 在所用主题目录下layout_partial中的head.swig里加入： &lt;script src=&quot;http://echarts.baidu.com/dist/echarts.common.min.js&quot;&gt;&lt;/script&gt; 安装hexo-tag-echarts插件 npm install hexo-tag-echarts --save 使用范例 对于echarts实例，将其提供的option部分复制，形成下述代码即可。 {% echarts 400 '81%' %} { tooltip : { trigger: 'axis', axisPointer : { // 坐标轴指示器，坐标轴触发有效 type : 'shadow' // 默认为直线，可选为：'line' | 'shadow' } }, legend: { data:['利润', '支出', '收入'] }, grid: { left: '3%', right: '4%', bottom: '3%', containLabel: true }, xAxis : [ { type : 'value' } ], yAxis : [ { type : 'category', axisTick : {show: false}, data : ['周一','周二','周三','周四','周五','周六','周日'] } ], series : [ { name:'利润', type:'bar', itemStyle : { normal: { label: {show: true, position: 'inside'} } }, data:[200, 170, 240, 244, 200, 220, 210] }, { name:'收入', type:'bar', stack: '总量', itemStyle: { normal: { label : {show: true} } }, data:[320, 302, 341, 374, 390, 450, 420] }, { name:'支出', type:'bar', stack: '总量', itemStyle: {normal: { label : {show: true, position: 'left'} }}, data:[-120, -132, -101, -134, -190, -230, -210] } ] }; {% endecharts %} 效果显示 // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById('echarts3539')); // 指定图表的配置项和数据 var option = { tooltip : { trigger: 'axis', axisPointer : { // 坐标轴指示器，坐标轴触发有效 type : 'shadow' // 默认为直线，可选为：'line' | 'shadow' } }, legend: { data:['利润', '支出', '收入'] }, grid: { left: '3%', right: '4%', bottom: '3%', containLabel: true }, xAxis : [ { type : 'value' } ], yAxis : [ { type : 'category', axisTick : {show: false}, data : ['周一','周二','周三','周四','周五','周六','周日'] } ], series : [ { name:'利润', type:'bar', itemStyle : { normal: { label: {show: true, position: 'inside'} } }, data:[200, 170, 240, 244, 200, 220, 210] }, { name:'收入', type:'bar', stack: '总量', itemStyle: { normal: { label : {show: true} } }, data:[320, 302, 341, 374, 390, 450, 420] }, { name:'支出', type:'bar', stack: '总量', itemStyle: {normal: { label : {show: true, position: 'left'} }}, data:[-120, -132, -101, -134, -190, -230, -210] } ] }; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); 分隔线 你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线： 代码： * * * *** ***** - - - --------------------------------------- 显示效果都一样： 代码 对于程序员来说这个功能是必不可少的，插入程序代码的方式有两种，一种是利用缩进(Tab), 另一种是利用”`”符号（一般在ESC键下方）包裹代码。 语法说明： 插入行内代码，即插入一个单词或者一句代码的情况，使用`code`这样的形式插入。插入多行代码，可以使用缩进或者“` code “`,具体看示例。注意： 缩进式插入前方必须有空行 行内式 代码： C语言里的函数 `scanf()` 怎么使用？ 显示效果： C语言里的函数 scanf() 怎么使用？ 缩进式多行代码 缩进 4 个空格或是 1 个制表符 一个代码区块会一直持续到没有缩进的那一行（或是文件结尾）。 代码： #include &lt;stdio.h&gt; int main(void) { printf(&quot;Hello world\n&quot;); } 显示效果： #include &lt;stdio.h&gt; int main(void) { printf(&quot;Hello world\n&quot;); } 用六个`包裹多行代码 代码： 12345#include &lt;stdio.h&gt;int main(void)&#123; printf(&quot;Hello world\n&quot;);&#125; 显示效果： 12345#include &lt;stdio.h&gt;int main(void)&#123;printf(&quot;Hello world\n&quot;);&#125; HTML 原始码 在代码区块里面， &amp; 、 &lt; 和 &gt; 会自动转成 HTML 实体，这样的方式让你非常容易使用 Markdown 插入范例用的 HTML 原始码，只需要复制贴上，剩下的 Markdown 都会帮你处理，例如： 代码： &lt;table&gt; &lt;tr&gt; &lt;th rowspan=&quot;2&quot;&gt;值班人员&lt;/th&gt; &lt;th&gt;星期一&lt;/th&gt; &lt;th&gt;星期二&lt;/th&gt; &lt;th&gt;星期三&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;李强&lt;/td&gt; &lt;td&gt;张明&lt;/td&gt; &lt;td&gt;王平&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 显示效果： 值班人员 星期一 星期二 星期三 李强 张明 王平 自定义字体 在 主题配置 - NexT 使用文档 中提及了如何设置字体样式，这里就不再赘述了。如果想自定义字体大小以及颜色，可以直接在 Markdown 文档中使用 html 语法 &lt;font size=4 &gt; 这里输入文字，自定义大小 &lt;/font&gt; &lt;font color=&quot;#FF0000&quot;&gt; 这里输入文字，自定义颜色的字体 &lt;/font&gt; 效果： 这里输入文字，自定义大小 这里输入文字，自定义颜色的字体 st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java常见排序算法]]></title>
    <url>%2F2017%2F05%2F30%2Fjava%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言日常操作中常见的排序方法有：冒泡排序、快速排序、选择排序、插入排序、希尔排序，甚至还有基数排序、鸡尾酒排序、桶排序、鸽巢排序、归并排序等。 冒泡排序一种简单的排序算法。它重复地走访过要排序的数列，依次比较相邻的两个元素，通过一次比较把未排序序列中最大（或最小）的元素放置在未排序序列的末尾，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。12345678910111213141516171819202122232425/** * 冒泡法排序&lt;br/&gt; * &lt;li&gt;比较相邻的元素。如果第一个比第二个大，就交换他们两个。&lt;/li&gt; * &lt;li&gt;对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。&lt;/li&gt; * &lt;li&gt;针对所有的元素重复以上的步骤，除了最后一个。&lt;/li&gt; * &lt;li&gt;持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。&lt;/li&gt; * * @param numbers * 需要排序的整型数组 */ public static void bubbleSort(int[] numbers) &#123; int temp; // 记录临时中间值 int size = numbers.length; // 数组大小 for (int i = 0; i &lt; size - 1; i++) &#123; for (int j = i + 1; j &lt; size; j++) &#123; if (numbers[i] &lt; numbers[j]) &#123; // 交换两数的位置 temp = numbers[i]; numbers[i] = numbers[j]; numbers[j] = temp; &#125; &#125; &#125; &#125; 方式2： 123456789101112131415public class BubbleSort &#123; public static void sort(int data[]) &#123; for (int i = 0; i &lt; data.length -1; i++) &#123; for (int j = 0; j &lt; data.length - i - 1; j++) &#123; if (data[j] &gt; data[j + 1]) &#123; int temp = data[j]; data[j] = data[j + 1]; data[j + 1] = temp; &#125; &#125; &#125; &#125; &#125; 快速排序使用分治法策略通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 12345678910111213141516171819202122232425262728293031323334353637/** * 快速排序&lt;br/&gt; * &lt;ul&gt; * &lt;li&gt;从数列中挑出一个元素，称为“基准”&lt;/li&gt; * &lt;li&gt;重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分割之后， * 该基准是它的最后位置。这个称为分割（partition）操作。&lt;/li&gt; * &lt;li&gt;递归地把小于基准值元素的子数列和大于基准值元素的子数列排序。&lt;/li&gt; * &lt;/ul&gt; * * @param numbers * @param start * @param end */ public static void quickSort(int[] numbers, int start, int end) &#123; if (start &lt; end) &#123; int base = numbers[start]; // 选定的基准值（第一个数值作为基准值） int temp; // 记录临时中间值 int i = start, j = end; do &#123; while ((numbers[i] &lt; base) &amp;&amp; (i &lt; end)) i++; while ((numbers[j] &gt; base) &amp;&amp; (j &gt; start)) j--; if (i &lt;= j) &#123; temp = numbers[i]; numbers[i] = numbers[j]; numbers[j] = temp; i++; j--; &#125; &#125; while (i &lt;= j); if (start &lt; j) quickSort(numbers, start, j); if (end &gt; i) quickSort(numbers, i, end); &#125; &#125; 方法2： 123456789101112131415161718192021public class QuickSort &#123; public static void sort(int data[], int start, int end) &#123; if (end - start &lt;= 0) &#123; return; &#125; int last = start; for (int i = start + 1; i &lt;= end; i++) &#123; if (data[i] &lt; data[start]) &#123; int temp = data[++last]; data[last] = data[i]; data[i] = temp; &#125; &#125; int temp = data[last]; data[last] = data[start]; data[start] = temp; sort(data, start, last - 1); sort(data, last + 1, end); &#125; &#125; 选择排序每一次从待排序的数据元素中选出最小（或最大）的一个元素，顺序放在已排好序的数列的最后，直到全部待排序的数据元素排完。 123456789101112131415161718192021/** * 选择排序&lt;br/&gt; * &lt;li&gt;在未排序序列中找到最小元素，存放到排序序列的起始位置&lt;/li&gt; * &lt;li&gt;再从剩余未排序元素中继续寻找最小元素，然后放到排序序列末尾。&lt;/li&gt; * &lt;li&gt;以此类推，直到所有元素均排序完毕。&lt;/li&gt; * * @param numbers */ public static void selectSort(int[] numbers) &#123; int size = numbers.length, temp; for (int i = 0; i &lt; size; i++) &#123; int k = i; for (int j = size - 1; j &gt;i; j--) &#123; if (numbers[j] &lt; numbers[k]) k = j; &#125; temp = numbers[i]; numbers[i] = numbers[k]; numbers[k] = temp; &#125; &#125; 方式2： 12345678910111213141516171819202122public class SelectionSort &#123; public static void sort(int data[]) &#123; int minVal; int minIndex; for (int i = 0; i &lt; data.length - 1; i++) &#123; minVal = data[i]; minIndex = i; for (int j = i + 1; j &lt; data.length; j++) &#123; if (data[j] &lt; minVal) &#123; minVal = data[j]; minIndex = j; &#125; &#125; if (minVal != data[i] &amp;&amp; minIndex != i) &#123; data[minIndex] = data[i]; data[i] = minVal; &#125; &#125; &#125; &#125; 插入排序将数列分为有序和无序两个部分，每次处理就是将无序数列的第一个元素与有序数列的元素从后往前逐个进行比较，找出插入位置，将该元素插入到有序数列的合适位置中。 12345678910111213141516171819202122/** * 插入排序&lt;br/&gt; * &lt;ul&gt; * &lt;li&gt;从第一个元素开始，该元素可以认为已经被排序&lt;/li&gt; * &lt;li&gt;取出下一个元素，在已经排序的元素序列中从后向前扫描&lt;/li&gt; * &lt;li&gt;如果该元素（已排序）大于新元素，将该元素移到下一位置&lt;/li&gt; * &lt;li&gt;重复步骤3，直到找到已排序的元素小于或者等于新元素的位置&lt;/li&gt; * &lt;li&gt;将新元素插入到该位置中&lt;/li&gt; * &lt;li&gt;重复步骤2&lt;/li&gt; * &lt;/ul&gt; * * @param numbers */ public static void insertSort(int[] numbers) &#123; int size = numbers.length, temp, j; for(int i=1; i&lt;size; i++) &#123; temp = numbers[i]; for(j = i; j &gt; 0 &amp;&amp; temp &lt; numbers[j-1]; j--) numbers[j] = numbers[j-1]; numbers[j] = temp; &#125; &#125; 方式2： 12345678910111213public class InsertionSort &#123; public static void sort(int data[]) &#123; for (int i = 1; i &lt; data.length; i++) &#123; for (int j = i; j &gt; 0; j--) &#123; if (data[j] &lt; data[j - 1]) &#123; int temp = data[j]; data[j] = data[j - 1]; data[j - 1] = temp; &#125; &#125; &#125; &#125; &#125; 归并排序将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。排序过程如下：（1）申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列（2）设定两个指针，最初位置分别为两个已经排序序列的起始位置（3）比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置（4）重复步骤3直到某一指针达到序列尾（5）将另一序列剩下的所有元素直接复制到合并序列尾 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 归并排序&lt;br/&gt; * &lt;ul&gt; * &lt;li&gt;申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列&lt;/li&gt; * &lt;li&gt;设定两个指针，最初位置分别为两个已经排序序列的起始位置&lt;/li&gt; * &lt;li&gt;比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置&lt;/li&gt; * &lt;li&gt;重复步骤3直到某一指针达到序列尾&lt;/li&gt; * &lt;li&gt;将另一序列剩下的所有元素直接复制到合并序列尾&lt;/li&gt; * &lt;/ul&gt; * * @param numbers */ public static void mergeSort(int[] numbers, int left, int right) &#123; int t = 1;// 每组元素个数 int size = right - left + 1; while (t &lt; size) &#123; int s = t;// 本次循环每组元素个数 t = 2 * s; int i = left; while (i + (t - 1) &lt; size) &#123; merge(numbers, i, i + (s - 1), i + (t - 1)); i += t; &#125; if (i + (s - 1) &lt; right) merge(numbers, i, i + (s - 1), right); &#125; &#125; /** * 归并算法实现 * * @param data * @param p * @param q * @param r */ private static void merge(int[] data, int p, int q, int r) &#123; int[] B = new int[data.length]; int s = p; int t = q + 1; int k = p; while (s &lt;= q &amp;&amp; t &lt;= r) &#123; if (data[s] &lt;= data[t]) &#123; B[k] = data[s]; s++; &#125; else &#123; B[k] = data[t]; t++; &#125; k++; &#125; if (s == q + 1) B[k++] = data[t++]; else B[k++] = data[s++]; for (int i = p; i &lt;= r; i++) data[i] = B[i]; &#125; 方法2： 123456789101112131415161718192021222324252627282930313233343536public class MergeSort &#123; public static void sort(int data[], int start, int end) &#123; if (start &lt; end) &#123; int mid = (start + end) / 2; sort(data, start, mid); sort(data, mid + 1, end); merge(data, start, mid, end); &#125; &#125; public static void merge(int data[], int start, int mid, int end) &#123; int temp[] = new int[end - start + 1]; int i = start; int j = mid + 1; int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (data[i] &lt; data[j]) &#123; temp[k++] = data[i++]; &#125; else &#123; temp[k++] = data[j++]; &#125; &#125; while (i &lt;= mid) &#123; temp[k++] = data[i++]; &#125; while (j &lt;= end) &#123; temp[k++] = data[j++]; &#125; for (k = 0, i = start; k &lt; temp.length; k++, i++) &#123; data[i] = temp[k]; &#125; &#125; &#125; 二分查找/折半查找有序的序列，每次都是以序列的中间位置的数来与待查找的关键字进行比较，每次缩小一半的查找范围，直到匹配成功 12345678910111213141516171819202122232425/** * 使用递归的二分查找 *title:recursionBinarySearch *@param arr 有序数组 *@param key 待查找关键字 *@return 找到的位置 */ public static int recursionBinarySearch(int[] arr,int key,int low,int high)&#123; if(key &lt; arr[low] || key &gt; arr[high] || low &gt; high)&#123; return -1; &#125; int middle = (low + high) / 2; //初始中间位置 if(arr[middle] &gt; key)&#123; //比关键字大则关键字在左区域 return recursionBinarySearch(arr, key, low, middle - 1); &#125;else if(arr[middle] &lt; key)&#123; //比关键字小则关键字在右区域 return recursionBinarySearch(arr, key, middle + 1, high); &#125;else &#123; return middle; &#125; &#125; 12345678910111213141516171819202122232425262728293031/** * 使用while查找 *title:commonBinarySearch *@param arr *@param key *@return 关键字位置 */ public static int commonBinarySearch(int[] arr,int key)&#123; int low = 0; int high = arr.length - 1; int middle = 0; //定义middle if(key &lt; arr[low] || key &gt; arr[high] || low &gt; high)&#123; return -1; &#125; while(low &lt;= high)&#123; middle = (low + high) / 2; if(arr[middle] &gt; key)&#123; //比关键字大则关键字在左区域 high = middle - 1; &#125;else if(arr[middle] &lt; key)&#123; //比关键字小则关键字在右区域 low = middle + 1; &#125;else&#123; return middle; &#125; &#125; return -1; //最后仍然没有找到，则返回-1 &#125; 所有排序算法整理成NumberSort类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import java.util.Random; //Java实现的排序类 public class NumberSort &#123; //私有构造方法，禁止实例化 private NumberSort() &#123; super(); &#125; //冒泡法排序 public static void bubbleSort(int[] numbers) &#123; int temp; // 记录临时中间值 int size = numbers.length; // 数组大小 for (int i = 0; i &lt; size - 1; i++) &#123; for (int j = i + 1; j &lt; size; j++) &#123; if (numbers[i] &lt; numbers[j]) &#123; // 交换两数的位置 temp = numbers[i]; numbers[i] = numbers[j]; numbers[j] = temp; &#125; &#125; &#125; &#125; //快速排序 public static void quickSort(int[] numbers, int start, int end) &#123; if (start &lt; end) &#123; int base = numbers[start]; // 选定的基准值（第一个数值作为基准值） int temp; // 记录临时中间值 int i = start, j = end; do &#123; while ((numbers[i] &lt; base) &amp;&amp; (i &lt; end)) i++; while ((numbers[j] &gt; base) &amp;&amp; (j &gt; start)) j--; if (i &lt;= j) &#123; temp = numbers[i]; numbers[i] = numbers[j]; numbers[j] = temp; i++; j--; &#125; &#125; while (i &lt;= j); if (start &lt; j) quickSort(numbers, start, j); if (end &gt; i) quickSort(numbers, i, end); &#125; &#125; //选择排序 public static void selectSort(int[] numbers) &#123; int size = numbers.length, temp; for (int i = 0; i &lt; size; i++) &#123; int k = i; for (int j = size - 1; j &gt; i; j--) &#123; if (numbers[j] &lt; numbers[k]) k = j; &#125; temp = numbers[i]; numbers[i] = numbers[k]; numbers[k] = temp; &#125; &#125; //插入排序 // @param numbers public static void insertSort(int[] numbers) &#123; int size = numbers.length, temp, j; for (int i = 1; i &lt; size; i++) &#123; temp = numbers[i]; for (j = i; j &gt; 0 &amp;&amp; temp &lt; numbers[j - 1]; j--) numbers[j] = numbers[j - 1]; numbers[j] = temp; &#125; &#125; //归并排序 public static void mergeSort(int[] numbers, int left, int right) &#123; int t = 1;// 每组元素个数 int size = right - left + 1; while (t &lt; size) &#123; int s = t;// 本次循环每组元素个数 t = 2 * s; int i = left; while (i + (t - 1) &lt; size) &#123; merge(numbers, i, i + (s - 1), i + (t - 1)); i += t; &#125; if (i + (s - 1) &lt; right) merge(numbers, i, i + (s - 1), right); &#125; &#125; //归并算法实现 private static void merge(int[] data, int p, int q, int r) &#123; int[] B = new int[data.length]; int s = p; int t = q + 1; int k = p; while (s &lt;= q &amp;&amp; t &lt;= r) &#123; if (data[s] &lt;= data[t]) &#123; B[k] = data[s]; s++; &#125; else &#123; B[k] = data[t]; t++; &#125; k++; &#125; if (s == q + 1) B[k++] = data[t++]; else B[k++] = data[s++]; for (int i = p; i &lt;= r; i++) data[i] = B[i]; &#125; &#125; 参考链接1参考链接2]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>sort</tag>
        <tag>冒泡</tag>
        <tag>快速</tag>
        <tag>选择</tag>
        <tag>插入</tag>
        <tag>希尔</tag>
        <tag>二分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql索引及查询优化]]></title>
    <url>%2F2017%2F05%2F29%2Fmysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[mysql调优在做性能测试中经常会遇到一些sql的问题，其实做性能测试这几年遇到问题最多还是数据库这块，要么就是IO高要么就是cpu高，所以对数据的优化在性能测试过程中占据着很重要的地方。 常用SQL语句优化： 数据库(表)设计合理,我们的表设计要符合3NF,3范式(规范的模式),有时我们需要适当的逆范式 sql语句的优化(索引，常用小技巧.) 慢查询 （分析出现出问题的sql） Explain （显示了mysql如何使用索引来处理select语句以及连接表。可以帮助选择更好的索引和写出更优化的查询语句） Profile（查询到 SQL 会执行多少时间, 并看出 CPU/Memory 使用量, 执行过程中 Systemlock, Table lock 花多少时间等等.） 数据的配置(缓存设大) 适当硬件配置和操作系统 (读写分离.) 数据的3NF(normal forms) 1NF :就是具有原子性，不可分割。简单来说就是无重复的行，并且列不可再分(只要使用的是关系性数据库，就自动符合)； 2NF: 在满足1NF 的基础上，我们考虑是否满足2NF: 只要表的记录满足唯一性,也是说,你的同一张表，不可能出现完全相同的记录, 一般说我们在表中设计一个主键即可； 3NF: 在满足2NF 的基础上，我们考虑是否满足3NF：即我们的字段信息可以通过关联的关系，派生即可.(通常我们通过外键来处理)。 逆范式（为什么需要逆范式:）: 逆范式化指的是通过增加冗余或重复的数据来提高数据库的读性能。例如：在user_role用户-角色中间表增加字段role_name(user表通过role_id与role表关联查询role_name，直接在user表中添加冗余字段role_name)。逆范式化可以减少关联查询时，join表的次数。 编号 姓名 出生日期 生肖 1 张三 1980-01-02 猴 2 李四 1983-04-02 牛 3 王五 1988-11-02 牛 生肖表 生肖编号 生肖名称 1 猴 2 牛 【说明】生肖其实是和时间有关系了，可以通过生日来确定生肖是什么，所以这里的生肖完全可以忽略不计，这样设计是为了减轻数据库的计算压力，所以才保留此字段的，也就是前面提到的“冗余字段”，逆范式。所以由此可以推出3NF是完全可以违反的。 总结： NF1 行不可重复，列不可再分 （必须满足) NF2 非主键列必须依赖于主键列 （必须满足） NF3 非主键列之间必须相互独立 （NF3可以不满足 ） sql语句分类 ddl(数据定义语言) [create truncate alter drop] dml(数据操作语言) [insert delete upate select] dtl(数据事务语句) [commit rollback savepoint] dcl(数据控制语句) [grant revoke] 优化查询分析工具查询数据库性能 show status like &quot;connections&quot;; # 连接服务器次数 show status like &quot;uptime&quot;; # 服务器上线时间 show status like &quot;slow_queries&quot;; #慢查询次数 show status like &quot;com_select&quot;; #查询操作次数 show status like &quot;com_insert&quot;; #插入次数 show status like &quot;com_delete&quot;; #删除次数 show variables like &quot;long_query_time&quot;; #慢查询时间 慢查询详见慢查询 分析查询语句详见explain分析查询 分析检测优化表Analyze TableMySQL 的Optimizer（优化元件）在优化SQL语句时，首先需要收集一些相关信息，其中就包括表的cardinality（可以翻译为“散列程度”），它表示某个索引对应的列包含多少个不同的值——如果cardinality大大少于数据的实际散列程度，那么索引就基本失效了。我们可以使用SHOW INDEX语句来查看索引的散列程度： SHOW INDEX FROM PLAYERS; TABLE KEY_NAME COLUMN_NAME CARDINALITY ------- -------- ----------- ----------- PLAYERS PRIMARY PLAYERNO 14 因为此时PLAYER表中不同的PLAYERNO数量远远多于14，索引基本失效。下面我们通过Analyze Table语句来修复索引： ANALYZE TABLE PLAYERS; SHOW INDEX FROM PLAYERS; 结果是： TABLE KEY_NAME COLUMN_NAME CARDINALITY ------- -------- ----------- ----------- PLAYERS PRIMARY PLAYERNO 1000 此时索引已经修复，查询效率大大提高。 需要注意的是，如果开启了binlog，那么Analyze Table的结果也会写入binlog，我们可以在analyze和table之间添加关键字local取消写入。 Checksum Table数据在传输时，可能会发生变化，也有可能因为其它原因损坏，为了保证数据的一致，我们可以计算checksum（校验值）。使用MyISAM引擎的表会把checksum存储起来，称为live checksum，当数据发生变化时，checksum会相应变化。在执行Checksum Table时，可以在最后指定选项qiuck或是extended；quick表示返回存储的checksum值，而extended会重新计算checksum，如果没有指定选项，则默认使用extended。 Optimize Table经常更新数据的磁盘需要整理碎片，数据库也是这样，Optimize Table语句对MyISAM和InnoDB类型的表都有效。如果表经常更新，就应当定期运行Optimize Table语句，保证效率。与Analyze Table一样，Optimize Table也可以使用local来取消写入binlog。 Check Table数据库经常可能遇到错误，譬如数据写入磁盘时发生错误，或是索引没有同步更新，或是数据库未关闭MySQL就停止了。遇到这些情况，数据就可能发生错误：Incorrect key file for table: ‘ ‘. Try to repair it.此时，我们可以使用Check Table语句来检查表及其对应的索引。譬如我们运行 CHECK TABLE PLAYERS; 结果是 TABLE OP MSG_TYPE MSG_TEXT -------------- ----- -------- -------- TENNIS.PLAYERS check status OK MySQL会保存表最近一次检查的时间，每次运行check table都会存储这些信息： 执行 SELECT TABLE_NAME, CHECK_TIME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = &#39;PLAYERS&#39; AND TABLE_SCHEMA = &#39;TENNIS&#39;; /*TENNIS是数据库名*/ 结果是 TABLE_NAME CHECK_TIME ---------- ------------------- PLAYERS 2006-08-21 16:44:25 Check Table还可以指定其它选项： UPGRADE：用来测试在更早版本的MySQL中建立的表是否与当前版本兼容。 QUICK：速度最快的选项，在检查各列的数据时，不会检查链接（link）的正确与否，如果没有遇到什么问题，可以使用这个选项。 FAST：只检查表是否正常关闭，如果在系统掉电之后没有遇到严重问题，可以使用这个选项。 CHANGED：只检查上次检查时间之后更新的数据。 MEDIUM：默认的选项，会检查索引文件和数据文件之间的链接正确性。 EXTENDED：最慢的选项，会进行全面的检查。 Repair Table用于修复表，只对MyISAM和ARCHIVE类型的表有效。这条语句同样可以指定选项： QUICK：最快的选项，只修复索引树。 EXTENDED：最慢的选项，需要逐行重建索引。 USE_FRM：只有当MYI文件丢失时才使用这个选项，全面重建整个索引。 与Analyze Table一样，Repair Table也可以使用local来取消写入binlog。 查询高速缓存show variables like ‘%query_cache%’; 查询状态show full processlist状态有sleep、query、locked、analyzing and statistics、coping to tmp table、Sorting result、sending data 开启慢查询日志以下所有配置是在5.1.73版本中 设置慢查询时间long_query_time是用来定义慢于多少秒的才算“慢查询” 1234567mysql&gt; show variables like &apos;long_query_time%&apos;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 1.000000 |+-----------------+----------+1 row in set (0.00 sec) 设置为1, 也就是执行时间超过1秒的都算慢查询。 12mysql&gt; set global long_query_time = 1;Query OK, 0 rows affected (0.00 sec) 查看是否打开慢查询日志记录123456789mysql&gt; show variables like &apos;slow%&apos;;+---------------------+---------------------------------+| Variable_name | Value |+---------------------+---------------------------------+| slow_launch_time | 2 || slow_query_log | ON || slow_query_log_file | /home/logs/slow.log |+---------------------+---------------------------------+3 rows in set (0.00 sec) 打开日志记录12mysql&gt; set global slow_query_log = on;Query OK, 0 rows affected (0.00 sec) 一旦slow_query_log变量被设置为ON，mysql会立即开始记录。/etc/my.cnf 里面可以设置上面MYSQL全局变量的初始值。 在[mysqld]下添加如下代码： 123long_query_time=1 #执行时间超过1s的都记录日志slow_query_log = 1 #开启慢查询slow_query_log_file=/home/logs/slow.log 使用select sleep(1)跑一条慢查询数据查看日志是否记录 【注意】注意这里文件夹权限问题，否则会报错/usr/libexec/mysqld: File &#39;/home/jet/logs/slow.log&#39; not found (Errcode: 13) 这里是因为jet目录所属用户和用户组是jet不是mysql导致，同时logs文件夹必须所属mysql用户和用户组；home文件夹当然是属于root用户的 12cd /homesudo chown mysql.mysql logs 下面测试直接写入到/home/test.txt同样会报权限错误: 1234mysql&gt; select user from user into outfile &apos;/home/test.txt&apos;;ERROR 1 (HY000): Can&apos;t create/write to file &apos;/home/test.txt&apos; (Errcode: 13)mysql&gt; select user from user into outfile &apos;/home/logs/test.txt&apos;;Query OK, 8 rows affected (0.00 sec) 如果写到/home/logs文件夹下，则通过: 12345678910111213141516171819[jet@jet logs]$ pwd/home/logs[jet@jet logs]$ lltotal 8-rw-rw---- 1 mysql mysql 174 Sep 7 17:22 slow.log-rw-rw-rw- 1 mysql mysql 31 Sep 7 17:27 test.txt[jet@jet logs]$ sudo more slow.log /usr/libexec/mysqld, Version: 5.1.73-log (Source distribution). started with:Tcp port: 0 Unix socket: /var/lib/mysql/mysql.sockTime Id Command Argument[jet@jet logs]$ sudo more test.txt rootjetrootrootrootroot 还有一种情况是，如果文件夹权限都没问题，依然报(Errcode: 13),那么可能是selinux关闭selinux: 1setenforce 0 或者修改配置文件sudo vim /etc/selinux/config将SELINUX=enforcing改为SELINUX=disabled mysqldumpslow/mysqlsla分析日志mysqldumpslowmysqldumpslow -s c -t 10 /home/logs/slow.log 这会输出记录次数最多的10条SQL语句，其中： -s, 是表示按照何种方式排序，c、t、l、r分别是按照记录次数、时间、查询时间、返回的记录数来排序，ac、at、al、ar，表示相应的倒叙；-t, 是top n的意思，即为返回前面多少条的数据；-g, 后边可以写一个正则匹配模式，大小写不敏感的；如下： 得到返回记录集最多的10个查询。1mysqldumpslow -s r -t 10 /home/logs/slow.log 得到按照时间排序的前10条里面含有左连接的查询语句。1mysqldumpslow -s t -t 10 -g “left join” /home/logs/slow.log 使用mysqldumpslow命令可以非常明确的得到各种我们需要的查询语句，对MySQL查询语句的监控、分析、优化是MySQL优化的第一步，也是非常重要的一步。 mysqlsla慢查询分析工具12345678910111213141516wget http: //hackmysql .com /scripts/mysqlsla-2 .03. tar .gztar xzf mysqlsla-2.03. tar .gzcd mysqlsla-2.03 perl Makefile.PLmakemake install#安装信息#Installing /usr/local/share/perl5/mysqlsla.pm#Installing /usr/local/share/man/man3/mysqlsla.3pm#Installing /usr/local/bin/mysqlsla#Appending installation info to /usr/lib/perl5/perllocal.pod file /usr/local/bin/mysqlsla#其实是一个perl脚本#/usr/local/bin/mysqlsla: a /usr/bin/perl -w script text executable 慢查询统计统计出现次数最多的前10条慢查询1mysqlsla -lt slow /home/logs/slow.log - top 10 - sort c_sum &gt; top10_count_sum.log 统计执行时间的总和前10条慢查询1mysqlsla -lt slow /home/logs/slow.log - top 10 - sort t_sum &gt; top10_time_sum.log 统计平均执行时间最长的前10条慢查询(常用)1mysqlsla -lt slow /home/logs/slow.log - top 10 - sort t_avg &gt; top10_time_avg.log explain分析查询使用 EXPLAIN 关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。这可以帮你分析你的查询语句或是表结构的性能瓶颈。通过explain命令可以得到:– 表的读取顺序– 数据读取操作的操作类型– 哪些索引可以使用– 哪些索引被实际使用– 表之间的引用– 每张表有多少行被优化器查询 EXPLAIN字段解释： Table：显示这一行的数据是关于哪张表的 possible_keys：显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key：实际使用的索引。如果为NULL，则没有使用索引。MYSQL很少会选择优化不足的索引，此时可以在SELECT语句中使用USE INDEX（index）来强制使用一个索引或者用IGNORE INDEX（index）来强制忽略索引 key_len：使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref：显示索引的哪一列被使用了，如果可能的话，是一个常数 rows：MySQL认为必须检索的用来返回请求数据的行数 type：这是最重要的字段之一，显示查询使用了何种类型。从最好到最差的连接类型为system、const、eq_reg、ref、range、index和ALL system、const：可以将查询的变量转为常量. 如id=1; id为 主键或唯一键. eq_ref：访问索引,返回某单一行的数据.(通常在联接时出现，查询使用的索引为主键或惟一键) ref：访问索引,返回某个值的数据.(可以返回多行) 通常使用=时发生 range：这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西，并且该字段上建有索引时发生的情况(注:不一定好于index) index：以索引的顺序进行全表扫描，优点是不用排序,缺点是还要全表扫描 ALL：全表扫描，应该尽量避免 Extra：关于MYSQL如何解析查询的额外信息，主要有以下几种 using index：只用到索引,可以避免访问表. using where：使用到where来过虑数据. 不是所有的where clause都要显示using where. 如以=方式访问索引. using tmporary：用到临时表，看到这个的时候，查询需要优化了。这里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上 using filesort：用到额外的排序. (当使用order by v1,而没用到索引时,就会使用额外的排序) range checked for eache record(index map:N)：没有好的索引. Distinct (一旦MYSQL找到了与行相联合匹配的行，就不再搜索了（ Not exists （MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行， 就不再搜索了） Where used (使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题) 索引及查询优化索引的类型 其实按照定义，除了聚集索引以外的索引都是非聚集索引，只是人们想细分一下非聚集索引，分成普通索引，唯一索引，全文索引。聚集索引的叶子节点就是对应的数据节点（MySQL的MyISAM除外，此存储引擎的聚集索引和非聚集索引只多了个唯一约束，其他没什么区别），可以直接获取到对应的全部列的数据，而非聚集索引在索引没有覆盖到对应的列的时候需要进行二次查询， 比较 聚集索引就字典的首字母查询 非聚集索引就是字典的偏旁部首查询 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续 如何创建 InnoDB按照主键进行聚集，如果没有定义主键，InnoDB会试着使用唯一的非空索引来代替。如果没有这种索引，InnoDB就会定义隐藏的主键然后在上面进行聚集。 所以，对于 聚集索引 来说，你创建主键的时候，自动就创建了主键的聚集索引。 而普通索引（非聚集索引）的语法，大多数数据库都是通用的 聚集索引 该索引中键值的逻辑顺序决定了表中相应行的物理顺序。 聚集索引确定表中数据的物理顺序。聚集索引类似于电话簿，后者按姓氏排列数据。由于聚集索引规定数据在表中的物理存储顺序，因此一个表只能包含一个聚集索引。但该索引可以包含多个列（组合索引），就像电话簿按姓氏和名字进行组织一样。 主键索引：主键是一种唯一索引，但必须指定为PRIMARY KEY。 非聚集索引 该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同。 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚簇索引：索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块。 普通索引：这是最基本的索引类型，没唯一性之类的限制。 唯一索引：和普通索引基本相同，但所有的索引列值保持唯一性。 全文索引：MYSQL从3.23.23开始支持全文索引和全文检索。在MYSQL中，全文索引的索引类型为FULLTEXT。全文索引可以在VARCHAR或者TEXT类型的列上创建。 大多数MySQL索引(PRIMARY KEY、UNIQUE、INDEX和FULLTEXT)使用平衡树中存储。空间列类型的索引使用R-树，MEMORY表支持hash索引。 单列索引和多列索引（复合索引） 索引可以是单列索引，也可以是多列索引。对相关的列使用索引是提高SELECT操作性能的最佳途径之一。 多列索引： MySQL可以为多个列创建索引。一个索引可以包括15个列。对于某些列类型，可以索引列的左前缀，列的顺序非常重要。 多列索引可以视为包含通过连接索引列的值而创建的值的排序的数组。一般来说，即使是限制最严格的单列索引，它的限制能力也远远低于多列索引。 最左前缀： 多列索引有一个特点，即最左前缀（Leftmost Prefixing）。假如有一个多列索引为key(firstname lastname age)，当搜索条件是以下各种列的组合和顺序时，MySQL将使用该多列索引： firstname，lastname，age firstname，lastname firstname 也就是说，相当于还建立了key(firstname lastname)和key(firstname)。 索引主要用于下面的操作： 快速找出匹配一个WHERE子句的行。 删除行。当执行联接时，从其它表检索行。 对具体有索引的列key_col找出MAX()或MIN()值。由预处理器进行优化，检查是否对索引中在key_col之前发生所有关键字元素使用了WHERE key_part_# = constant。在这种情况下，MySQL为每个MIN()或MAX()表达式执行一次关键字查找，并用常数替换它。如果所有表达式替换为常量，查询立即返回。例如： SELECT MIN(key2), MAX (key2) FROM tb WHERE key1=10; 如果对一个可用关键字的最左面的前缀进行了排序或分组(例如，ORDER BY key_part_1,key_part_2)，排序或分组一个表。如果所有关键字元素后面有DESC，关键字以倒序被读取。 在一些情况中，可以对一个查询进行优化以便不用查询数据行即可以检索值。如果查询只使用来自某个表的数字型并且构成某些关键字的最左面前缀的列，为了更快，可以从索引树检索出值。 1SELECT key_part3 FROM tb WHERE key_part1=1 有时MySQL不使用索引，即使有可用的索引。一种情形是当优化器估计到使用索引将需要MySQL访问表中的大部分行时。(在这种情况下，表扫描可能会更快些）。然而，如果此类查询使用LIMIT只搜索部分行，MySQL则使用索引，因为它可以更快地找到几行并在结果中返回。 合理的建立索引 (1) 越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快。 (2) 简单的数据类型更好：整型数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应该用内置的日期和时间数据类型，而不是用字符串来存储时间；以及用整型数据类型存储IP地址。 (3) 尽量避免NULL：应该指定列为NOT NULL，除非你想存储NULL。在MySQL中，含有NULL的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。应该用0、一个特殊的值或者一个空串代替NULL (4) 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描 (5) 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num = 0 (6) 值分布很稀少的字段不适合建索引，例如”性别”这种只有两三个值的字段 (7) 字符字段只建前缀索引 (8) 并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 (9) 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 (10) 使用聚集索引的查询效率要比非聚集索引的效率要高，但是如果需要频繁去改变聚集索引的值，写入性能并不高，因为需要移动对应数据的物理位置。 (11) 非聚集索引在查询的时候可以的话就避免二次查询，这样性能会大幅提升。 (12) 不是所有的表都适合建立索引，只有数据量大表才适合建立索引，且建立在选择性高的列上面性能会更好。 字段 尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED VARCHAR的长度只分配真正需要的空间 使用枚举或整数代替字符串类型 尽量使用TIMESTAMP而非DATETIME， 单表不要有太多字段，建议在20以内 避免使用NULL字段，很难查询优化且占用额外索引空间 用整型来存IP SQL语句注意点 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num = 10 or num = 20,用union all（union 和union all区别，尽量用union all） select id from t where num = 10 union all select id from t where num = 20 下面的查询也将导致全表扫描：select id from t where name like ‘%abc%’若要提高效率，可以考虑全文检索。 in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：select id from t where num = @num 可以改为强制查询使用索引： select id from t with(index(索引名)) where num = @num 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。如：select id from t where num / 2 = 100 select id from t where substring(name, 1 ,3) = ’abc’查询name以abc开头的id列表 分别应改为: select id from t where num = 100 * 2 select id from t where name like ‘abc%’ 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 很多时候用 exists 代替 in 是一个好的选择：select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num = a.num) 当结果集只有一行数据时使用LIMIT 1 避免SELECT *，始终指定你需要的列 从表中读取越多的数据，查询会变得更慢。他增加了磁盘需要操作的时间，还是在数据库服务器与WEB服务器是独立分开的情况下。你将会经历非常漫长的网络延迟，仅仅是因为数据不必要的在服务器之间传输。 使用连接JOIN来代替子查询Sub-Queries 连接JOIN.. 之所以更有效率一些，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。 使用ENUM、CHAR 而不是VARCHAR，使用合理的字段属性长度 尽可能的使用NOT NULL 固定长度的表会更快 拆分大的DELETE 或INSERT 语句如果你需要在一个在线的网站上去执行一个大的 DELETE 或 INSERT 查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。Apache 会有很多的子进程或线程。所以，其工作起来相当有效率，而我们的服务器也不希望有太多的子进程，线程和数据库链接，这是极大的占服务器资源的事情，尤其是内存。如果你把你的表锁上一段时间，比如30秒钟，那么对于一个有很高访问量的站点来说，这30秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你泊WEB服务Crash，还可能会让你的整台服务器马上掛了。所以，如果你有一个大的处理，你定你一定把其拆分，使用 LIMIT 条件是一个好的方法。下面是一个示例：DELETE FROM logs WHERE log_date &lt;= &#39;2009-11-01&#39; LIMIT 1000; 查询的列越小越快 永远为每张表设置一个ID我们应该为数据库里的每张表都设置一个ID做为其主键，而且最好的是一个INT型的(推荐使用UNSIGNED)，并设置上自动增加的AUTO_INCREMENT标志。就算是你 users 表有一个主键叫 “email”的字段，你也别让它成为主键。使用 VARCHAR 类型来当主键会使用得性能下降。另外，在你的程序中，你应该使用表的ID来构造你的数据结构。 使用 ENUM 而不是 VARCHARENUM 类型是非常快和紧凑的。在实际上，其保存的是 TINYINT，但其外表上显示为字符串。这样一来，用这个字段来做一些选项列表变得相当的完美。如果你有一个字段，比如“性别”，“国家”，“民族”，“状态”或“部门”，你知道这些字段的取值是有限而且固定的，那么，你应该使用 ENUM 而不是 VARCHAR。 把IP地址存成 UNSIGNED INT很多程序员都会创建一个 VARCHAR(15) 字段来存放字符串形式的IP而不是整形的IP。如果你用整形来存放，只需要4个字节，并且你可以有定长的字段。而且，这会为你带来查询上的优势，尤其是当 你需要使用这样的WHERE条件：IP between ip1 and ip2。我们必需要使用UNSIGNED INT，因为 IP地址会使用整个32位的无符号整形。而你的查询，你可以使用 INET_ATON() 来把一个字符串IP转成一个整形，并使用 INET_NTOA() 把一个整形转成一个字符串IP。在PHP中，也有这样的函数 ip2long() 和 long2ip()。UPDATE users SET ip = INET_ATON(&#39;{$_SERVER[&#39;REMOTE_ADDR&#39;]}&#39;) WHERE ... 垂直分割“垂直分割”是一种把数据库中的表按列变成几张表的方法，这样可以降低表的复杂度和字段的数目，从而达到优化的目的。(以前，在银行做过项目，见过一张表有100多个字段，很恐怖)示例一：在Users表中有一个字段是家庭地址，这个字段是可选字段，相比起，而且你在数据库操作的时候除了个 人信息外，你并不需要经常读取或是改写这个字段。那么，为什么不把他放到另外一张表中呢? 这样会让你的表有更好的性能，大家想想是不是，大量的时候，我对于用户表来说，只有用户ID，用户名，口令，用户角色等会被经常使用。小一点的表总是会有 好的性能。示例二： 你有一个叫 “last_login” 的字段，它会在每次用户登录时被更新。但是，每次更新时会导致该表的查询缓存被清空。所以，你可以把这个字段放到另一个表中，这样就不会影响你对用户 ID，用户名，用户角色的不停地读取了，因为查询缓存会帮你增加很多性能。另外，你需要注意的是，这些被分出去的字段所形成的表，你不会经常性地去Join他们，不然的话，这样的性能会比不分割时还要差，而且，会是极数级的下降。 选择正确的存储引擎在 MySQL 中有两个存储引擎 MyISAM 和 InnoDB，每个引擎都有利有弊。酷壳以前文章《MySQL: InnoDB 还是 MyISAM?》讨论和这个事情。MyISAM 适合于一些需要大量查询的应用，但其对于有大量写操作并不是很好。甚至你只是需要update一个字段，整个表都会被锁起来，而别的进程，就算是读进程都 无法操作直到读操作完成。另外，MyISAM 对于 SELECT COUNT(*) 这类的计算是超快无比的。InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用，它会比 MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务。 Where条件 在查询中，WHERE条件也是一个比较重要的因素，尽量少并且是合理的where条件是很重要的，尽量在多个条件的时候，把会提取尽量少数据量的条件放在前面，减少后一个where条件的查询时间。 有些where条件会导致索引无效： where子句的查询条件里有！=，MySQL将无法使用索引。 where子句使用了Mysql函数的时候，索引将无效，比如：select * from tb where left(name, 4) = ‘xxx’ 使用LIKE进行搜索匹配的时候，这样索引是有效的：select * from tbl1 where name like ‘xxx%’，而like ‘%xxx%’ 时索引无效 原文参考1原文参考2 示例开发需要定期的删除表里一定时间以前的数据，SQL如下 1mysql &gt; delete from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; AND status = 2 limit 500\G; 前段时间在优化的时候，已经在相应的查询条件上加上了索引 1KEY `idx_bizdate_st` (`biz_date`,`status`) 但是实际执行的SQL依然非常慢，为什么呢，我们来一步步分析验证下 分析 表上的字段既然都有索引，那么按照之前的文章分析，是两个字段都可以走上索引的。如果有疑问，请参考文章 10分钟让你明白MySQL是如何利用索引的 既然能够利用索引，表的总大小也就是200M左右，那么为什么形成了慢查呢？ 我们查看执行计划，去掉limit 后，发现他选择了走全表扫描。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869mysql &gt; desc select * from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos;;+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+| 1 | SIMPLE | testtable | ALL | idx_bizdate_st | NULL | NULL | NULL | 980626 | Using where |+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+1 row in set (0.00 sec)-- 只查询biz_date-- 关键点：rows:980626;type:ALL mysql &gt; desc select * from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+| 1 | SIMPLE | testtable | ALL | idx_bizdate_st | NULL | NULL | NULL | 980632 | Using where |+----+-------------+-----------+------+----------------+------+---------+------+--------+-------------+1 row in set (0.00 sec)-- 查询biz_date + status -- 关键点：rows:980632;type:ALL mysql &gt; desc select * from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2 limit 100;+----+-------------+-----------+-------+----------------+----------------+---------+------+--------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+----------------+----------------+---------+------+--------+-----------------------+| 1 | SIMPLE | testtable | range | idx_bizdate_st | idx_bizdate_st | 6 | NULL | 490319 | Using index condition |+----+-------------+-----------+-------+----------------+----------------+---------+------+--------+-----------------------+1 row in set (0.00 sec)-- 查询biz_date + status+ limit -- 关键点：rows:490319; mysql &gt; select count(*) from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;+----------+| count(*) |+----------+| 0 |+----------+1 row in set (0.34 sec)mysql &gt; select count(*) from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos;;+----------+| count(*) |+----------+| 970183 |+----------+1 row in set (0.33 sec)mysql &gt; select count(*) from testtable;+----------+| count(*) |+----------+| 991421 |+----------+1 row in set (0.19 sec)mysql &gt; select distinct biz_status from whwtestbuffer;+------------+| biz_status |+------------+| 1 || 2 || 4 |+------------+ 通过以上查询，我们可以发现如下几点问题： 通过 biz_date 预估出来的行数 和 biz_date + status=2 预估出来的行数几乎一样，为98w。实际查询表 biz_date + status=2 一条记录都没有。整表数据量达到了99万，MySQL发现通过索引扫描需要98w行（预估）因此，MySQL通过统计信息预估的时候，发现需要扫描的索引行数几乎占到了整个表，放弃了使用索引，选择了走全表扫描。那是不是他的统计信息有问题呢？我们重新收集了下表统计信息，发现执行计划的预估行数还是一样，猜测只能根据组合索引的第一个字段进行预估（待确定） 那我们试下直接强制让他走索引呢？ 12345mysql &gt; select * from testtable WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;Empty set (0.79 sec)mysql &gt; select * from testtable force index(idx_bizdate_st) WHERE biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;Empty set (0.16 sec) 我们发现，强制指定索引后，查询耗时和没有强制索引比较，的确执行速度快了很多，因为没有强制索引是全表扫描嘛！但是！依然非常慢！ 那么还有什么办法去优化这个本来应该很快的查询呢？ 大家应该都听说过要选择性好的字段放在组合索引的最前面？是的，相对于status字段，biz_date 的选择性更加不错，那组合索引本身已经没有好调整了 那，能不能让他不要扫描索引的那么多范围呢？之前的索引模型中也说过，MySQL是通过索引去确定一个扫描范围，如果能够定位到尽可能小的范围，那是不是速度上会快很多呢？ 并且业务逻辑上是定期删除一定日期之前的数据。所以逻辑上来说，每次删除都是只删除一天的数据，直接让SQL扫描一天的范围。那么我们就可以改写SQL啦! 12345678910mysql &gt; select * from testtable WHERE biz_date &gt;= &apos;2017-08-20 00:00:00&apos; and biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;Empty set (0.00 sec)mysql &gt; desc select * from testtable WHERE biz_date &gt;= &apos;2017-08-20 00:00:00&apos; and biz_date &lt;= &apos;2017-08-21 00:00:00&apos; and status = 2;+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+| 1 | SIMPLE | testtable | range | idx_bizdate_st | idx_bizdate_st | 6 | NULL | 789 | Using index condition |+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+1 row in set (0.00 sec) rows降低了很多，乖乖的走了索引 1234567mysql &gt; desc select * from testtable WHERE biz_date &gt;= &apos;2017-08-20 00:00:00&apos; and biz_date &lt;= &apos;2017-08-21 00:00:00&apos; ;+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+| 1 | SIMPLE | testtable | range | idx_bizdate_st | idx_bizdate_st | 5 | NULL | 1318 | Using index condition |+----+-------------+------------------+-------+----------------+----------------+---------+------+------+-----------------------+1 row in set (0.00 sec) 即使没有status，也是肯定走索引啦 这个问题，我原本打算用hint，强制让他走索引，但是实际上强制走索引的执行时间并不能带来满意的效果。结合业务逻辑，来优化SQL，是最好的方式，也是终极法宝，一定要好好利用。]]></content>
      <categories>
        <category>database</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>index</tag>
        <tag>mysqldumpslow</tag>
        <tag>explain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java常用集合/容器]]></title>
    <url>%2F2017%2F05%2F28%2Fjava%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[引言Collection 顶级接口├ List 子接口 按进入先后有序保存 可重复│ ├ LinkedList 实现类 链表 插入删除 没有同步 线程不安全│ ├ ArrayList 实现类 动态数组 随机访问 没有同步 线程不安全│ └ Vector 实现类 老的动态数组 同步 线程安全│ └ Stack 实现类├ Set 子接口 仅接收一次，并做内部排序│ ├ SortedSet 子接口│ │ └NavigableSet 子接口│ │ └ TreeSet 实现类│ └ HashSet 实现类 以哈希表的形式存放元素，插入删除速度很快，元素无序但不允许重复│ └ LinkedHashSet└ Queue 子接口 队列 Map 顶级接口├ SortedMap 子接口 (extends Map)│ └ NavigableMap 子接口 (extends SortedMap)│ └ TreeMap 实现类 (extends AbstractMap)├ HashTable 实现类 (extends Dictionary(abstract)) 线程安全├ AbstractMap 实现类├ HashMap 实现类 (extends AbstractMap) 线程不安全│ └ LinkedHashMap 子类├ LinkedHashMap 实现类 (extends HashMap)├ IdentityHashMap 实现类 (extends AbstractMap)└ WeakHashMap 实现类 (extends AbstractMap) 关系图如下： 上图只列出了图中类的一些主要继承和实现关系，并不精确，只表示了大概的关系，有些父接口并没有在图中画出，以及有写继承关系跨过了中间的某些类，图中让它直接继承了间接父类。 java.util.Collection 是一个集合接口。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式。java.util.Collections 是一个包装类。它包含有各种有关集合操作的静态多态方法。此类不能实例化，就像一个工具类，服务于Java的Collection框架。就像Arrays服务于数组一样 Collection(集合，java编程思想中翻译为容器)检查Collection中的元素，可以使用foreach进行遍历，也可以使用迭代器，Collection支持iterator()方法，通过该方法可以访问Collection中的每一个元素。用法如下： 1234Iterator it=collection.iterator(); while(it.hasNext())&#123; Object obj=it.next(); &#125; 线程安全就是说多线程访问同一代码，不会产生不确定的结果。 Set和List是由Collection派生的两个子接口 对于 List ，关心的是顺序，它保证维护元素特定的顺序（允许有相同元素），使用此接口能够精确的控制每个元素插入的位置。用户能够使用索引（元素在 List 中的位置，类似于数组下标）来访问 List 中的元素。 对于 Set ，只关心某元素是否属于 Set （不 允许有相同元素 ），而不关心它的顺序。 Set：无序，不允许重复。检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。List：有序，可以有重复元素。和数组类似，List可以动态增长，查找元素效率高，插入删除元素效率低，因为会引起其他元素位置改变。 List接口List是有序的Collection，使用此接口能够精确的控制每个元素插入的位置。用户能够使用索引的位置来访问List中的元素，类似于Java数组。List允许有相同的元素存在。除了具有Collection接口必备的的iterator()方法外，还提供了listIterator()方法，返回一个 ListIterator接口。和标准的 Iterator 接口相比， ListIterator 多了一些 add()之类的方法，允许添加，删除，设定元素， 还能向前或向后遍历。实现List接口的常用类有LinkedList、ArrayList、Vector LinkedList类LinkedList实现了List类接口，允许null元素。此外LinkedList提供额外的addFirst(), addLast(), getFirst(), getLast(), removeFirst(), removeLast(), insertFirst(), insertLast()方法在 LinkedList 的首部或尾部，这些方法（没有在任何接口或基类中定义过）使 LinkedList 可被用作堆栈（ stack ），队列（ queue ）或双向队列（ deque ）。 Queue接口定义了队列存取元素的基本方法，基于LinkedList的实现原理，它非常适合队列的特性。队列，遵循先进先出(FIFO)原则.1&gt;.Queue常用方法offer() 向队列末尾追加新元素poll() 获取队首元素，获取后该元素就从队列中被删除peek() 获取队首元素，但不从队列中删除 2.Deque 双端队列，两端都可以进出队但当我们约束从队列的一端进出队时，就形成了另一种存取模式先进后出(FILO)原则，这就是栈结构1&gt;.Deue常用方法push() 压入元素pop() “弹出”栈顶元素（注意，当栈中没有元素的时候pop方法会引发异常） LinkedList没有同步方法。如果多个线程想访问同一个List，则必须自己实现访问同步。一种解决办法是在创建List时构造一个同步的List： 1List list = Collections.synchronizedList(new LinkedList&lt;&gt;()); AyyayList类ArrayList实现了可变大小的数组。它允许所有元素，包括null。ArrayList没有同步。size(),isEmpty(),get(),set()方法运行时间为常数。但是add()方法开销为分摊的常数，添加n个元素需要O(n)的时间。其他的方法运行时间为线性。每个ArrayList实例都有一个容量（Capactity），即用于存储元素的数组的大小。这个容量可随着不断添加新元素而自动增加，但是增长算法并没有定义。当需要插入大量元素时，在插入之前可以调用ensureCapacity()方法来增加ArrayList容量已提高插入效率 Array（[]）：最高效；但是其容量固定且无法动态改变；ArrayList： 容量可动态增长；但牺牲效率； 基于效率和类型检验，应尽可能使用Array，无法确定数组大小时才使用ArrayList！不过当你试着解决更一般化的问题时，Array的功能就可能过于受限。 数组扩容是对ArrayList效率影响比较大的一个因素。每当执行Add、AddRange、Insert、InsertRange等添加元素的方法，都会检查内部数组的容量是否不够了，如果是，它就会以当前容量的两倍来重新构建一个数组，将旧元素Copy到新数组中，然后丢弃旧数组，在这个临界点的扩容操作，应该来说是比较影响效率的。 java中ArrayList（默认size =10），超过容量时才会进行扩容：((旧容量 * 3) / 2) + 1 Collections.sort(list)方法做自然排序，默认为升序 实现Comparator接口，自定义为降序 12345678class MyComparator implements Comparator&lt;String&gt;&#123; /** * 根据字符串的长度进行比较,降序 */ public int compare(String o1, String o2) &#123; return o2.length() - o1.length(); //如果大于返回true，即先找大的 &#125; &#125; 12345MyComparator comparator = new MyComparator(); /* * 使用Collections的重载sort方法 */ Collections.sort(list, comparator); ArrayList和LinkedListArrayList：允许对元素进行快速随机访问，但是向 List 中间插入与移除元素的速度很慢。 ListIterator 只应该用来由后向前遍历 ArrayList, 而不是用来插入和移除元素。因为那比 LinkedList 开销要大很多。LinkedList：对顺序访问进行了优化，向 List 中间插入与删除的开销并不大。随机访问则相对较慢。 1． 对ArrayList和LinkedList而言，在列表末尾增加一个元素所花的开销都是固定的。对ArrayList而言，主要是在内部数组中增加一 项，指向所添加的元素，偶尔可能会导致对数组重新进行分配；而对LinkedList而言，这个开销是统一的，分配一个内部Entry对象。 2．在ArrayList的中间插入或删除一个元素意味着这个列表中剩余的元素都会被移动；而在LinkedList的中间插入或删除一个元素的开销是固定的。 3．LinkedList不支持高效的随机元素访问。 4．ArrayList的空间浪费主要体现在在list列表的结尾预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗相当的空间 可以这样说：当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList了。 Vector类Vector非常类似ArrayList，但是Vector是同步的。由Vector创建的iterator，虽然和ArrayLsit创建的iterator是同一接口，但是，因为Vector是同步的，当一个iterator被创建而且正在被使用，另一个线程改变了Vector状态，这时调用iterator的方法时将抛出ConcurrentModificationException，因此必须捕获该异常。 比ArrayList多了枚举遍历1234567//Enumeration遍历 Integer value=null;Enumeration enu=vector.elements();while(enu.hasMoreElements())&#123;value=(Integer)enu.nextElement();&#125; Stack类Stack继承自Vector，实现了一个后进先出(LIFO)的堆栈。Stack提供了5个额外的方法使得Vector得以被当做堆栈使用。基本的push和pop方法，还有peek方法得到栈顶的元素，empty方法测试堆栈是否为空，serach方法检测一个元素在堆栈中的位置。Stack刚创建后是空栈。 Set接口Set是一种不包含重复元素的Collection，即任意的两个元素e1和e2都有e1.equals(e2)=false,Set最多有一个null元素。很明显，Set的构造函数有一个约束条件，传入的Collection参数不能包含重复的元素。请注意：必须小心操作可变对象。如果一个Set中的可变元素改变了自身的状态导致Object.equals(Object)=true将导致一些问题 HashSetHashSet调用对象的hashCode(),获得哈希码，然后在集合中计算存放对象的位置(bucket)。通过比较哈希码与equals()方法来判别是否重复。所以，重载了equals()方法同时也要重载hashCode(); SortedSetTreeSetTreeSet 继承自SortedSet接口，二叉树实现，能够对集合中对象排序。默认排序方式是自然排序即升序，但该方式只能对实现了Comparable接口的对象排序，java中对Integer、Byte、Double、Character、String等数值型和字符型对象都实现了该接口。 Map接口Map没有继承Collection接口，Map提供key到value的映射。一个Map中不能包含相同的key，每个key只能映射一个value,所以是用键来索引值。 方法 put(Object key, Object value) 添加一个“值” ( 想要得东西 ) 和与“值”相关联的“键” (key) ( 使用它来查找 ) 。方法 get(Object key) 返回与给定“键”相关联的“值”。 可以用 containsKey() 和 containsValue() 测试 Map 中是否包含某个“键”或“值”。 Map 同样对每个元素保存一份，但这是基于 “ 键 “ 的， Map 也有内置的排序，因而不关心元素添加的顺序。如果添加元素的顺序对你很重要，应该使用 LinkedHashSet 或者 LinkedHashMap. LinkedHashMap: 是根据元素增加或者访问的先后顺序进行排序，而 TreeMap 则根据元素的 Key 进行排序 (由 Comparator 或者 Comparable 确定)。 LinkedHashSet:底层数据结构由哈希表和链表组成,哈希表保证元素的唯一性,链表保证元素有素。(存储和取出是一致) 对于效率， Map 由于采用了哈希散列，查找元素时明显比 ArrayList 快。看看 get() 要做哪些事，就会明白为什么在 ArrayList 中搜索“键”是相当慢的。而这正是 HashMap 提高速度的地方。 HashMap 使用了特殊的值，称为“散列码” (hash code) ，来取代对键的缓慢搜索。“散列码”是“相对唯一”用以代表对象的 int 值，它是通过将该对象的某些信息进行转换而生成的（在下面总结二：需要的注意的地方有更进一步探讨）。所有 Java 对象都能产生散列码，因为 hashCode()是定义在基类 Object 中的方法 。 HashMap 就是使用对象的 hashCode() 进行快速查询的,显著提高性能。 Map接口提供了3种集合的视图:Map的内容可以被当作一组key集合，一组value集合，或者一组key—value映射。 Map 中元素，可以将 key 序列、 value 序列单独抽取出来。 使用 keySet() 抽取 key 序列，将 map 中的所有 keys 生成一个 Set 。 使用 values() 抽取 value 序列，将 map 中的所有 values 生成一个 Collection 。 为什么一个生成 Set ，一个生成 Collection ？那是因为， key 总是独一无二的， value 允许重复。 HashTable类HashTable继承自Map接口，同步，是线程安全的，实现了一个key--value映射的哈希表。任何非空的对象都可作为key或者value。添加数据使用put(key,value),取出数据使用get(key)，这两个基本操作的时间开销为常数。HashTable通过initial caoacity和load factor两个参数调整性能。通常缺省的load factor为 0.75较好地实现了时间和空间的均衡。增大了load factor可以节省空间但相应的查找时间将增大，这会影响像get和put这样的操作 HashMap类HashMap和HashTable类似，不同之处在于HashMap是非同步的，并且允许null，即null value和null key，但是将HashMap视为Collection时，其迭代操作时间开销和HahMap的容量成比例。因此，如果迭代操作的性能相当重要的话，不要将HashMap的初始化容量设的过高，或者load factor过低 Capacity 容量: HashMap中数组的长度Initial capacity 初始容量:创建HashMap时，数组的默认大小，16。 size 大小:HashMap中的元素数量 load factor 加载因子:默认值0.75是一个比值 size/capacity 每当size/capacity达到加载因子时，数组扩容并且对原数组中的数据重新进行散列算法并存入扩容后的数组中 HashMap和HashTablea.HashMap去掉了HashTable的contains方法，但是加上了containsValue()和containsKey()方法。b.HashTable同步的，而HashMap是非同步的，效率上比HashTable要高。c.HashMap允许空键值，而HashTable不允许。 由于作为 key 的对象将通过计算其散列函数来确定与之对应的 value 的位置，因此任何作为 key 的对象都必须实现 hashCode 方法和 equals 方法。 hashCode 方法和 equals 方法继承自根类 Object ，如果你用自定义的类当作 key 的话，要相当小心，按照散列函数的定义，如果两个对象相同，即obj1.equals(obj2)=true，则它们的 hashCode 必须相同，但如果两个对象不同，则它们的 hashCode 不一定不同，如果两个不同对象的 hashCode 相同，这种现象称为冲突，冲突会导致操作哈希表的时间开销增大，所以尽量定义好的 hashCode() 方法，能加快哈希表的操作。如果相同的对象有不同的 hashCode ，对哈希表的操作会出现意想不到的结果（期待的 get 方法返回null ），要避免冲突问题，只需要牢记一条：要同时复写 equals 方法和 hashCode 方法，而不要只写其中一个。 LinkedHashMap 类类似于 HashMap ，但是迭代遍历它时，取得“键值对”的顺序是其插入次序，或者是最近最少使用 (LRU) 的次序。只比 HashMap 慢一点。而在迭代访问时发而更快，因为它使用链表维护内部次序。 WeakHashMap类WeakHashMap是一种改进的HashMap，他对key实行弱引用，如果一个key不再被外部所引用，那么该key可以被GC回收 SortedMapNavigableMapTreeMap基于红黑树数据结构的实现。查看“键”或“键值对”时，它们会被排序 ( 次序由 Comparabel 或 Comparator 决定 ) 。 TreeMap 的特点在于，你得到的结果是经过排序的。 TreeMap 是唯一的带有 subMap() 方法的 Map ，它可以返回一个子树。 IdentifyHashMap 类使用 == 代替 equals() 对“键”作比较的 hash map 。专为解决特殊问题而设计。 线程安全集合类与非线程安全集合类LinkedList、ArrayList、HashSet是非线程安全的，Vector是线程安全的;HashMap是非线程安全的，HashTable是线程安全的;StringBuilder是非线程安全的，StringBuffer是线程安全的。 集合适用场景对于查找和删除较为频繁，且元素数量较多的应用，Set或Map是更好的选择；ArrayList适用于通过位置来读取元素的场景；LinkedList 适用于要头尾操作或插入指定位置的场景；Vector 适用于要线程安全的ArrayList的场景；Stack 适用于线程安全的LIFO场景；HashSet 适用于对排序没有要求的非重复元素的存放；TreeSet 适用于要排序的非重复元素的存放；HashMap 适用于大部分key-value的存取的快速查找场景；TreeMap 适用于需排序存放的key-value场景。 当元素个数固定，用 Array ，因为 Array 效率是最高的。 如果程序在单线程环境中，或者访问仅仅在一个线程中进行，考虑非同步的类，其效率较高，如果多个线程可能同时操作一个类，应该使用同步的类。 要特别注意对哈希表的操作，作为 key 的对象要同时正确复写 equals 方法和 hashCode 方法。 尽量返回接口而非实际的类型，如返回 List 而非 ArrayList ，这样如果以后需要将 ArrayList 换成 LinkedList 时，客户端代码不用改变。这就是针对抽象编程。 数组与集合数组Java 所有“存储及随机访问一连串对象”的做法，数组是最有效率的一种。但缺点是容量固定且无法动态改变。 array 还有一个缺点是，无法判断其中实际存有多少元素， length 只是告诉我们 array 的容量。 Java 中有一个数组类 (Arrays) ，专门用来操作 Array([]) 。数组类 (Arrays) 中拥有一组 static 函数： equals() ：比较两个 array 是否相等。 array 拥有相同元素个数，且所有对应元素两两相等。 fill() ：将值填入 array 中。 sort() ：用来对 array 进行排序。 binarySearch() ：在排好序的 array 中寻找元素。 System.arraycopy() ： array 的复制。 如下：12345678int[] array = new int[5];//填充数组Arrays.fill(array, 5);System.out.println(&quot;填充数组：Arrays.fill(array, 5)：&quot;);if (array!=null) &#123;for (int i = 0; i &lt; array.length; i++) &#123;System.out.print(array[i]+&quot; &quot;);&#125; 另外该类还提供了一个静态方法，将数组转换成List：Arrays.asList(T ... array); 若编写程序时不知道究竟需要多少对象，需要在空间不足时自动扩增容量，则需要使用容器类库， array 不适用。 联系与区别容器类仅能持有对象引用（指向对象的指针），而不是将对象信息 copy 一份至数列某位置。一旦将对象置入容器内，便损失了该对象的型别信息。 list/set类型，每个位置只有一个元素。 Map 类型，持有 key-value 对 (pair) ，像个小型数据库。 Collections 是针对集合类的一个帮助类。提供了一系列静态方法实现对各种集合的搜索、排序、线程完全化等操作。相当于对 Array 进行类似操作的类—— Arrays 。 如Collections.max(Collection coll); 取 coll 中最大的元素。 Collections.sort(List list); 对 list 中元素排序 vector容器确切知道它所持有的对象隶属什么型别,vector 不进行边界检查。 HashMap 会利用对象的 hashCode 来快速找到 key 。 哈希码 (hashing) 就是将对象的信息经过一些转变形成一个独一无二的 int 值，这个值存储在一个 array 中。我们都知道所有存储结构中， array 查找速度是最快的。所以，可以加速查找。发生碰撞时，让 array 指向多个 values 。即，数组每个位置上又生成一个链表。详细参见hashmap的工作原理 同步的集合类/容器类（Synchronized）：一般的集合类和容器类都是非同步的（除了Vector和Hashtable以及它们的子类），如果用在多线程环境下必须自己写有关同步的代码。JDK中为我们提供了与一般集合容器类相应的同步类，SynchronizedCollection&lt;E&gt;、SynchronizedList&lt;E&gt;、SynchronizedMap&lt;E&gt;、SynchronizedSet&lt;E&gt;、SynchronizedSortedSet&lt;E&gt;、SynchronizedMap&lt;E&gt;，这些类都是Collections类中的静态内部类，通过Collections.synchronizedXXX()方法就可以获得相关的同步集合容器类，并且得到的同步类与作为参数的非同步类共享数据空间，当在同步类中对集合或者容器做出修改，在非同步的类中能同步感受到。例如： 12345List&lt; String&gt; list = new LinkedList&lt;&gt;(); list.add(&quot;one&quot;); List&lt;String&gt; list1 = Collections.synchronizedList(list); list1.remove(0); System.out.println(list.size()); 上述代码输出结果为0. 并发的集合类/容器类：1.同步集合或容器中，基本上在内部是使用synchronized来实现的，而且一个类中所有的方法使用的是同一个锁对象，在很多这会造成性能上的问题。另外，有一些复合操作，需要保持原子性，如获取最后一个元素（代码见下面），而同步类这时候就做不到了，所以这时候要考虑使用并发类了，并发类位于Java.util.concurrent中。 1234567/** *获取最后一个元素 */ public static Object getLast(Vector list) &#123; int lastIndex = list.size() - 1; return list.get(lastIndex); &#125; 2.并发容器有ConcurrentHashMap，它采用更细粒度的锁，被称为分段锁，允许一定数量的线程并发的修改它，能够提供高并发的性能。3.还有一些有用的适用于某些并发场合的集合类，CopyOnWriteArrayList等。4.在这里只要Java中存在同步集合容器和并发集合容器类就可以了，当遇到多线程的情况或者要求并发的情况下，再去找相关的合适的同步或并发的集合容器类。 原文参考]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>Collection</tag>
        <tag>Collections</tag>
        <tag>Map</tag>
        <tag>ArrayList</tag>
        <tag>Vector</tag>
        <tag>LinkedList</tag>
        <tag>HashSet</tag>
        <tag>TreeSet</tag>
        <tag>HashMap</tag>
        <tag>HashTable</tag>
        <tag>TreeMap</tag>
        <tag>LinkedHashMap</tag>
        <tag>WeakHashMap</tag>
        <tag>Arrays</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归和迭代]]></title>
    <url>%2F2017%2F05%2F27%2F%E9%80%92%E5%BD%92%E5%92%8C%E8%BF%AD%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[前言递归最大的有点就是把一个复杂的算法分解成若干相同的可重复的步骤。所以，使用递归实现一个计算逻辑往往只需要很短的代码就能解决，并且这样的代码也比较容易理解。但是，递归就意味着大量的函数调用。函数调用的局部状态之所以用栈来记录的。所以，这样就可能浪费大量的空间，如果递归太深的话还有可能导致堆栈溢出。 递归都可以用迭代来代替。但是相对于递归的简单易懂，迭代就比较生硬难懂了。尤其是遇到一个比较复杂的场景的时候。但是，代码的难以理解带来的优点也比较明显。迭代的效率比递归要高，并且在空间消耗上也比较小。 原则： 递归中一定有迭代，但是迭代中不一定有递归，大部分可以相互转换。能用迭代的不要用递归，递归调用函数不仅浪费空间，如果递归太深的话还容易造成堆栈的溢出。 递归程序调用自身的编程技巧称为递归,是函数自己调用自己. 一个函数在其定义中直接或间接调用自身的一种方法,它通常把一个大型的复杂的问题转化为一个与原问题相似的规模较小的问题来解决,可以极大的减少代码量.递归的能力在于用有限的语句来定义对象的无限集合. 使用递归要注意的有两点: 1)递归就是在过程或函数里面调用自身; 2)在使用递归时,必须有一个明确的递归结束条件,称为递归出口. 递归分为两个阶段: 1)递推:把复杂的问题的求解推到比原问题简单一些的问题的求解; 2)回归:当获得最简单的情况后,逐步返回,依次得到复杂的解. 利用递归可以解决很多问题:如阶乘,背包问题,汉诺塔问题,…等. 斐波那契数列为:0,1,1,2,3,5...12345678910111213141516fib(0)=0;fib(1)=1;fib(n)=fib(n-1)+fib(n-2); int fib(int n) &#123; if(0 == n) return 0; if(1 == n) return 1; if(n &gt; 1) return fib(n-1)+fib(n-2); &#125; 上面就是一个简单的递归调用了.由于递归引起一系列的函数调用,并且有可能会有一系列的重复计算,递归算法的执行效率相对较低. 虽然使用递归的方式会有冗余计算，可以用迭代来代替。但是这并不表明递归可以完全被取代。因为递归有更好的可读性。 使用迭代：12345678910int fib (int n) &#123; int fib = 0; int a = 1; for(int i=0; i&lt;n; i++) &#123; int temp = fib; fib = fib + a; a = temp; &#125; return fib;&#125; 迭代利用变量的原值推算出变量的一个新值.如果递归是自己调用自己的话,迭代就是A不停的调用B. 递归中一定有迭代,但是迭代中不一定有递归,大部分可以相互转换.能用迭代的不用递归,递归调用函数,浪费空间,并且递归太深容易造成堆栈的溢出.12345678910111213141516//这是递归 int funcA(int n) &#123; if(n &gt; 1) return n+funcA(n-1); else return 1; &#125; //这是迭代 int funcB(int n) &#123; int i,s=0; for(i=1;i&lt;n;i++) s+=i; return s; &#125; n! 1234567891011121314151617//递归int factorial (int n) &#123; if (n == 1) &#123; return 1; &#125; else &#123; return n*factorial(n-1); &#125;&#125; //迭代int factorial (int n) &#123; int product = 1; for(int i=2; i&lt;n; i++) &#123; product *= i; &#125; return product;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StringBuilder和StringBuffer]]></title>
    <url>%2F2017%2F05%2F27%2FStringBuilder%E5%92%8CStringBuffer%2F</url>
    <content type="text"><![CDATA[前言StringBuffer、StringBuilder和String一样，也用来代表字符串。String类是不可变类，任何对String的改变都 会引发新的String对象的生成； StringBuffer则是可变类，任何对它所指代的字符串的改变都不会产生新的对象。既然可变和不可变都有了，为何还有一个StringBuilder呢？相信初期的你，在进行append时，一般都会选择StringBuffer吧！先说一下集合的故事，HashTable是线程安全的，很多方法都是synchronized方法，而HashMap不是线程安全的，但其在单线程程序中的性能比HashTable要高。StringBuffer和StringBuilder类的区别也是如此，他们的原理和操作基本相同，区别在于StringBufferd支持并发操作，线性安全的，适合多线程中使用。StringBuilder不支持并发操作，线性不安全的，不适合多线程中使用。新引入的StringBuilder类不是线程安全的，但其在单线程中的性能比StringBuffer高。 String 字符串常量StringBuffer 字符串变量（线程安全）StringBuilder 字符串变量（非线程安全） 一般情况向性能比较：String &lt; StringBuffer &lt; StringBuilder 比较123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public class StringTest &#123; public static String BASEINFO = &quot;Mr.Y&quot;; public static final int COUNT = 2000000; /** * 执行一项String赋值测试 */ public static void doStringTest() &#123; String str = new String(BASEINFO); long starttime = System.currentTimeMillis(); for (int i = 0; i &lt; COUNT / 100; i++) &#123; str = str + &quot;miss&quot;; &#125; long endtime = System.currentTimeMillis(); System.out.println((endtime - starttime) + &quot; millis has costed when used String.&quot;); &#125; /** * 执行一项StringBuffer赋值测试 */ public static void doStringBufferTest() &#123; StringBuffer sb = new StringBuffer(BASEINFO); long starttime = System.currentTimeMillis(); for (int i = 0; i &lt; COUNT; i++) &#123; sb = sb.append(&quot;miss&quot;); &#125; long endtime = System.currentTimeMillis(); System.out.println((endtime - starttime) + &quot; millis has costed when used StringBuffer.&quot;); &#125; /** * 执行一项StringBuilder赋值测试 */ public static void doStringBuilderTest() &#123; StringBuilder sb = new StringBuilder(BASEINFO); long starttime = System.currentTimeMillis(); for (int i = 0; i &lt; COUNT; i++) &#123; sb = sb.append(&quot;miss&quot;); &#125; long endtime = System.currentTimeMillis(); System.out.println((endtime - starttime) + &quot; millis has costed when used StringBuilder.&quot;); &#125; /** * 测试StringBuffer遍历赋值结果 * * @param mlist */ public static void doStringBufferListTest(List&lt;String&gt; mlist) &#123; StringBuffer sb = new StringBuffer(); long starttime = System.currentTimeMillis(); for (String string : mlist) &#123; sb.append(string); &#125; long endtime = System.currentTimeMillis(); System.out.println(sb.toString() + &quot;buffer cost:&quot; + (endtime - starttime) + &quot; millis&quot;); &#125; /** * 测试StringBuilder迭代赋值结果 * * @param mlist */ public static void doStringBuilderListTest(List&lt;String&gt; mlist) &#123; StringBuilder sb = new StringBuilder(); long starttime = System.currentTimeMillis(); for (Iterator&lt;String&gt; iterator = mlist.iterator(); iterator.hasNext();) &#123; sb.append(iterator.next()); &#125; long endtime = System.currentTimeMillis(); System.out.println(sb.toString() + &quot;builder cost:&quot; + (endtime - starttime) + &quot; millis&quot;); &#125; public static void main(String[] args) &#123; doStringTest(); doStringBufferTest(); doStringBuilderTest(); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot; I &quot;); list.add(&quot; like &quot;); list.add(&quot; BeiJing &quot;); list.add(&quot; tian &quot;); list.add(&quot; an &quot;); list.add(&quot; men &quot;); list.add(&quot; . &quot;); doStringBufferListTest(list); doStringBuilderListTest(list); &#125; &#125; 看一下执行结果：123452711 millis has costed when used String.211 millis has costed when used StringBuffer.141 millis has costed when used StringBuilder.I like BeiJing tian an men . buffer cost:1 millisI like BeiJing tian an men . builder cost:0 millis 从上面的结果可以看出，不考虑多线程，采用String对象时（我把Count/100），执行时间比其他两个都要高，而采用StringBuffer对象和采用StringBuilder对象的差别也比较明显。由此可见，如果我们的程序是在单线程下运行，或者是不必考虑到线程同步问题，我们应该优先使用StringBuilder类；如果要保证线程安全，自然是StringBuffer。从后面List的测试结果可以看出，除了对多线程的支持不一样外，这两个类的使用方式和结果几乎没有任何差别， StringBuffer/StringBuilder（由于StringBuffer和StringBuilder在使用上几乎一样，所以只写一个，以下部分内容网络各处收集，不再标注出处）1StringBuffer s = new StringBuffer(); 这样初始化出的StringBuffer对象是一个空的对象，1StringBuffer sb1=new StringBuffer(512); 分配了长度512字节的字符缓冲区。1StringBuffer sb2=new StringBuffer(“how are you?”) 创建带有内容的StringBuffer对象，在字符缓冲区中存放字符串“how are you?” append方法该方法的作用是追加内容到当前StringBuffer对象的末尾，类似于字符串的连接，调用该方法以后，StringBuffer对象的内容也发生改变，例如：12StringBuffer sb = new StringBuffer(“abc”);sb.append(true); 则对象sb的值将变成”abctrue”使用该方法进行字符串的连接，将比String更加节约内容，经常应用于数据库SQL语句的连接。 deleteCharAt方法public StringBuffer deleteCharAt(int index)该方法的作用是删除指定位置的字符，然后将剩余的内容形成新的字符串。例如：12StringBuffer sb = new StringBuffer(“KMing”);sb. deleteCharAt(1); 该代码的作用删除字符串对象sb中索引值为1的字符，也就是删除第二个字符，剩余的内容组成一个新的字符串。所以对象sb的值变 为”King”。还存在一个功能类似的delete方法：public StringBuffer delete(int start,int end)该方法的作用是删除指定区间以内的所有字符，包含start，不包含end索引值的区间。例如：12StringBuffer sb = new StringBuffer(“TestString”);sb. delete (1,4);//[1,4)左开右闭 该代码的作用是删除索引值1(包括)到索引值4(不包括)之间的所有字符，剩余的字符形成新的字符串。则对象sb的值是”TString”。 insert方法public StringBuffer insert(int offset, boolean b)该方法的作用是在StringBuffer对象中插入内容，然后形成新的字符串。例如：12StringBuffer sb = new StringBuffer(“TestString”);sb.insert(4,false); 该示例代码的作用是在对象sb的索引值4的位置插入false值，形成新的字符串，则执行以后对象sb的值是”TestfalseString”。 reverse方法public StringBuffer reverse()该方法的作用是将StringBuffer对象中的内容反转，然后形成新的字符串。例如：12StringBuffer sb = new StringBuffer(“abc”);sb.reverse(); 经过反转以后，对象sb中的内容将变为”cba”。 setCharAt方法public void setCharAt(int index, char ch)该方法的作用是修改对象中索引值为index位置的字符为新的字符ch。例如：12StringBuffer sb = new StringBuffer(“abc”);sb.setCharAt(1,’D’); 则对象sb的值将变成”aDc”。 trimToSize方法public void trimToSize()该方法的作用是将StringBuffer对象的中存储空间缩小到和字符串长度一样的长度，减少空间的浪费，和String的trim()是一样的作用，不再举例。 length方法该方法的作用是获取字符串长度 ，不用再说了吧。 setlength方法该方法的作用是设置字符串缓冲区大小。12StringBuffer sb=new StringBuffer();sb.setlength(100); 如果用小于当前字符串长度的值调用setlength()方法，则新长度后面的字符将丢失。 sb.capacity方法该方法的作用是获取字符串的容量。12StringBuffer sb=new StringBuffer(“string”);int i=sb.capacity(); ensureCapacity方法该方法的作用是重新设置字符串容量的大小。12StringBuffer sb=new StringBuffer();sb.ensureCapacity(32); //预先设置sb的容量为32 getChars方法该方法的作用是将字符串的子字符串复制给数组。12345678910getChars(int start,int end,char chars[],int charStart); StringBuffer sb = new StringBuffer(&quot;I love You&quot;);int begin = 0;int end = 5;//注意ch字符数组的长度一定要大于等于begin到end之间字符的长度//小于的话会报ArrayIndexOutOfBoundsException//如果大于的话，大于的字符会以空格补齐char[] ch = new char[end-begin];sb.getChars(begin, end, ch, 0);System.out.println(ch); 结果：I lov]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>StringBuffer</tag>
        <tag>StringBuilder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-NexT主题搭建个人博客]]></title>
    <url>%2F2017%2F05%2F27%2FHexo-NexT%E4%B8%BB%E9%A2%98%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[linux下搭建hexo之next主题个人博客，仅供参考！有问题请留言，谢谢支持！ 所需环境 下载安装nodejs和git(for linux)，声明一下我的搭建环境centos6.8 nodejs安装 准备命令： yum -y install gcc make gcc-c++ openssl-devel wget 载源码及解压： wget http://nodejs.org/dist/v0.10.26/node-v0.10.26.tar.gz tar -zvxf node-v0.10.26.tar.gz 编译及安装： make &amp;&amp; make install 验证是否安装配置成功： node -v 安装Express开发框架 npm install -g express npm install -g express-generator express -t ejs newsproject npm install 新建成功在nodesj目录下会生成newproject目录，其目录下大致有以下文件 bin 相关运行脚本 public 静态资源 routes 路由表 views 试图模板 app.js 视图文件夹 packge.json 项目依赖说明 启动项目 npm start 在浏览器访问http://127.0.0.1:3000/ 测试出现下面图视表示nodejs安装成功，并可以正常部署项目 node -v npm -v git安装 在线源安装 yum install git 安装Hexo命令解释 常用命令: hexo help #查看帮助 hexo init #初始化一个目录 hexo new &quot;postName&quot; #新建文章 hexo new page &quot;pageName&quot; #新建页面 hexo generate #生成网页, 可以在 public 目录查看整个网站的文件 hexo server #本地预览, &#39;Ctrl+C&#39;关闭 hexo deploy #部署.deploy目录 hexo clean #清除缓存, 强烈建议每次执行命令前先清理缓存, 每次部署前先删除 .deploy 文件夹 复合命令: hexo deploy -g #生成加部署 hexo server -g #生成加预览 简写： hexo n == hexo new hexo g == hexo generate hexo s == hexo server hexo d == hexo deploy 安装插件, plugin-name为插件名 npm install plugin-name --save #安装 npm update #升级 npm uninstall plugin-name #卸载 安装主题, repository为主题的 git 仓库, 为要存放在本地的目录名 git clone repository themes/theme-name 修改网站配置文件 theme: theme-name 在线源安装 npm install -g hexo-cli #g代表全局,npm默认为当前项目安装 hexo init #hexo初始化 npm install #安装依赖包 hexo g #生成静态资源生成public文件夹(浏览器访问资源) hexo s #启动服务 在浏览器访问：http://localhost:4000/ 便可查看hexo默认的主题 clone NexT主题,在blog根目录执行命令 git clone https://github.com/iissnan/hexo-theme-next themes/next clone完成后打开blog下的themes文件夹就可以看到有两个主题，一个是默认的，一个是刚刚clone的NexT主题站点配置文件_config.yml主题配置文件themes/next/_config.yml在站点配置文件_config.yml中进行搜索key为’theme’，如果没有theme就添加key为 theme，其值为next #Extensions ##Plugins: https://hexo.io/plugins/ ##Themes: https://hexo.io/themes/ #theme: landscape theme: next 需要注意的是theme: next ，冒号后面有一个空格 hexo clean #清除静态资源 hexo g hexo s 在浏览器中访问http://localhost:4000/ 便可看到效果 打开主题配置文件_config.yml找到Schemes，如下所示有三种方案可选，只需将#去掉就可以了，可以都尝试一下，选择喜欢的风格，修改之后不用重启服务直接刷新浏览器就能看到效果 #切换样式 #Schemes #scheme: Muse #scheme: Mist scheme: Pisces 到此本地资源基本上完成了，下面贴出站点配置_config.yml和主题配置themes/next/_config.yml里面注释了信息仅供参考 全局配置 _config.yml，配置文件的冒号”:”后面有空格 # Site #站点信息 title: Hank subtitle: Hank&#39;s Blog description: 关注WEB前端，前端开发 author: hank author_title: &#39;Web Developer &amp; Designer&#39; avatar: css/img/avatar.png location: &#39;Beijing, China&#39; follow: https://github.com/huangjihua/ language: zh-CN since: 2015 timezone: Asia/Beijing #时区 # URL #链接格式 url: http://blog.huangjihua.com #网址 root: / #根目录 permalink: post/:title.html #文章的链接格式 permalink_defaults: # Directory #目录 source_dir: source #源文件 public_dir: public #生成的网页文件 tag_dir: tags #标签 archive_dir: archives #归档 category_dir: categories #分类 code_dir: downloads/code i18n_dir: :lang #国际化 skip_render: # Writing #写作 new_post_name: :title.md #新文章标题 default_layout: post #默认模板(post page photo draft) titlecase: false #标题转换成大写 external_link: true #新标签页里打开连接 filename_case: 0 render_drafts: false post_asset_folder: false relative_link: false future: true highlight: #语法高亮 enable: true line_number: false #显示行号 auto_detect: true tab_replace: # Category &amp; Tag #分类和标签 default_category: uncategorized #默认分类 category_map: tag_map: # Date / Time format #日期时间格式 ## http://momentjs.com/docs/#/displaying/format/ date_format: YYYY-MM-DD time_format: HH:mm:ss # Pagination #分页 per_page: 20 #每页文章数, 设置成 0 禁用分页 pagination_dir: page # Extensions #插件和主题 ## 插件: http://hexo.io/plugins/ ## 主题: http://hexo.io/themes/ theme: next # Deployment #部署, huangjihua是我的用户名, 同时发布在 GitHub 和 GitCafe 上面 deploy: type: git repository: github: https://github.com/huangjihua/huangjihua.github.io.git,master gitcafe: https://gitcafe.com/huangjihua/huangjihua.git,master # Disqus #Disqus评论系统 disqus_shortname: plugins: #插件，例如生成 RSS 和站点地图的 - hexo-generator-feed - hexo-generator-sitemap # Assets css: css js: js images: images # Theme version version: 5.0.1 # Donate 文章末尾显示打赏按钮 reward_comment: 如果文章对您有用请随意打赏，谢谢支持！ wechatpay: /img/w.png alipay: /img/z.jpg 菜单栏 菜单栏为中文简体字，主题配置文件 language: zh-Hans 这样就引用了themes\next\languages\zh-Hans.yml文件，打开这个文件就可以看每个key对应的都是中文字体 标签和分类 在主题配置文件中找到menu，如果想隐藏菜单栏中的某个选项只要在前面加上 # 即可 menu: home: / categories: /categories #分类 archives: /archives #归档 tags: /tags #标签 message: /message #留言 about: /about #关于 # commonweal: /404.html #公益 我们将categories 和tags 设为显示,再找到menu_icons, 这里是每一个菜单选项前面对应的小图标icon menu_icons: enable: true #KeyMapsToMenuItemKey: NameOfTheIconFromFontAwesome home: home categories: th tags: tags archives: archive commonweal: heartbeat message: external-link about: user 我们将categories 和tags 设为显示,生成categories 和 tags的页面 hexo n page &quot;categories&quot; hexo n page &quot;tags&quot; 会生成source\categories 和 source\tags 两个文件夹，里面都有index.md文件.修改内容为 --- title: categories type: categories --- --- title: tags type: tags --- 再使用命令生成博文文件 hexo n &quot;name&quot; #name 文章名称 生成文章时，在对应的name.md中可以这样添加标签和分类 --- title: Hexo-NexT主题搭建个人博客 date: 2017-05-24 15:39:31 update: 2017-05-24 06:19:11 categories: hexo #分类 tags: [nodejs, hexo, next] #[标签1, 标签2..., 标签n] --- 网易云跟帖评论功能 主题配置文件 menu: home: / categories: /categories archives: /archives tags: /tags message: /message #新增 message about: /about #commonweal: /404.html 配置图标 menu_icons: enable: true home: home categories: th tags: tags archives: archive commonweal: heartbeat message: external-link #新增 message about: user 配置对应的中文名称，在themes/next/languages/zh-Hans.yml文件中修改如下 menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 message: 留言 #新增 留言 commonweal: 公益404 udpate: 更新 然后执行如下命令 hexo n page &quot;message&quot; 重新清除，生成和启动便可看到效果，只不过留言功能什么也没有 hexo clean hexo g hexo s 进入官网https://manage.gentie.163.com 进行注册、登录（可以用QQ）。登录完成在首页进入后台管理，填写基本信息（站点信息），获取代码找到 productKey的值，写入主题配置文件gentie_productKey处在主题配置文件中修改如下 # Gentie productKey gentie_productKey: your key 留言功能已完成去掉分类和其他的选项的留言功能，只需修改index.md在source/categories/index.md 中修改如下 --- title: categories type: categories comments: false #去掉评论功能 date: 2017-05-24 21:07:35 --- 修改头像 设置菜单栏头像把头像0.jpg图片放在themes/next/source/img中在站点的_config.yml，修改字段 avatar 设置头像 avatar: /img/0.jpg 可以指定网址图片url 侧边栏社交链接 侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址 ##Social links social: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 微博: http://weibo.com/your-user-name 豆瓣: http://douban.com/people/your-user-name 知乎: http://www.zhihu.com/people/your-user-name #Social Icons 设定链接的图标，对应的字段是 social_icons。其键值格式是 匹配键: Font Awesome 图标名称， 匹配键 与上一步所配置的链接的 显示文本 相同（大小写严格匹配），图标名称 是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标。 social_icons: enable: true #Icon Mappings GitHub: github Twitter: twitter 微博: weibo 友情链接 编辑 主题配置文件 添加： # Blogrolls links_title: 友情链接 links_layout: inline links_icon: link # 设置图标 links: 百度: http://www.baidu.com 文章配置开启阅读全文 修改主题配置文件 auto_excerpt: enable: true # 开启文章阅全文 length: 2000 # 显示长度 新增阅读量 为每一篇文章统计阅读量使用leancloud进行文章变更统计，进入官网https://leancloud.cn/ 注册打开LeanCloud官网，进入注册页面注册。完成邮箱激活后，点击头像，进入控制台页面创建新应用点击应用创建新应用，应用名称随便起创建Class文件进入到应用后点击存储，在左侧点击设置，选择”创建Class”，名称必须为Counter修改主题配置文件 ##文章阅读量 leancloud_visitors: enable: true app_id: **你的app_id** app_key: **你的app_key** 其中，app_id和app_key在你所创建的应用的设置-&gt;应用Key中修改 themes/next/layout_macro/post.swig 文件配置themes/next/layout/_layout.swig文件在最后div标签中查找是否引用了_scripts/third-party/lean-analytics.swig文件，如果没有增加以下代码修改语言配置文件 themes/next/languages/zh-Hans.yml post字段 post: sticky: 置顶 posted: 发表于 updated: 最近 update: 更新于 in: 分类于 visitors: 阅读量 read_more: 阅读全文 untitled: 未命名 toc_empty: 此文章未包含目录 其他语言与之类似，将visitors设置成你希望翻译的字段。最后，重新清除并生成你的网站即可。Web安全性为了保证应用的统计计数功能仅应用于自己的博客系统，你可以在应用-&gt;设置-&gt;安全中心的Web安全域名中加入自己的博客域名，以保证数据的调用安全。直接加上首页地址即可,保存三分之后生效，这时在本地方访问便不会统计。 文章最近更新时间 确保themes/next/layout_scripts/third-party/lean-analytics.swig文件已有update添加文章更新时间scaffolds/post.md 文件 --- title: {{ title }} date: {{ date }} categories: tags: update: {{ date }} # 新增更新时间 --- themes/next/layout_macro/post.swig 文件在每次新建文章时，默认更新时间就是发表时间，更新文章时需要手动修改udpate的值。例如source_posts\hexo之next主题.md --- title: Hexo之NexT主题搭建博客详细过程 date: 2017-05-24 15:39:31 update: 2017-05-24 15:39:31 #每次更新需手动修改成这样的格式时间 categories: hexo tags: [nodejs, hexo] --- 修改themes\next\languages\zh-Hans.yml post: sticky: 置顶 posted: 发表于 updated: 最近 update: 更新于 # 新增 in: 分类于 visitors: 阅读量 read_more: 阅读全文 untitled: 未命名 toc_empty: 此文章未包含目录 为文章增加分享 使用百度分享只修改主题配置文件即可 baidushare: type: button 为文章增加打赏 修改主题配置文件, 收款二维码放在themes/next/source/img 目录下 #Donate 文章末尾显示打赏按钮 reward_comment: 如果文章对您有用请随意打赏，谢谢支持！ wechatpay: /img/w.png alipay: /img/z.jpg 设置代码高亮主题 NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties：在主题配置文件中修改 # Code Highlight theme # Available value: # normal | night | night eighties | night blue | night bright # https://github.com/chriskempson/tomorrow-theme highlight_theme: night eighties # 修改即可 文章添加音乐链接 Hexo支持解析markdown语法，因此每篇博文都是以.md结尾的文件。而markdown又支持如表格、脚注、内嵌HTML等等，所以在.md文件中直接添加html代码！网音乐云音乐，虾米音乐都可以生成内嵌音乐的html代码，复制粘贴到.md文件中即可 &lt;div&gt; &lt;center&gt; &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;https://music.163.com/outchain/player?type=2&amp;id=33856282&amp;auto=0&amp;height=66&quot;&gt; &lt;/iframe&gt; &lt;/center&gt; &lt;/div&gt; 本地站内搜索 以前使用的Swiftype现在不能免费使用了，我这里就是用本地配置进行站内搜索安装 hexo-generator-search，在站点的根目录下执行以下命令: npm install hexo-generator-search --save 编辑 站点配置文件，新增以下内容到任意位置： search: path: search.xml field: post 不蒜子统计站点访次数和访问量 修改themes/next/layout/_partials目录下的footer.swig &lt;div class=&quot;total_count&quot;&gt; 本站共 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;&lt;span id=&quot;site_pv&quot;&gt;次访问&lt;/span&gt; 您是第 &lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;&lt;span id=&quot;site_uv&quot;&gt;个小伙伴&lt;/span&gt; 本页累计 &lt;span id=&quot;busuanzi_value_page_pv&quot;&gt;&lt;/span&gt;&lt;span id=&quot;page_pv&quot;&gt;次阅读&lt;/span&gt; &lt;span id=&quot;showDays&quot;&gt;&lt;/span&gt; &lt;/div&gt; {% endif %} 百度/google收录你的站点 安装sitemap插件 npm install hexo-generator-sitemap --save #google npm install hexo-generator-baidu-sitemap --save #百度 在站点配置文件_config.yml中添加如下代码： #自动生成sitemap sitemap: path: sitemap.xml baidusitemap: path: baidusitemap.xml 会在public目录下生成baidusitmap.xml 和 sitemap.xml两个文件安装hexo-deployer-git插件 npm install hexo-deployer-git --save #部署到github 在项目根目录下执行命令 hexo clean # 清除资源 hexo g # 生成静态资源 hexo d # 部署 登录在github打开存放博客资源的仓库，就能看到部署的资源文件分别向google,baidu提交站点地图sitemap.xml,baidusitmap.xml百度站长： http://zhanzhang.baidu.com/site/index输入你想添加的网站：你的博客首页地址然后按照提示选择验证方式，点击完成验证。 google站长： https://www.google.com/webmasters/tools/home?hl=zh-CN添加网站后，进行左侧抓取站点地图，添加/测试站点地图，填写sitemap.xml。sitemap.xml就在github存放博客 仓库的根目录下验证完成后大概过一天时间便可google到你的站点了，百度不定。 统计本站运行天数 修改blog/themes/next/layout_partials/footer.swig文件，在后面追加以下代码 &lt;script&gt; var birthDay = new Date(&#39;05/24/2016&#39;); var now = new Date(); var duration = now.getTime() - birthDay.getTime(); var total= Math.floor(duration / (1000 * 60 * 60 * 24)); document.getElementById(&#39;showDays&#39;).innerHTML=&#39;本站已运行&#39; + total + &#39;天&#39;; &lt;/script&gt; 在最后一个div中追加一下代码 &lt;span id=&quot;showDays&quot;&gt;&lt;/span&gt; 腾讯公益404页面 腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://www.ixirong.com/404.html 使用方法，新建 404.html 页面，放到主题的 source 目录下，内容如下： &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; hexo-admin后台管理博客按照官方的方式安装 hexo-adminhexo s启动后路径+/admin如：http://192.168.40.200:4005/admin/ 添加本地图片 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image —save，这是下载安装一个可以上传本地图片的插件. 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时(旧日志就自己手动去/source/_posts创建日志的同名文件夹即可)，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片： 12![你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。 最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是，而不是&lt;img src=”xxxx/图片名.jpg&gt;。这很重要，关乎你的网页是否可以真正加载你想插入的图片。 到此本地资源博客搭建已经完成，有些细微调节可根据浏览器控制台找到对应的模板进行调节，比如站点背景，代码字体字体颜色，页面宽度等等 托管到github 修改站点配置文件 # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repository: git@github.com:jethan/githug.git branch: master]]></content>
      <categories>
        <category>个人博客</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java后端开发面试]]></title>
    <url>%2F2017%2F05%2F07%2Fjava%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[在IT圈，跳槽是一件很平常的事情，干的不爽、钱没给到位、工作反复无趣没有提升等等因素。所以这里有必要说说面试，有些小型的创业公司可能没有那么多流程，面试官的差不多都是与你同级的，只是入职时间比你早而已。标准其实不复杂：第一能干活，第二Java基础要好，第三最好熟悉些分布式框架，我相信其它公司招初级开发时，应该也照着这个标准来面的。 我也知道，不少候选人能力其实不差，但面试时没准备或不会说，这样的人可能在进团队干活后确实能达到期望，但可能就无法通过面试，但面试官总是只根据面试情况来判断。 但现实情况是，大多数人可能面试前没准备，或准备方法不得当。要知道，我们平时干活更偏重于业务，不可能大量接触到算法，数据结构，底层代码这类面试必问的问题点，换句话说，面试准备点和平时工作要点匹配度很小。 本文里，将通过一些常用的问题来介绍面试的准备技巧。大家在看后一定会感叹：只要方法得当，准备面试第一不难，第二用的时间也不会太多。 框架是重点，但别让人感觉你只会山寨别人的代码在面试前，我会阅读简历以查看候选人在框架方面的项目经验，在候选人的项目介绍的环节，我也会着重关注候选人最近的框架经验，目前比较热门的是SSM。不过，一般工作在5年内的候选人，大多仅仅是能“山寨”别人的代码，也就是说能在现有框架的基础上，照着别人写的流程，扩展出新的功能模块。比如要写个股票挂单的功能模块，是会模仿现有的下单流程，然后从前端到后端再到数据库，依样画葫芦写一遍，最多把功能相关的代码点改掉。其实我们每个人都这样过来的，但在面试时，如果你仅仅表现出这样的能力，就和大多数人的水平差不多了，在这点就没法体现出你的优势了。我们知道，如果单纯使用SSM框架，大多数项目都会有痛点。比如数据库性能差，或者业务模块比较复杂，并发量比较高，用Spring MVC里的Controller无法满足跳转的需求。所以我一般还会主动问：你除了依照现有框架写业务代码时，还做了哪些改动？我听到的回答有：增加了Redis缓存，以避免频繁调用一些不变的数据。或者，在MyBitas的xml里，select语句where条件有isnull，即这个值有就增加一个where条件，对此，会对任何一个where增加一个不带isnull的查询条件，以免该语句当传入参数都是null时，做全表扫描。或者，干脆说，后端异步返回的数据量很大，时间很长，我在项目里就调大了异步返回的最大时间，或者对返回信息做了压缩处理，以增加网络传输性能。对于这个问题，我不在乎听到什么回答，我只关心回答符不符逻辑。一般只要答对，我就会给出“在框架层面有自己的体会，有一定的了解”，否则，我就只会给出“只能在项目经理带领下编写框架代码，对框架本身了解不多”。其实，在准备面试时，归纳框架里的要点并不难，我就不信所有人在做项目时一点积累也没，只要你说出来，可以说，这方面你就碾压了将近7成的竞争者。 别单纯看单机版的框架，适当了解些分布式此外，在描述项目里框架技术时，最好你再带些分布式的技术。下面我列些大家可以准备的分布式技术。 反向代理方面，nginx的基本配置，比如如何通过lua语言设置规则，如何设置session粘滞。如果可以，再看些nginx的底层，比如协议，集群设置，失效转移等。 远程调用dubbo方面，可以看下dubbo和zookeeper整合的知识点，再深一步，了解下dubbo底层的传输协议和序列化方式。 消息队列方面，可以看下kafka或任意一种组件的使用方式，简单点可以看下配置，工作组的设置，再深入点，可以看下Kafka集群，持久化的方式，以及发送消息是用长连接还是短拦截。 以上仅仅是用3个组件举例，大家还可以看下Redis缓存，日志框架，MyCAT分库分表等。准备的方式有两大类，第一是要会说怎么用，这比较简单，能通过配置文件搭建成一个功能模块即可，第二是可以适当读些底层代码，以此了解下协议，集群和失效转移之类的高级知识点。 如果能在面试中侃侃而谈分布式组件的底层，那么得到的评价就会比较好了，比如“深入了解框架底层”，或“框架经验丰富”，这样就算去面试架构师也行了，更何况是高级开发。 数据库方面，别就知道增删改查，得了解性能优化在实际项目里，大多数程序员用到的可能仅仅是增删改查，当我们用Mybatis时，这个情况更普遍。不过如果你面试时也这样表现，估计你的能力就和其它竞争者差不多了。 这方面，你可以准备如下的技能。 SQL高级方面，比如group by, having，左连接，子查询（带in），行转列等高级用法。 建表方面，你可以考虑下，你项目是用三范式还是反范式，理由是什么？ 尤其是优化，你可以准备下如何通过执行计划查看SQL语句改进点的方式，或者其它能改善SQL性能的方式（比如建索引等）。 如果你感觉有能力，还可以准备些MySQL集群，MyCAT分库分表的技能。比如通过LVS+Keepalived实现MySQL负载均衡，MyCAT的配置方式。同样，如果可以，也看些相关的底层代码。 哪怕你在前三点表现一般，那么至少也能超越将近一般的候选人，尤其当你在SQL优化方面表现非常好，那么你在面试高级开发时，数据库层面一定是达标的，如果你连第四点也回答非常好，那么恭喜你，你在数据库方面的能力甚至达到了初级架构的级别。 Java核心方面，围绕数据结构和性能优化准备面试题Java核心这块，网上的面试题很多，不过在此之外，大家还应当着重关注集合（即数据结构）和多线程并发这两块，在此基础上，大家可以准备些设计模式和虚拟机的说辞。 下面列些我一般会问的部分问题： String a = “123”; String b = “123”; a==b的结果是什么？ 这包含了内存，String存储方式等诸多知识点。 HashMap里的hashcode方法和equal方法什么时候需要重写？如果不重写会有什么后果？对此大家可以进一步了解HashMap（甚至ConcurrentHashMap）的底层实现。 ArrayList和LinkedList底层实现有什么差别？它们各自适用于哪些场合？对此大家也可以了解下相关底层代码。 volatile关键字有什么作用？由此展开，大家可以了解下线程内存和堆内存的差别。 CompletableFuture，这个是JDK1.8里的新特性，通过它怎么实现多线程并发控制？ JVM里，new出来的对象是在哪个区？再深入一下，问下如何查看和优化JVM虚拟机内存。 Java的静态代理和动态代理有什么差别？最好结合底层代码来说。 通过上述的问题点，我其实不仅仅停留在“会用”级别，比如我不会问如何在ArrayList里放元素。大家可以看到，上述问题包含了“多线程并发”，“JVM优化”，“数据结构对象底层代码”等细节，大家也可以举一反三，通过看一些高级知识，多准备些其它类似面试题。 我们知道，目前Java开发是以Web框架为主，那么为什么还要问Java核心知识点呢？我这个是有切身体会的。 之前在我团队里，我见过两个人，一个是就会干活，具体表现是会用Java核心基本的API，而且也没有深入了解的意愿（估计不知道该怎么深入了解），另一位平时专门会看些Java并发，虚拟机等的高级知识。过了半年以后，后者的能力快速升级到高级开发，由于对JAVA核心知识点了解很透彻，所以看一些分布式组件的底层实现没什么大问题。 而前者，一直在重复劳动，能力也只一直停留在“会干活”的层面。 而在现实的面试中，如果不熟悉Java核心知识点，估计升高级开发都难，更别说是面试架构师级别的岗位了。 Linux方面，至少了解如何看日志排查问题如果候选人能证明自己有“排查问题”和“解决问题”的能力，这绝对是个加分项，但怎么证明？ 目前大多数的互联网项目，都是部署在Linux上，也就是说，日志都是在Linux，下面归纳些实际的Linux操作。 能通过less命令打开文件，通过Shift+G到达文件底部，再通过?+关键字的方式来根据关键来搜索信息。 能通过grep的方式查关键字，具体用法是, grep 关键字 文件名，如果要两次在结果里查找的话，就用grep 关键字1 文件名 | 关键字2 —color。最后—color是高亮关键字。 能通过vi来编辑文件。 能通过chmod来设置文件的权限。 当然，还有更多更实用的Linux命令，但在实际面试过程中，不少候选人连一条linux命令也不知道。还是这句话，你哪怕知道些很基本的，也比一般人强了。 通读一段底层代码，作为加分项如何证明自己对一个知识点非常了解?莫过于能通过底层代码来说明。我在和不少工作经验在5年之内的程序员沟通时，不少人认为这很难？确实，如果要通过阅读底层代码了解分布式组件，那难度不小，但如果如下部分的底层代码，并不难懂。 ArrayList,LinkedList的底层代码里，包含着基于数组和链表的实现方式，如果大家能以此讲清楚扩容，“通过枚举器遍历“等方式，绝对能证明自己。 HashMap直接对应着Hash表这个数据结构，在HashMap的底层代码里，包含着hashcode的put，get等的操作，甚至在ConcurrentHashMap里，还包含着Lock的逻辑。我相信，如果大家在面试中，看看而言ConcurrentHashMap，再结合在纸上边说边画，那一定能征服面试官。 可以看下静态代理和动态代理的实现方式，再深入一下，可以看下Spring AOP里的实现代码。 或许Spirng IOC和MVC的底层实现代码比较难看懂，但大家可以说些关键的类，根据关键流程说下它们的实现方式。 其实准备的底层代码未必要多，而且也不限于在哪个方面，比如集合里基于红黑树的TreeSet，基于NIO的开源框架，甚至分布式组件的Dubbo，都可以准备。而且准备时未必要背出所有的底层（事实上很难做到），你只要能结合一些重要的类和方法，讲清楚思路即可（比如讲清楚HashMap如何通过hashCode快速定位）。那么在面试时，如何找到个好机会说出你准备好的上述底层代码？在面试时，总会被问到集合，Spring MVC框架等相关知识点，你在回答时，顺便说一句，“我还了解这块的底层实现”，那么面试官一定会追问，那么你就可以说出来了。不要小看这个对候选人的帮助，一旦你讲了，只要意思到位，那么最少能得到个“肯积极专业“的评价，如果描述很清楚，那么评价就会升级到“熟悉Java核心技能（或Spring MVC），且基本功扎实”。要知道，面试中，很少有人能讲清楚底层代码，所以你抛出了这个话题，哪怕最后没达到预期效果，面试官也不会由此对你降低评价。所以说，准备这块绝对是“有百利而无一害”的挣钱买卖。 一切的一切，把上述技能嵌入到你做过的项目里在面试过程中，我经常会听到一些比较遗憾的回答，比如候选人对SQL优化技能讲得头头是道，但最后得知，这是他平时自学时掌握的，并没用在实际项目里。 当然这总比不说要好，所以我会写下“在平时自学过SQL优化技能”，但如果在项目里实践过，那么我就会写下“有实际数据库SQL优化的技能”。大家可以对比下两者的差别，一个是偏重理论，一个是直接能干活了。其实，很多场景里，我就不信在实际项目里一定没有实践过SQL优化技能。 从这个案例中，我想告诉大家的是，你之前费了千辛万苦（其实方法方向得到，也不用费太大精力）准备的很多技能和说辞，最后应该落实到你的实际项目里。 比如你有过在Linux日志里查询关键字排查问题的经验，在描述时你可以带一句，在之前的项目里我就这样干的。又如，你通过看底层代码，了解了TreeSet和HashSet的差别以及它们的适用范围，那么你就可以回想下你之前做的项目，是否有个场景仅仅适用于TreeSet？如果有，那么你就可以适当描述下项目的需求，然后说，通过读底层代码，我了解了两者的差别，而且在这个实际需求里，我就用了TreeSet，而且我还专门做了对比性试验，发现用TreeSet比HashSet要高xx个百分点。 请记得，“实践经验”一定比“理论经验”值钱，而且大多数你知道的理论上的经验，一定在你的项目里用过。所以，如果你仅仅让面试官感觉你只有“理论经验”，那就太亏了。 小结：前文更多讲述的准备面试的方法前文给出的面试题并不多，大家更多看到的是面试官发现的诸多候选人的痛点。 前文的用意是让大家别再重蹈别人的覆辙，这还不算，本文还给出了不少准备面试的方法。你的能力或许比别人出众，但如果你准备面试的方式和别人差不多，或者就拿你在项目里干的活来说事，而没有归纳出你在项目中的亮点，那么面试官还真的会看扁你。 JAVA多线程和并发基础面试问答多线程和并发问题是Java技术面试中面试官比较喜欢问的问题之一。在这里，从面试的角度列出了大部分重要的问题，但是你仍然应该牢固的掌握Java多线程基础知识来对应日后碰到的问题。 Java多线程面试问题JAVA多线程和并发基础面试问答多线程和并发问题是Java技术面试中面试官比较喜欢问的问题之一。在这里，从面试的角度列出了大部分重要的问题，但是你仍然应该牢固的掌握Java多线程基础知识来对应日后碰到的问题。 Java多线程面试问题什么是线程？线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成该任务只需10毫秒。Java在语言层面对多线程提供了卓越的支持，它也是一个很好的卖点。欲了解更多详细信息请点击这里。 进程和线程之间有什么不同？一个进程是一个独立(self contained)的运行环境，它可以被看作一个程序或者一个应用。而线程是在进程中执行的一个任务。Java运行环境是一个包含了不同的类和程序的单一进程。线程可以被称为轻量级进程。线程需要较少的资源来创建和驻留在进程中，并且可以共享进程中的资源。线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。更多详细信息请点击这里。 有哪些不同的线程生命周期？当我们在Java程序中新建一个线程时，它的状态是New。当我们调用线程的start()方法时，状态被改变为Runnable。线程调度器会为Runnable线程池中的线程分配CPU时间并且将它们的状态改变为Running。其他的线程状态还有Waiting，Blocked 和Dead。读这篇文章可以了解更多关于线程生命周期的知识。 多线程编程的好处是什么？在多线程程序中，多个线程被并发的执行以提高程序的效率，CPU不会因为某个线程需要等待资源而进入空闲状态。多个线程共享堆内存(heap memory)，因此创建多个线程去执行一些任务会比创建多个进程更好。举个例子，Servlets比CGI更好，是因为Servlets支持多线程而CGI不支持。 用户线程和守护线程有什么区别？当我们在Java程序中创建一个线程，它就被称为用户线程。一个守护线程是在后台执行并且不会阻止JVM终止的线程。当没有用户线程在运行的时候，JVM关闭程序并且退出。一个守护线程创建的子线程依然是守护线程。 我们如何创建一个线程？有两种创建线程的方法：一是实现Runnable接口，然后将它传递给Thread的构造函数，创建一个Thread对象；二是直接继承Thread类。若想了解更多可以阅读这篇关于如何在Java中创建线程的文章。 用Runnable还是Thread？ 这个问题是上题的后续，大家都知道我们可以通过继承Thread类或者调用Runnable接口来实现线程，问题是，那个方法更好呢？什么情况下使用它？这个问题很容易回答，如果你知道Java不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是调用Runnable接口好了。更多详细信息请点击这里。 Thread 类中的start() 和 run() 方法有什么区别？ 这个问题经常被问到，但还是能从此区分出面试者对Java线程模型的理解程度。start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。更多讨论请点击这里。 可以直接调用Thread类的run()方法么？当然可以，但是如果我们调用了Thread的run()方法，它的行为就会和普通的方法一样，为了在新的线程中执行我们的代码，必须使用Thread.start()方法。 Java中Runnable和Callable有什么不同？ Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这些功能。Callable可以返回装载有计算结果的Future对象。更多讨论请点击这里。 Java中CyclicBarrier 和 CountDownLatch有什么不同？ CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。点此查看更多信息和示例代码。 Java中Semaphore是什么？ Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。更多详细信息请点击这里。 Java内存模型是什么？ Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了： 线程内的代码能够按先后顺序执行，这被称为程序次序规则。对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。一个线程的所有操作都会在线程终止之前，线程终止规则。一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。可传递性 强烈建议大家阅读《Java并发编程实践》第十六章来加深对Java内存模型的理解。 Java中的volatile 变量是什么？ volatile是一个特殊的修饰符，只有成员变量才能使用它。在Java并发程序缺少同步类的情况下，多线程对成员变量的操作对其它线程是透明的。volatile变量可以保证下一个读取操作会在前一个写操作之后发生，就是上一题的volatile变量规则。点击这里查看更多volatile的相关内容。 volatile关键字在Java中有什么作用？当我们使用volatile关键字去修饰变量的时候，所以线程都会直接读取该变量并且不缓存它。这就确保了线程读取到的变量是同内存中是一致的。 什么是线程安全？Vector是一个线程安全类吗？ 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。点击这里查看更多。 如何确保线程安全？在Java中可以有很多方法来保证线程安全——同步，使用原子类(atomic concurrent classes)，实现并发锁，使用volatile关键字，使用不变类和线程安全类。在线程安全教程中，你可以学到更多。 Java中什么是竞态条件？ 举个例子说明。 竞态条件会导致程序在并发情况下出现一些bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的bugs。这种bugs很难发现而且会重复出现，因为线程间的随机竞争。一个例子就是无序处理，详见这里。 如何让正在运行的线程暂停一段时间？我们可以使用Thread类的Sleep()方法让线程暂停一段时间。需要注意的是，这并不会让线程终止，一旦从休眠中唤醒线程，线程的状态将会被改变为Runnable，并且根据线程调度，它将得到执行。 Java中如何停止一个线程？ Java提供了很丰富的API但没有为停止线程提供API。JDK 1.0本来有一些像stop(), suspend() 和 resume()的控制方法但是由于潜在的死锁威胁因此在后续的JDK版本中他们被弃用了，之后Java API的设计者就没有提供一个兼容且线程安全的方法来停止一个线程。当run() 或者 call() 方法执行完的时候线程会自动结束,如果要手动结束一个线程，你可以用volatile 布尔变量来退出run()方法的循环或者是取消任务来中断线程。点击这里查看示例代码。 一个线程运行时发生异常会怎样？ 这是我在一次面试中遇到的一个很刁钻的Java面试题, 简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler()来查询线程的UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException()方法进行处理。 如何在两个线程间共享数据？ 你可以通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构。这篇教程《Java线程间通信》(涉及到在两个线程间共享对象)用wait和notify方法实现了生产者消费者模型。 Java中notify 和 notifyAll有什么区别？ 这又是一个刁钻的问题，因为多线程可以等待单监控锁，Java API 的设计人员提供了一些方法当等待条件改变的时候通知它们，但是这些方法没有完全实现。notify()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。详情。 为什么wait, notify 和 notifyAll这些方法不在thread类里面？ Java的每个对象中都有一个锁(monitor，也可以成为监视器) 并且wait()，notify()等方法用于等待对象的锁或者通知其他线程对象的监视器可用。在Java的线程中并没有可供任何对象使用的锁和同步器。这就是为什么这些方法是Object类的一部分，这样Java的每一个类都有用于线程间通信的基本方法 这是个设计相关的问题，它考察的是面试者对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。你也可以查看这篇文章了解更多。 为什么wait(), notify()和notifyAll()必须在同步方法或者同步块中被调用？当一个线程需要调用对象的wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的notify()方法。同样的，当一个线程需要调用对象的notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用，否则会抛出IllegalMonitorStateException异常。 Thread类中的yield方法有什么作用？ Yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。 Thread.yield()在哪个线程中被调用就表示哪个线程愿意放弃处理器资源.点击这里查看更多yield方法的相关内容。 为什么Thread类的sleep()和yield()方法是静态的？Thread类的sleep()和yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 你对线程优先级的理解是什么？每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个int变量(从1-10)，1代表最低优先级，10代表最高优先级。 什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing)？线程调度器是一个操作系统服务，它负责为Runnable状态的线程分配CPU时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。时间分片是指将可用的CPU时间分配给可用的Runnable线程的过程。分配CPU时间可以基于线程优先级或者线程等待的时间。线程调度并不受到Java虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。 什么是线程池？ 为什么要使用它？ 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。 一个线程池管理了一组工作线程，同时它还包括了一个用于放置等待执行的任务的队列。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。更多内容详见这篇文章。 在多线程中，什么是上下文切换(context-switching)？上下文切换是存储和恢复CPU状态的过程，它使得线程执行能够从中断点恢复执行。上下文切换是多任务操作系统和多线程环境的基本特征。 你如何确保main()方法所在的线程是Java程序最后结束的线程？我们可以使用Thread类的join()方法来确保所有程序创建的线程在main()方法退出前结束。这里有一篇文章关于Thread类的joint()方法。 线程之间是如何通信的？当线程间是可以共享资源时，线程间通信是协调它们的重要的手段。Object类中wait()\notify()\notifyAll()方法可以用于线程间通信关于资源的锁的状态。点击这里有更多关于线程wait, notify和notifyAll。 同步方法和同步块，哪个是更好的选择？同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。 如何创建守护线程？使用Thread类的setDaemon(true)方法可以将线程设置为守护线程，需要注意的是，需要在调用start()方法前调用这个方法，否则会抛出IllegalThreadStateException异常。 什么是ThreadLocal?ThreadLocal用于创建线程的本地变量，我们知道一个对象的所有线程会共享它的全局变量，所以这些变量不是线程安全的，我们可以使用同步技术。但是当我们不想使用同步的时候，我们可以选择ThreadLocal变量。 每个线程都会拥有他们自己的Thread变量，它们可以使用get()\set()方法去获取他们的默认值或者在线程内部改变他们的值。ThreadLocal实例通常是希望它们同线程状态关联起来是private static属性。在ThreadLocal例子这篇文章中你可以看到一个关于ThreadLocal的小程序。 ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。查看答案了解更多。 什么是FutureTask？ 在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 Java中interrupted 和 isInterruptedd方法的区别？ interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 什么是Thread Group？为什么建议使用它？ThreadGroup是一个类，它的目的是提供关于线程组的信息。 ThreadGroup API比较薄弱，它并没有比Thread提供了更多的功能。它有两个主要的功能：一是获取线程组中处于活跃状态线程的列表；二是设置为线程设置未捕获异常处理器(ncaught exception handler)。但在Java 1.5中Thread类也添加了setUncaughtExceptionHandler(UncaughtExceptionHandler eh) 方法，所以ThreadGroup是已经过时的，不建议继续使用。 t1.setUncaughtExceptionHandler(new UncaughtExceptionHandler(){ @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(&quot;exception occured:&quot;+e.getMessage()); } }); 什么是Java Timer类？如何创建一个有特定时间间隔的任务？java.util.Timer是一个工具类，可以用于安排一个线程在未来的某个特定时间执行。Timer类可以用安排一次性任务或者周期任务。 java.util.TimerTask是一个实现了Runnable接口的抽象类，我们需要去继承这个类来创建我们自己的定时任务并使用Timer去安排它的执行。 这里有关于java Timer的例子。 为什么你应该在循环中检查等待条件? 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait()方法效果更好的原因，你可以在Eclipse中创建模板调用wait和notify试一试。如果你想了解更多关于这个问题的内容，我推荐你阅读《Effective Java》这本书中的线程和同步章节。 Java中的同步集合与并发集合有什么区别？ 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。更多内容详见答案。 Java中ConcurrentHashMap的并发度是什么？ ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。欲了解更多并发度和内部大小调整请阅读文章How ConcurrentHashMap works in Java。 Java中堆和栈有什么不同？ 为什么把这个问题归类在多线程和并发面试题里？因为栈是一块和线程紧密相关的内存区域。每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 更多内容详见答案。 如何写代码来解决生产者消费者问题？ 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用wait和notify来解决这个问题，比较赞的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型，这篇教程有实现它。 什么是死锁(Deadlock)？如何分析和避免死锁？ Java多线程中的死锁 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。这篇教程有代码示例和避免死锁的讨论细节。 分析死锁，我们需要查看Java应用程序的线程转储。我们需要找出那些状态为BLOCKED的线程和他们等待的资源。每个资源都有一个唯一的id，用这个id我们可以找出哪些线程已经拥有了它的对象锁。 避免嵌套锁，只在需要的地方使用锁和避免无限期等待是避免死锁的通常办法，阅读这篇文章去学习如何分析死锁。 Java中活锁和死锁有什么区别？ 这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁？ 我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我参加了一次电话面试。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。你可以查看这篇文章了解更多。 什么是Java线程转储(Thread Dump)?如何在Java中获取线程堆栈？ 线程转储是一个JVM活动线程的列表，它对于分析系统瓶颈和死锁非常有用。 对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id,可以编写一些脚本去定时的产生线程转储以待分析。 读这篇文档可以了解更多关于产生线程转储的知识。 JVM中哪个参数是用来控制线程的栈堆栈小的 这个问题很简单， -Xss参数用来控制线程的堆栈大小。你可以查看JVM配置列表来了解这个参数的更多信息。 Java中synchronized 和 ReentrantLock 有什么不同？ Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。你可以查看这篇文章了解更多 Java中的ReadWriteLock是什么？ 一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java中的ReadWriteLock是Java 5 中新增的一个接口，一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读锁。 有三个线程T1，T2，T3，怎么确保它们按顺序执行？ 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。你可以查看这篇文章了解更多。 如果你提交任务时，线程池队列已满。会时发会生什么？ 这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。 什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。更多详细信息请点击这里。 Java线程池中submit() 和 execute()方法有什么区别？ 两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中, 而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。更多详细信息请点击这里。 Swing是线程安全的吗？ 为什么？ 你可以很肯定的给出回答，Swing不是线程安全的，但是你应该解释这么回答的原因即便面试官没有问你为什么。当我们说swing不是线程安全的常常提到它的组件，这些组件不能在多线程中进行修改，所有对GUI组件的更新都要在AWT线程中完成，而Swing提供了同步和异步两种回调方法来进行更新。点击这里查看更多swing和线程安全的相关内容。 Java中invokeAndWait 和 invokeLater有什么区别？ 这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而invokeLater()方法是异步调用更新组件的。更多详细信息请点击这里。 Swing API中哪些方法是线程安全的？ 这个问题又提到了swing和线程安全，虽然组件不是线程安全的但是有一些方法是可以被多线程安全调用的，比如repaint(), revalidate()。 JTextComponent的setText()方法和JTextArea的insert() 和 append() 方法也是线程安全的。 如何在Java中创建Immutable对象？ 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。how to make an object Immutable in Java有详细的教程，看完你可以充满自信。 多线程中的忙循环是什么? 忙循环就是程序员用循环让一个线程等待，不像传统方法wait(), sleep() 或 yield() 它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。你可以查看这篇文章获得更多信息。 volatile 变量和 atomic 变量有什么不同？ 这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。Volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么 count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 如果同步块内的线程抛出异常会发生什么？ 这个问题坑了很多Java程序员，若你能想到锁是否释放这条线索来回答还有点希望答对。无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 单例模式的双检锁是什么？ 这个问题在Java面试中经常被问到，但是面试官对回答此问题的满意度仅为50%。一半的人写不出双检锁还有一半的人说不出它的隐患和Java1.5是如何对它修正的。它其实是一个用来创建线程安全的单例的老方法，当单例实例第一次被创建时它试图用单个锁进行性能优化，但是由于太过于复杂在JDK1.4中它是失败的，我个人也不喜欢它。无论如何，即便你也不喜欢它但是还是要了解一下，因为它经常被问到。你可以查看how double checked locking on Singleton works这篇文章获得更多信息。 如何在Java中创建线程安全的Singleton？ 这是上面那个问题的后续，如果你不喜欢双检锁而面试官问了创建Singleton类的替代方法，你可以利用JVM的类加载和静态变量初始化特征来创建Singleton实例，或者是利用枚举类型来创建Singleton，我很喜欢用这种方法。你可以查看这篇文章获得更多信息。 写出3条你遵循的多线程最佳实践 这种问题我最喜欢了，我相信你在写并发代码来提升性能的时候也会遵循某些最佳实践。以下三条最佳实践我觉得大多数Java程序员都应该遵循： 给你的线程起个有意义的名字。 这样可以方便找bug或追踪。 OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。 避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。 多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。Java并发集合有更详细的说明。 如何强制启动一个线程？ 这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc()来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API。 Java中的fork join框架是什么？ fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。你可以查看这篇文章获得更多信息。 Java多线程中调用wait() 和 sleep()方法有什么不同？ Java程序中wait 和 sleep都会造成某种形式的暂停，它们可以满足不同的需要。wait()方法用于线程间通信，如果等待条件为真且其它线程被唤醒时它会释放锁，而sleep()方法仅仅释放CPU资源或者让当前线程停止执行一段时间，但不会释放锁。你可以查看这篇文章获得更多信息。 Java并发面试问题什么是原子操作？在Java Concurrency API中有哪些原子类(atomic classes)？原子操作是指一个不受其他操作影响的操作任务单元。原子操作是在多线程环境下避免数据不一致必须的手段。 int++并不是一个原子操作，所以当一个线程读取它的值并加1时，另外一个线程有可能会读到之前的值，这就会引发错误。 为了解决这个问题，必须保证增加操作是原子的，在JDK1.5之前我们可以使用同步技术来做到这一点。到JDK1.5，java.util.concurrent.atomic包提供了int和long类型的装类，它们可以自动的保证对于他们的操作是原子的并且不需要使用同步。可以阅读这篇文章来了解Java的atomic类。 Java Concurrency API中的Lock接口(Lock interface)是什么？对比同步它有什么优势？Lock接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 它的优势有： 可以使锁更公平可以使线程在等待锁的时候响应中断可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间可以在不同的范围，以不同的顺序获取和释放锁阅读更多关于锁的例子 什么是Executors框架？Executor框架同java.util.concurrent.Executor 接口在Java 5中被引入。Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。 无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用Executors框架可以非常方便的创建一个线程池，阅读这篇文章可以了解如何使用Executor框架创建一个线程池。 什么是阻塞队列？如何使用阻塞队列来实现生产者-消费者模型？java.util.concurrent.BlockingQueue的特性是：当队列是空的时，从队列中获取或删除元素的操作将会被阻塞，或者当队列是满时，往队列里添加元素的操作会被阻塞。 阻塞队列不接受空值，当你尝试向队列中添加空值的时候，它会抛出NullPointerException。 阻塞队列的实现都是线程安全的，所有的查询方法都是原子的并且使用了内部锁或者其他形式的并发控制。 BlockingQueue 接口是java collections框架的一部分，它主要用于实现生产者-消费者问题。 阅读这篇文章了解如何使用阻塞队列实现生产者-消费者问题。 什么是Callable和Future?Java 5在concurrency包中引入了java.util.concurrent.Callable 接口，它和Runnable接口很相似，但它可以返回一个对象或者抛出一个异常。 Callable接口使用泛型去定义它的返回类型。Executors类提供了一些有用的方法去在线程池中执行Callable内的任务。由于Callable任务是并行的，我们必须等待它返回的结果。java.util.concurrent.Future对象为我们解决了这个问题。在线程池提交Callable任务后返回了一个Future对象，使用它我们可以知道Callable任务的状态和得到Callable返回的执行结果。Future提供了get()方法让我们可以等待Callable结束并获取它的执行结果。 阅读这篇文章了解更多关于Callable，Future的例子。 什么是FutureTask?FutureTask是Future的一个基础实现，我们可以将它同Executors使用处理异步任务。通常我们不需要使用FutureTask类，但当我们打算重写Future接口的一些方法并保持原来基础的实现是，它就变得非常有用。我们可以仅仅继承于它并重写我们需要的方法。阅读Java FutureTask例子，学习如何使用它。 什么是并发容器的实现？Java集合类都是快速失败的，这就意味着当集合被改变且一个线程在使用迭代器遍历集合的时候，迭代器的next()方法将抛出ConcurrentModificationException异常。 并发容器支持并发的遍历和并发的更新。 主要的类有ConcurrentHashMap, CopyOnWriteArrayList 和CopyOnWriteArraySet。阅读这篇文章了解如何避免ConcurrentModificationException Executors类是什么？Executors为Executor，ExecutorService，ScheduledExecutorService，ThreadFactory和Callable类提供了一些工具方法。 Executors可以用于方便的创建线程池。]]></content>
      <categories>
        <category>java</category>
        <category>interview</category>
      </categories>
      <tags>
        <tag>面试题</tag>
        <tag>面试技巧</tag>
      </tags>
  </entry>
</search>